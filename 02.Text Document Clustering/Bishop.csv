,topic,text
0,[introduction]," problem searching patterns data fundamental one long successful history instance extensive astronomical observations tycho brahe century allowed johannes kepler discover empirical laws planetary motion turn provided springboard development classical mechanics similarly discovery regularities atomic spectra played key role development veriﬁcation quantum physics early twentieth century ﬁeld pattern recognition concerned automatic discovariance ery regularities data use computer algorithms use regularities take actions classifying data different categories consider example recognizing handwritten digits illustrated figure digit corresponds pixel image represented vector comprising real numbers goal build machine take vector input produce identity digit output nontrivial problem due wide variability handwriting could introduction figure examples hand written dig taken zip codes tackled using handcrafted rules heuristics distinguishing digits based shapes strokes practice approach leads proliferation rules exceptions rules invariably gives poor results far better results obtained adopting machine learning approach large set digits called training set used tune parameters adaptive model categories digits training set known advance typically inspecting individually hand labelling express category digit using target vector represents identity corresponding digit suitable techniques representing cate gories terms vectors discussed later note one target vector digit image result running machine learning algorithm expressed function takes new digit image input generates output vector encoded way target vectors precise form function determined training phase also known learning phase basis training data model trained termine identity new digit images said comprise test set ability categorize correctly new examples differ used training known generalization practical applications variability input vectors training data comprise tiny fraction possible input vectors generalization central goal pattern recognition practical applications original input variables typically prepro cessed transform new space variables hoped pattern recognition problem easier solve instance digit recognition problem images digits typically translated scaled digit contained within box ﬁxed size greatly reduces variability within digit class location scale digits makes much easier subsequent pattern recognition algorithm distinguish different classes pre processing stage sometimes also called feature extraction note new test data must pre processed using steps training data pre processing might also performed order speed computation example goal real time face detection high resolution video stream computer must handle huge numbers pixels per second presenting directly complex pattern recognition algorithm may computationally infeasi ble instead aim ﬁnd useful features fast compute yet introduction also preserve useful discriminatory information enabling faces distinguished non faces features used inputs pattern recognition algorithm instance average value image intensity rectangular subregion evaluated extremely efﬁciently viola jones set features prove effective fast face detection number features smaller number pixels kind pre processing repre sents form dimensionality reduction care must taken pre processing often information discarded information important solution problem overall accuracy system suffer applications training data comprises examples input vectors along corresponding target vectors known supervised learning prob lems cases digit recognition example aim assign input vector one ﬁnite number discrete categories called classiﬁcation problems desired output consists one continuous variables task called regression example regression problem would prediction yield chemical manufacturing process inputs consist concentrations reactants temperature pressure pattern recognition problems training data consists set input vectors without corresponding target values goal unsupervised learning problems may discover groups similar examples within data called clustering determine distribution data within input space known density estimation project data high dimensional space two three dimensions purpose visualization finally technique reinforcement learning sutton barto con cerned problem ﬁnding suitable actions take given situation order maximize reward learning algorithm given examples optimal outputs contrast supervised learning must instead discover process trial error typically sequence states actions learning algorithm interacting environment many cases current action affects immediate reward also impact ward subsequent time steps example using appropriate reinforcement learning techniques neural network learn play game backgammon high standard tesauro network must learn take board position input along result dice throw produce strong move output done network play copy perhaps million games major challenge game backgammon involve dozens moves yet end game reward form victory achieved reward must attributed appropriately moves led even though moves good ones others less example credit assignment problem general feature inforcement learning trade exploration system tries new kinds actions see effective exploitation system makes use actions known yield high reward strong focus either exploration exploitation yield poor results reinforcement learning continues active area machine learning research however introduction figure plot training data set points shown blue circles comprising observation input variable along corresponding target variable green curve shows function sin used generate data goal predict value new value without knowledge green curve detailed treatment lies beyond scope book although tasks needs tools techniques many key ideas underpin common problems one main goals chapter introduce relatively informal way several important concepts illustrate using simple examples later book shall see ideas emerge context sophisti cated models applicable real world pattern recognition applications chapter also provides self contained introduction three important tools used throughout book namely probability theory decision theory information theory although might sound like daunting topics fact straightforward clear understanding essential machine learning techniques used best effect practical applications 
"
1,"[example, polynomial, curve, fitting]"," begin introducing simple regression problem shall use running example throughout chapter motivate number key concepts suppose observe real valued input variable wish use observation predict value real valued target variable present purposes structive consider artiﬁcial example using synthetically generated data know precise process generated data comparison learned model data example generated function sin random noise included target values described detail appendix suppose given training set comprising observations written together corresponding observations values denoted figure shows plot training set comprising data points input data set figure generated choosing values spaced uniformly range target data set obtained ﬁrst computing corresponding values function example polynomial curve fitting sin adding small level random noise gaussian distri bution gaussian distribution discussed section point order obtain corresponding value generating data way capturing property many real datasets namely possess underlying regularity wish learn individual observations corrupted random noise noise might arise intrinsically stochastic random processes radioactive decay typically due sources variability unobserved goal exploit training set order make predictions value target variable new value input variable shall see later involves implicitly trying discover underlying function sin intrinsically difﬁcult problem generalize ﬁnite data set furthermore observed data corrupted noise given uncertainty appropriate value probability theory discussed section provides framework expressing uncertainty precise quantitative manner decision theory discussed section allows exploit probabilistic representation order make predictions optimal according appropriate criteria moment however shall proceed rather informally consider simple approach based curve ﬁtting particular shall data using polynomial function form order polynomial denotes raised power polynomial coefﬁcients collectively denoted vector note although polynomial function nonlinear function linear function coefﬁcients functions polynomial linear unknown parameters important properties called linear models discussed extensively chapters values coefﬁcients determined ﬁtting polynomial training data done minimizing error function measures misﬁt function given value training set data points one simple choice error function widely used given sum squares errors predictions data point corresponding target values minimize factor included later convenience shall discuss tivation choice error function later chapter moment simply note nonnegative quantity would zero introduction figure error function corre sponds one half sum squares displacements shown vertical green bars data point function function pass exactly training data point geomet rical interpretation sum squares error function illustrated figure solve curve ﬁtting problem choosing value small possible error function quadratic function coefﬁcients derivatives respect coefﬁcients linear elements minimization error function unique solution denoted found closed form resulting polynomial exercise given function remains problem choosing order polynomial shall see turn example important concept called model comparison model selection figure show four examples results ﬁtting polynomials orders data set shown figure notice constant ﬁrst order polynomials give rather poor ﬁts data consequently rather poor representations function sin third order polynomial seems give best function sin examples shown figure much higher order polynomial obtain excellent training data fact polynomial passes exactly data point however ﬁtted curve oscillates wildly gives poor representation function sin latter behaviour known ﬁtting noted earlier goal achieve good generalization making accurate predictions new data obtain quantitative insight dependence generalization performance considering separate test set comprising data points generated using exactly procedure used generate training set points new choices random noise values included target values choice evaluate residual value given training data also evaluate test data set sometimes convenient use root mean square example polynomial curve fitting figure plots polynomials various orders shown red curves ﬁtted data set shown figure rms error deﬁned rms division allows compare different sizes datasets equal footing square root ensures rms measured scale units target variable graphs training test set rms errors shown various values figure test set error measure well predicting values new data observations note figure small values give relatively large values test set error attributed fact corresponding polynomials rather inﬂexible incapable capturing oscillations function sin values range give small values test set error also give reasonable representations generating function sin seen case figure introduction figure graphs root mean square error deﬁned evaluated training set independent test set various values training test training set error goes zero might expect polynomial contains degrees freedom corresponding coefﬁcients tuned exactly data points training set however test set error become large saw figure corresponding function exhibits wild oscillations may seem paradoxical polynomial given order contains lower order polynomials special cases polynomial therefore capa ble generating results least good polynomial furthermore might suppose best predictor new data would function sin data generated shall see later indeed case know power series expansion function sin contains terms orders might expect results improve monotonically increase gain insight problem examining values efﬁcients obtained polynomials various order shown table see increases magnitude coefﬁcients typically gets larger particular polynomial coefﬁcients become ﬁnely tuned data developing large positive negative values correspond table table coefﬁcients polynomials various order observe typical mag nitude coefﬁcients creases dramatically der polynomial increases example polynomial curve fitting figure plots solutions obtained minimizing sum squares error function using polynomial data points left plot data points right plot see increasing size data set reduces ﬁtting probleming polynomial function matches data points exactly data points particularly near ends range function exhibits large oscillations observed figure intuitively happening ﬂexible polynomials larger values becoming increasingly tuned random noise target values also interesting examine behaviour given model size data set varied shown figure see given model complexity ﬁtting problem become less severe size data set increases another way say larger data set complex words ﬂexible model afford data one rough heuristic sometimes advocated number data points less multiple say number adaptive parameters model however shall see chapter number parameters necessarily appropriate measure model complexity also something rather unsatisfying limit number parameters model according size available training set would seem reasonable choose complexity model according complexity problem solved shall see least squares approach ﬁnding model parameters represents speciﬁc case maximum likelihood discussed section ﬁtting problem understood general property maximum likelihood adopting bayesian approach section ﬁtting problem avoided shall see difﬁculty bayesian perspective employing models number parameters greatly exceeds number data points indeed bayesian model effective number parameters adapts automatically size data set moment however instructive continue current approach consider practice apply datasets limited size introduction figure plots polynomials ﬁtted data set shown figure using regularized error function two values regularization parameter corresponding case regularizer corresponding shown bottom right figure may wish use relatively complex ﬂexible models one technique often used control ﬁtting phenomenon cases regularization involves adding penalty term error function order discourage coefﬁcients reaching large values simplest penalty term takes form sum squares coefﬁcients leading modiﬁed error function form coefﬁcient governs rel ative importance regularization term compared sum squares error term note often coefﬁcient omitted regularizer inclusion causes results depend choice origin target variable hastie may included regularization coefﬁcient shall discuss topic detail section error function minimized exactly closed form techniques known exercise statistics literature shrinkage methods reduce value coefﬁcients particular case quadratic regularizer called ridge regression hoerl kennard context neural networks approach known weight decay figure shows results ﬁtting polynomial order data set using regularized error function given see value ﬁtting suppressed obtain much closer representation underlying function sin however use large value obtain poor shown figure corresponding coefﬁcients ﬁtted polynomials given table showing regularization desired effect reducing example polynomial curve fitting table table coefﬁcients polynomials various values regularization parameter note corresponds model regularization graph bottom right figure see value increases typical magnitude coefﬁcients gets smaller magnitude coefﬁcients impact regularization term generalization error seen plotting value rms error training test sets shown figure see effect controls effective complexity model hence determines degree ﬁtting issue model complexity important one discussed length section simply note trying solve practical application using approach minimizing error function would ﬁnd way determine suitable value model complexity results suggest simple way achieving namely taking available data partitioning training set used determine coefﬁcients separate validation set also called hold set used optimize model complexity either many cases however prove wasteful valuable training data seek sophisticated approaches far discussion polynomial curve ﬁtting appealed largely tuition seek principled approach solving problems pattern recognition turning discussion probability theory well providing foundation nearly subsequent developments book also figure graph root mean square ror versus polynomial training test introduction give important insights concepts introduced con text polynomial curve ﬁtting allow extend complex situations 
"
2,"[probability, theory]"," key concept ﬁeld pattern recognition uncertainty arises noise measurements well ﬁnite size datasets prob ability theory provides consistent framework quantiﬁcation manipulation uncertainty forms one central foundations pattern recognition combined decision theory discussed section allows make optimal predictions given information available even though information may incomplete ambiguous introduce basic concepts probability theory considering simple example imagine two boxes one red one blue red box apples oranges blue box apples orange illustrated figure suppose randomly pick one boxes box randomly select item fruit observed sort fruit replace box came could imagine repeating process many times let suppose pick red box time pick blue box time remove item fruit box equally likely select pieces fruit box example identity box chosen random variable shall denote random variable take one two possible values namely corresponding red box corresponding blue box similarly identity fruit also random variable denoted take either values apple orange begin shall deﬁne probability event fraction times event occurs total number trials limit total number trials goes inﬁnity thus probability selecting red box figure use simple example two coloured boxes containing fruit apples shown green anges shown orange intro duce basic ideas probability probability theory figure derive sum product rules probability considering two random variables takes values takes values illustration consider total number instances variables denote number instances number points corresponding cell array number points column corresponding denoted number points row corresponding denoted probability selecting blue box write probabilities note deﬁnition probabilities must lie interval also events mutually exclusive include possible outcomes instance example box must either red blue see probabilities events must sum one ask questions overall probability lection procedure pick apple given chosen orange probability box chose blue one answer questions indeed much complex questions associated problems pattern recognition equipped two ementary rules probability known sum rule product rule obtained rules shall return boxes fruit example order derive rules probability consider slightly general ample shown figure involving two random variables could instance box fruit variables considered shall suppose take values take values consider total trials sample variables let number trials also let number trials takes value irrespective value takes denoted similarly let number trials takes value denoted probability take value take value written called joint probability given number points falling cell fraction total number points hence implicitly considering limit similarly probability takes value irrespective value written given fraction total number points fall column number instances column figure sum number instances cell column therefore introduction sum rule probability note sometimes called marginal probability obtained marginalizing summing variables case consider instances fraction instances written called conditional probability given obtained ﬁnding fraction points column fall cell hence given derive following relationship product rule probability far quite careful make distinction random variable box fruit example values random variable take example box red one thus probability takes value denoted although helps avoid ambiguity leads rather cumbersome notation many cases need pedantry instead may simply write denote distribution random variable denote distribution evaluated particular value provided interpretation clear context compact notation write two fundamental rules probability theory following form rules probability sum rule product rule probability theory simply probability two simple rules form basis probabilistic machinery use throughout book product rule together symmetry property immediately obtain following relationship conditional probabilities called bayes theorem plays central role pattern recognition machine learning using sum rule denominator bayes theorem expressed terms quantities appearing numerator view denominator bayes theorem normalization constant required ensure sum conditional probability left hand side values equals one figure show simple example involving joint distribution two variables illustrate concept marginal conditional distributions ﬁnite sample data points drawn joint distribution shown top left top right histogram fractions data points two values deﬁnition probability fractions would equal corresponding probabilities limit view histogram simple way model probability distribution given ﬁnite number points drawn distribution modelling distributions data lies heart statistical pattern recognition explored great detail book remaining two plots figure show corresponding histogram estimates let return example involving boxes fruit moment shall explicit distinguishing random variables instantiations seen probabilities selecting either red blue boxes given respectively note satisfy suppose pick box random turns blue box probability selecting apple fraction apples blue box fact write four conditional probabilities type fruit given selected box introduction figure illustration distribution two variables takes possible values takes two possible values top left ﬁgure shows sample points drawn joint probability distri bution variables remaining ﬁgures show histogram estimates marginal distributions well conditional distribution corresponding bottom row top left ﬁgure note probabilities normalized similarly use sum product rules probability evaluate overall probability choosing apple follows using sum rule probability theory suppose instead told piece fruit selected orange would like know box came requires evaluate probability distribution boxes conditioned identity fruit whereas probabilities give probability distribution fruit conditioned identity box solve problem reversing conditional probability using bayes theorem give sum rule follows provide important interpretation bayes theorem follows asked box chosen told identity selected item fruit complete information available provided probability call prior probability probability available observe identity fruit told fruit orange use bayes theorem compute probability shall call posterior probability probability obtained observed note example prior probability selecting red box likely select blue box red one however observed piece selected fruit orange ﬁnd posterior probability red box likely box selected fact red one result accords intuition proportion oranges much higher red box blue box observation fruit orange provides signiﬁcant evidence favouring red box fact evidence sufﬁciently strong outweighs prior makes likely red box chosen rather blue one finally note joint distribution two variables factorizes product marginals said independent product rule see conditional distribution given indeed independent value instance boxes fruit example box contained fraction apples oranges probability selecting say apple independent box chosen 
"
3,"[probability, theory, probability, densities]"," well considering probabilities deﬁned discrete sets events also wish consider probabilities respect continuous variables shall limit relatively informal discussion probability real valued variable falling interval given called probability density illustrated figure probability lie interval given introduction figure concept probability discrete variables tended probability density continuous variable probability lying interval given probability density expressed derivative cumulative distri bution function probabilities nonnegative value must lie real axis probability density must satisfy two conditions nonlinear change variable probability density transforms differently simple function due jacobian factor instance consider change variables function becomes consider probability density corresponds density respect new variable sufﬁces denote fact different densities observations falling range small values transformed range hence one consequence property concept maximum probability density dependent choice variable exercise probability lies interval given cumulative distribution function deﬁned satisﬁes shown figure several continuous variables denoted collectively vector deﬁne joint probability density probability theory probability falling inﬁnitesimal volume containing point given multivariate probability density must satisfy integral taken whole space also consider joint probability distributions combination discrete continuous variables note discrete variable sometimes called probability mass function regarded set probability masses concentrated allowed values sum product rules probability well bayes theorem apply equally case probability densities combinations discrete con tinuous variables instance two real variables sum product rules take form formal justiﬁcation sum product rules continuous variables feller requires branch mathematics called measure theory lies outside scope book validity seen informally however dividing real variable intervals width considering discrete probability distributionintervals taking limit turns sums integrals gives desired result 
"
4,"[probability, theory, expectations, covariances]"," one important operations involving probabilities ﬁnding weighted averages functions average value function probability distribution called expectation denoted discrete distribution given average weighted relative probabilities different values case continuous variables expectations expressed terms integration respect corresponding probability density either case given ﬁnite number points drawn probability distribution probability density expectation approximated introduction ﬁnite sum points shall make extensive use result discuss sampling methods chapter approximation becomes exact limit sometimes considering expectations functions several variables case use subscript indicate variable averaged instance denotes average function respect distribution note function also consider conditional expectation respect conditional distribution analogous deﬁnition continuous variables variance deﬁned var provides measure much variability around mean value expanding square see variance also written terms expectations exercise var particular consider variance variable given var two random variables covariance deﬁned covariance expresses extent vary together indepen dent covariance vanishes exercise case two vectors random variables covariance matrix covariance consider covariance components vector use slightly simpler notation covariance probability theory 
"
5,"[probability, theory, bayesian, probabilities]"," far chapter viewed probabilities terms frequencies random repeatable events shall refer classical frequentist interpretation probability turn general bayesian view probabilities provide quantiﬁcation uncertainty consider uncertain event example whether moon orbit around sun whether arctic ice cap disappeared end century events repeated numerous times order deﬁne notion probability earlier context boxes fruit nevertheless generally idea example quickly think polar ice melting obtain fresh evidence instance new earth observation satellite gathering novel forms diagnostic information may revise opinion rate ice loss assessment matters affect actions take instance extent endeavour reduce emission greenhouse gasses circumstances would like able quantify expression uncertainty make precise revisions uncertainty light new evidence well subsequently able take optimal actions decisions consequence achieved elegant general bayesian interpretation probability use probability represent uncertainty however hoc choice inevitable respect common sense making rational coherent inferences instance cox showed numerical values used represent degrees belief simple set axioms encoding common sense properties beliefs leads uniquely set rules manipulating degrees belief equivalent sum product rules probability provided ﬁrst rigorous proof probability theory could regarded extension boolean logic situations involving uncertainty jaynes numerous authors proposed different sets properties axioms measures uncertainty satisfy ramsey good savage definetti lindley case resulting numerical quantities behave precisely according rules probability therefore natural refer quantities bayesian probabilities ﬁeld pattern recognition helpful general 
"
6,"[thomas, bayes]"," thomas bayes born tun bridge wells clergyman well amateur scientist mathematician studied logic theology edinburgh univer sity elected fellow royal society century sues regarding probability arose connection gambling new concept insurance one particularly important problem concerned called verse probability solution proposed thomas bayes paper essay towards solving problem doctrine chances published three years death philo sophical transactions royal society fact bayes formulated theory case uniform prior pierre simon laplace independently rediscovered theory general form demonstrated broad applicability introduction probability consider example polynomial curve ﬁtting discussed section seems reasonable apply frequentist notion probability random values observed variables however would like address quantify uncertainty surrounds appropriate choice model parameters shall see bayesian perspective use machinery probability theory describe uncertainty model parameters indeed choice model bayes theorem acquires new signiﬁcance recall boxes fruit example observation identity fruit provided relevant information altered probability chosen box red one example bayes theorem used convert prior probability posterior probability incorporating evidence provided observed data shall see detail later adopt similar approach making inferences quantities parameters polynomial curve ﬁtting example capture assumptions observing data form prior probability distribution effect observed data expressed conditional probability shall see later section represented explicitly bayes theorem takes form allows evaluate uncertainty observed form posterior probability quantity right hand side bayes theorem evaluated observed data set viewed function parameter vector case called likelihood function expresses probable observed data set different settings parameter vector note likelihood probability distribution integral respect necessarily equal one given deﬁnition likelihood state bayes theorem words posterior likelihood prior quantities viewed functions denominator normalization constant ensures posterior distribution left hand side valid probability density integrates one indeed integrating sides respect express denominator bayes theorem terms prior distribution likelihood function bayesian frequentist paradigms likelihood function plays central role however manner used fundamentally dif ferent two approaches frequentist setting considered ﬁxed parameter whose value determined form estimator error bars probability theory estimate obtained considering distribution possible datasets contrast bayesian viewpoint single data set namely one actually observed uncertainty parameters expressed probability distribution widely used frequentist estimator maximum likelihood set value maximizes likelihood function corresponds choosing value probability observed data set maximized machine learning literature negative log likelihood function called error function negative logarithm monotonically creasing function maximizing likelihood equivalent minimizing error one approach determining frequentist error bars bootstrap efron hastie multiple datasets created follows suppose original data set consists data points create new data set drawing points random replacement points may replicated whereas points may absent process repeated times generate datasets size obtained sampling original data set statistical accuracy parameter estimates evaluated looking variability predictions different bootstrap datasets one advantage bayesian viewpoint inclusion prior knowledge arises naturally suppose instance fair looking coin tossed three times lands heads time classical maximum likelihood estimate probability landing heads would give implying future tosses land section heads contrast bayesian approach reasonable prior lead much less extreme conclusion much controversy debate associated relative mer frequentist bayesian paradigms helped fact unique frequentist even bayesian viewpoint instance one common criticism bayesian approach prior distribution ten selected basis mathematical convenience rather reﬂection prior beliefs even subjective nature conclusions pendence choice prior seen source difﬁculty reducing dependence prior one motivation called noninformative priors section however lead difﬁculties comparing different models indeed bayesian methods based poor choices prior give poor results high conﬁdence frequentist evaluation methods offer protection prob lems techniques cross validation remain useful areas model section comparison book places strong emphasis bayesian viewpoint reﬂecting huge growth practical importance bayesian methods past years also discussing useful frequentist concepts required although bayesian framework origins century practical application bayesian methods long time severely limited difﬁculties carrying full bayesian procedure particularly need marginalize sum integrate whole parameter space shall introduction see required order make predictions compare different models development sampling methods markov chain monte carlo discussed chapter along dramatic improvements speed memory capacity computers opened door practical use bayesian techniques pressive range problem domains monte carlo methods ﬂexible applied wide range models however computationally intensive mainly used small scale problems recently highly efﬁcient deterministic approximation schemes variational bayes expectation propagation discussed chapter developed offer complementary alternative sampling methods allowed bayesian techniques used large scale applications blei 
"
7,"[thomas, bayes, gaussian, distribution]"," shall devote whole chapter study various probability distributions key properties convenient however introduce one important probability distributions continuous variables called normal gaussian distribution shall make extensive use distribution remainder chapter indeed throughout much book case single real valued variable gaussian distribution ﬁned exp governed two parameters called mean called variance square root variance given called standard deviation reciprocal variance written called precision shall see motivation terms shortly figure shows plot gaussian distribution form see gaussian distribution satisﬁes also straightforward show gaussian normalized exercise 
"
8,"[pierre-simon, laplace]"," said laplace seriously lacking modesty one point declared best mathematician france time claim arguably true well proliﬁc mathe matics also made numerous contributions tronomy including nebular hypothesis earth thought formed condensation cooling large rotating disk gas dust published ﬁrst edition eorie analytique des probabilit laplace states probability theory nothing common sense reduced calculation work included discussion inverse probability calculation later termed bayes theorem poincar used solve problems life expectancy jurisprudence planetary masses triangulation error estimation probability theory figure plot univariate gaussian showing mean standard deviation thus satisﬁes two requirements valid probability density readily ﬁnd expectations functions gaussian distribution particular average value given exercise parameter represents average value distribution referred mean similarly second order moment follows variance given var hence referred variance parameter maximum distribution known mode gaussian mode coincides mean exercise also interested gaussian distribution deﬁned dimensional vector continuous variables given exp dimensional vector called mean matrix called covariance denotes determinant shall make use multivariate gaussian distribution brieﬂy chapter although properties studied detail section introduction figure illustration likelihood function gaussian distribution shown red curve black points note data set values likelihood function given corresponds product blue values maximizing likelihood volves adjusting mean variance gaussian maximize product suppose data set observations rep resenting observations scalar variable note using type face distinguish single observation vector valued variable denote shall suppose observations drawn independently gaussian distribution whose mean variance unknown would like determine parameters data set data points drawn independently distribution said independent identically distributed often abbreviated seen joint probability two independent events given product marginal probabilities event separately data set therefore write probability data set given form viewed function likelihood function gaussian interpreted diagrammatically figure one common criterion determining parameters probability distribution using observed data set ﬁnd parameter values maximize likelihood function might seem like strange criterion fore going discussion probability theory would seem natural maximize probability parameters given data probability data given parameters fact two criteria related shall discuss context curve ﬁtting section moment however shall determine values unknown parame ters gaussian maximizing likelihood function practice convenient maximize log likelihood function logarithm monotonically increasing function argument maximization log function equivalent maximization function taking log simpliﬁes subsequent mathematical analysis also helps numerically product large number small probabilities easily underﬂow numerical precision computer resolved computing instead sum log probabilities log likelihood probability theory function written form maximizing respect obtain maximum likelihood solution given exercise sample mean mean observed values similarly maximizing respect obtain maximum likelihood solution variance form sample variance measured respect sample mean note performing joint maximization respect case gaussian distribution solution decouples ﬁrst evaluate subsequently use result evaluate later chapter also subsequent chapters shall highlight signiﬁcant limitations maximum likelihood approach give indication problem context solutions maximum likelihood parameter settings univariate gaussian distribution particular shall show maximum likelihood approach systematically underestimates variance distribution example phenomenon called bias related problem ﬁtting encountered context polynomial curve ﬁtting section ﬁrst note maximum likelihood solutions functions data set values consider expectations quantities respect data set values come gaussian distribution parameters straightforward show exercise average maximum likelihood estimate obtain correct mean underestimate true variance factor intuition behind result given figure follows following estimate variance parameter unbiased introduction figure illustration bias arises using maximum likelihood determine variance gaussian green curve shows true gaussian distribution data generated three red curves show gaussian distributions obtained ﬁtting three datasets consisting two data points shown blueing maximum likelihood results averaged across three datasets mean correct variance systematically estimated measured relative sample mean relative true mean section shall see result arises automatically adopt bayesian approach note bias maximum likelihood solution becomes less signiﬁcant number data points increases limit maximum likelihood solution variance equals true variance distribution generated data practice anything small bias prove serious problem however throughout book shall interested complex models many parameters bias problems asso ciated maximum likelihood much severe fact shall see issue bias maximum likelihood lies root ﬁtting problem encountered earlier context polynomial curve ﬁtting 
"
9,"[pierre-simon, laplace, curve, ﬁtting, re-visited]"," seen problem polynomial curve ﬁtting expressed terms error minimization return curve ﬁtting example view section probabilistic perspective thereby gaining insights error functions regularization well taking towards full bayesian treatment goal curve ﬁtting problem able make predictions target variable given new value input variable basis set training data comprising input values corresponding target values express uncertainty value target variable using probability distribution purpose shall assume given value corresponding value gaussian distribution mean equal value polynomial curve given thus consistency notation later chapters deﬁned precision parameter corresponding inverse variance distribution illustrated schematically figure probability theory figure schematic illustration gaussian conditional distribution given given mean given polynomial function precision given parameter related variance use training data determine values unknown parameters maximum likelihood data assumed drawn independently distribution likelihood function given case simple gaussian distribution earlier convenient maximize logarithm likelihood function substituting form gaussian distribution given obtain log likelihood function form consider ﬁrst determination maximum likelihood solution polynomial coefﬁcients denoted determined maxi mizing respect purpose omit last two terms right hand side depend also note scaling log likelihood positive constant coefﬁcient alter location maximum respect replace coefﬁcient finally instead maximizing log likelihood equivalently minimize negative log likelihood therefore see maximizing likelihood equivalent far determining concerned minimizing sum squares error function deﬁned thus sum squares error function arisen consequence maximizing likelihood assumption gaussian noise distribution also use maximum likelihood determine precision parameter gaussian conditional distribution maximizing respect gives introduction ﬁrst determine parameter vector governing mean sub sequently use ﬁnd precision case simple gaussian distribution section determined parameters make predictions new values probabilistic model expressed terms predictive distribution gives probability distribution rather simply point estimate obtained substituting maximum likelihood parameters give let take step towards bayesian approach introduce prior distribution polynomial coefﬁcients simplicity let consider gaussian distribution form exp precision distribution total number elements vector order polynomial variables control distribution model parameters called hyperparameters using bayes theorem posterior distribution proportional product prior distribution likelihood function determine ﬁnding probable value given data words maximizing posterior distribution technique called maximum posterior simply map taking negative logarithm combining ﬁnd maximum posterior given minimum thus see maximizing posterior distribution equivalent minimizing regularized sum squares error function encountered earlier form regularization parameter given 
"
10,"[pierre-simon, laplace, bayesian, curve, ﬁtting]"," although included prior distribution far still making point estimate yet amount bayesian treatment fully bayesian approach consistently apply sum product rules probability requires shall see shortly integrate values marginalizations lie heart bayesian methods pattern recognition probability theory curve ﬁtting problem given training data along new test point goal predict value therefore wish evaluate predictive distribution shall assume parameters ﬁxed known advance later chapters shall discuss parameters inferred data bayesian setting bayesian treatment simply corresponds consistent application sum product rules probability allow predictive distribution written form given omitted dependence simplify notation posterior distribution parameters found normalizing right hand side shall see section problems curve ﬁtting example posterior distribution gaussian evaluated analytically similarly integration also performed analytically result predictive distribution given gaussian form mean variance given matrix given unit matrix deﬁned vector elements see variance well mean predictive distribution dependent ﬁrst term represents uncertainty predicted value due noise target variables expressed already maximum likelihood predictive distribution however second term arises uncertainty parameters consequence bayesian treatment predictive distribution synthetic sinusoidal regression problem illustrated figure introduction figure predictive distribution resulting bayesian treatment polynomial curve ﬁtting using polynomial ﬁxed parameters corresponding known noise variance red curve denotes mean predictive distribution red region corresponds stan dard deviation around mean 
"
11,"[model, selection]"," example polynomial curve ﬁtting using least squares saw optimal order polynomial gave best generalization order polynomial controls number free parameters model thereby governs model complexity regularized least squares regularization coefﬁcient also controls effective complexity model whereas complex models mixture distributions neural networks may multiple parameters governing complexity practical application need determine values parameters principal objective usually achieve best predictive performance new data furthermore well ﬁnding appropriate values complexity parameters within given model may wish consider range different types model order ﬁnd best one particular application already seen maximum likelihood approach perfor mance training set good indicator predictive performance seen data due problem ﬁtting data plentiful one approach simply use available data train range models given model range values complexity parameters compare independent data sometimes called validation set select one best predictive performance model design iterated many times using limited size data set ﬁtting validation data occur may necessary keep aside third test set performance selected model ﬁnally evaluated many applications however supply data training testing limited order build good models wish use much available data possible training however validation set small give relatively noisy estimate predictive performance one solution dilemma use cross validation illustrated figure allows proportion available data used training making use curse dimensionality figure technique fold cross validation illus trated case involves taking available data partitioning groups simplest case equal size groups used train set models evaluated maining group procedure repeated possible choices held group indicated red blocks perfor mance scores runs averaged run data assess performance data particularly scarce may appropriate consider case total number data points gives leave one technique one major drawback cross validation number training runs must performed increased factor prove problematic models training computationally expensive problem techniques cross validation use separate data assess performance might multiple complexity parameters single model stance might several regularization parameters exploring combinations settings parameters could worst case require number training runs exponential number parameters clearly need better approach ideally rely training data allow multiple hyperparameters model types compared single training run foreneed ﬁnd measure performance depends training data suffer bias due ﬁtting historically various information criteria proposed attempt correct bias maximum likelihood addition penalty term compensate ﬁtting complex models example akaike information criterion aic akaike chooses model quan tity largest best log likelihood number adjustable parameters model variant quantity called bayesian information criterion bic discussed section criteria take account uncertainty model parameters however practice tend favour overly simple models therefore turn section fully bayesian approach shall see complexity penalties arise natural principled way 
"
12,"[curse, dimensionality]"," polynomial curve ﬁtting example one input variable practical applications pattern recognition however deal spaces introduction figure scatter plot oil ﬂow data input variables red denotes homoge nous class green denotes annular class blue denotes laminar class goal classify new test point noted high dimensionality comprising many input variables discuss poses serious challenges important factor inﬂuencing design pattern recognition techniques order illustrate problem consider synthetically generated data set representing measurements taken pipeline containing mixture oil ter gas bishop james three materials present one three different geometrical conﬁgurations known homogenous annular laminar fractions three materials also vary data point comprises dimensional input vector consisting measurements taken gamma ray densitometers measure attenuation gamma rays passing along narrow beams pipe data set described detail appendix figure shows points data set plot showing two measurements remaining ten input values ignored purposes illustration data point labelled according three geomet rical classes belongs goal use data training set order able classify new observation one denoted cross figure observe cross surrounded numerous red points might suppose belongs red class however also plenty green points nearby might think could instead belong green class seems unlikely belongs blue class intuition identity cross determined strongly nearby points training set less strongly distant points fact intuition turns reasonable discussed fully later chapters turn intuition learning algorithm one simple approach would divide input space regular cells indicated figure given test point wish predict class ﬁrst decide cell belongs ﬁnd training data points curse dimensionality figure illustration simple approach solution classiﬁcation problem input space divided cells new test point assigned class majority number rep resentatives cell test point shall see shortly simplistic approach severe shortcomings fall cell identity test point predicted class largest number training points cell test point probabilities broken random numerous problems naive approach one vere becomes apparent consider extension problems larger numbers input variables corresponding input spaces higher dimensionality origin problem illustrated figure shows divide region space regular cells number cells grows exponentially dimensionality space problem exponentially large number cells would need exponentially large quantity training data order ensure cells empty clearly hope applying technique space variables need ﬁnd sophisticated approach gain insight problems high dimensional spaces returning example polynomial curve ﬁtting considering would section figure illustration curse dimensionality showing number regions regular grid grows exponentially dimensionality space clarity subset cubical regions shown introduction extend approach deal input spaces several variables input variables general polynomial coefﬁcients order would take form ijk increases number independent coefﬁcients coefﬁcients independent due interchange symmetries amongst variables grows pro portionally practice capture complex dependencies data may need use higher order polynomial polynomial order growth number coefﬁcients like although power law growth exercise rather exponential growth still points method becoming rapidly unwieldy limited practical utility geometrical intuitions formed life spent space three mensions fail badly consider spaces higher dimensionality simple example consider sphere radius space dimensions ask fraction volume sphere lies radius evaluate fraction noting volume sphere radius dimensions must scale write constant depends thus required fraction given exercise plotted function various values figure see large fraction tends even small values thus spaces high dimensionality volume sphere concentrated thin shell near surface example direct relevance pattern recognition consider behaviour gaussian distribution high dimensional space transform cartesian polar coordinates integrate directional variables obtain expression density function radius origin exercise thus probability mass inside thin shell thickness located radius distribution plotted various values figure see large probability mass gaussian concentrated thin shell severe difﬁculty arise spaces many dimensions sometimes called curse dimensionality bellman book shall make tensive use illustrative examples involving input spaces one two dimensions makes particularly easy illustrate techniques graphically reader warned however intuitions developed spaces low dimensionality generalize spaces many dimensions curse dimensionality figure plot fraction volume sphere lying range various values dimensionality volume fraction although curse dimensionality certainly raises important issues pattern recognition applications prevent ﬁnding effective techniques applicable high dimensional spaces reasons twofold first real data often conﬁned region space lower effective dimension ality particular directions important variations target variables occur may conﬁned second real data typically exhibit smoothness properties least locally part small changes input variables produce small changes target variables ploit local interpolation like techniques allow make predictions target variables new values input variables successful pattern recognition tech niques exploit one properties consider example application manufacturing images captured identical planar objects con veyor belt goal determine orientation image point figure plot probability density respect radius gaussian distribution various values dimensionality high dimensional space probability mass gaussian cated within thin shell speciﬁc radius introduction high dimensional space whose dimensionality determined number pixels objects occur different positions within image different orientations three degrees freedom variability images set images live three dimensional manifold embedded within high dimensional space due complex relationships object position orientation pixel intensities manifold highly nonlinear goal learn model take input image output orientation object irrespective position one degree freedom variability within manifold signiﬁcant 
"
13,"[decision, theory]"," seen section probability theory provides consistent mathematical framework quantifying manipulating uncertainty turn discussion decision theory combined probability theory allows make optimal decisions situations involving uncertainty encountered pattern recognition suppose input vector together corresponding vector target variables goal predict given new value regression problems comprise continuous variables whereas classiﬁcation problems represent class labels joint probability distribution provides complete summary uncertainty associated variables determination set training data example inference typically difﬁcult problem whose solution forms subject much book practical application however must often make speciﬁc prediction value generally take speciﬁc action based understanding values likely take aspect subject decision theory consider example medical diagnosis problem taken ray image patient wish determine whether patient cancer case input vector set pixel intensities image output variable represent presence cancer denote class absence cancer denote class might instance choose binary variable corresponds class corresponds class shall see later choice label values particularly convenient probabilistic models general inference problem involves determining joint distribution equivalently gives complete probabilistic description situation although useful informative quantity end must decide either give treatment patient would like choice optimal appropriate sense duda hart decision step subject decision theory tell make optimal decisions given appropriate probabilities shall see decision stage generally simple even trivial solved inference problem give introduction key ideas decision theory required decision theory rest book background well detailed accounts found berger bather giving detailed analysis let ﬁrst consider informally might expect probabilities play role making decisions obtain ray image new patient goal decide two classes assign image interested probabilities two classes given image given using bayes theorem probabilities expressed form note quantities appearing bayes theorem obtained joint distribution either marginalizing conditioning respect appropriate variables interpret prior probability class corresponding posterior probability thus repre sents probability person cancer take ray measurement similarly corresponding probability revised using bayes theorem light information contained ray aim minimize chance assigning wrong class intuitively would choose class higher posterior probability show intuition correct also discuss general criteria making decisions 
"
14,"[decision, theory, minimizing, misclassiﬁcation, rate]"," suppose goal simply make misclassiﬁcations possible need rule assigns value one available classes rule divide input space regions called decision regions one class points assigned class boundaries decision regions called decision boundaries decision surfaces note decision region need contiguous could comprise number disjoint regions shall encounter examples decision boundaries decision regions later chapters order ﬁnd optimal decision rule consider ﬁrst case two classes cancer problem instance mistake occurs input vector belonging class assigned class vice versa probability occurring given mistake free choose decision rule assigns point one two classes clearly minimize mistake arrange assigned whichever class smaller value integrand thus given value assign class product rule probability factor common terms restate result saying minimum introduction figure schematic illustration joint probabilities two classes plotted together decision boundary values classiﬁed class hence belong decision region whereas points classiﬁed belong errors arise blue green red regions errors due points class misclassiﬁed represented sum red green regions conversely points region errors due points class misclassiﬁed represented blue region vary location decision boundary combined areas blue green regions remains constant whereas size red region varies optimal choice curves cross corresponding case red region disappears equivalent minimum misclassiﬁcation rate decision rule assigns value class higher posterior probability probability making mistake obtained value assigned class posterior probability largest result illustrated two classes single input variable figure general case classes slightly easier maximize probability correct given correct maximized regions chosen assigned class largest using product rule noting factor common terms see assigned class largest posterior probability decision theory figure example loss matrix elements cancer treatment problem rows correspond true class whereas columns cor respond assignment class made decision criterion cancer normal cancer normal 
"
15,"[decision, theory, minimizing, expected, loss]"," many applications objective complex simply mini mizing number misclassiﬁcations let consider medical diagnosis problem note patient cancer incorrectly diagnosed cancer consequences may patient distress plus need investigations conversely patient cancer diagnosed healthy result may premature death due lack treatment thus consequences two types mistake dramatically different would clearly better make fewer mistakes second kind even expense making mistakes ﬁrst kind formalize issues introduction loss function also called cost function single overall measure loss incurred taking available decisions actions goal minimize total loss incurred note authors consider instead utility function whose value aim maximize equivalent concepts take utility simply negative loss throughout text shall use loss function convention suppose new value true class assign class may may equal incur level loss denote view element loss matrix instance cancer example might loss matrix form shown figure particular loss matrix says loss incurred correct decision made loss healthy patient diagnosed cancer whereas loss patient cancer diagnosed healthy optimal solution one minimizes loss function however loss function depends true class unknown given input vector uncertainty true class expressed joint probability distribution seek instead minimize average loss average computed respect distribution given assigned independently one decision regions goal choose regions order minimize expected loss implies minimize use product rule eliminate common factor thus decision rule minimizes expected loss one assigns introduction figure illustration reject option inputs larger two poste rior probabilities less equal threshold rejected reject region new class quantity minimum clearly trivial know posterior class proba bilities reject option seen classiﬁcation errors arise regions input space largest posterior probabilities signiﬁcantly less unity equivalently joint distributions comparable values regions relatively uncertain class membership applications appropriate avoid making decisions difﬁcult cases anticipation lower error rate examples classiﬁcation decisionmade known reject option example hypothetical medical illustration may appropriate use automatic system classify ray images little doubt correct class leaving human expert classify ambiguous cases achieve introducing threshold rejecting inputs largest posterior probabilities less equal illustrated case two classes single continuous input variable figure note setting ensure examples rejected whereas classes setting ensure examples rejected thus fraction examples get rejected controlled value easily extend reject criterion minimize expected loss loss matrix given taking account loss incurred reject decision made exercise inference decision broken classiﬁcation problem two separate stages inference stage use training data learn model decision theory subsequent decision stage use posterior probabilities make timal class assignments alternative possibility would solve problems together simply learn function maps inputs directly decisions function called discriminant function fact identify three distinct approaches solving decision problems used practical applications given decreasing order complexity first solve inference problem determining class conditional densities class individually also separately infer prior class probabilities use bayes theorem form ﬁnd posterior class probabilities usual denominator bayes theorem found terms quantities appearing numerator equivalently model joint distribution directly normalize obtain posterior probabilities found posterior probabilities use decision theory determine class membership new input approaches explicitly implicitly model distribution inputs well outputs known generative models sampling possible generate synthetic data points input space first solve inference problem determining posterior class probabilities subsequently use decision theory assign new one classes approaches model posterior probabilities directly called discriminative models find function called discriminant function maps input directly onto class label instance case two class problems might binary valued represents class represents class case probabilities play role let consider relative merits three alternatives approach demanding involves ﬁnding joint distribution many applications high dimensionality consequently may need large training set order able determine class conditional densities reasonable accuracy note class priors often esti mated simply fractions training set data points classes one advantage approach however also allows marginal density data determined useful detecting new data points low probability model predictions may introduction class densities figure example class conditional densities two classes single input variable left plot together corresponding posterior probabilities right plot note left hand mode class conditional density shown blue left plot effect posterior probabilities vertical green line right plot shows decision boundary gives minimum misclassiﬁcation rate low accuracy known outlier detection novelty detection bishop tarassenko however wish make classiﬁcation decisions wasteful computational resources excessively demanding data ﬁnd joint distribution fact really need posterior probabilities obtained directly approach indeed class conditional densities may contain lot structure little effect posterior probabilities illustrated figure much interest exploring relative merits generative discriminative approaches machine learning ﬁnding ways combine jebara lasserre even simpler approach use training data ﬁnd discriminant function maps directly onto class label thereby combining inference decision stages single learning problem example figure would correspond ﬁnding value shown vertical green line decision boundary giving minimum probability misclassiﬁcation option however longer access posterior probabilities many powerful reasons wanting compute posterior probabilities even subsequently use make decisions include minimizing risk consider problem elements loss matrix subjected revision time time might occur ﬁnancial decision theory application know posterior probabilities trivially revise minimum risk decision criterion modifying appropriately discriminant function change loss matrix would require return training data solve classiﬁcation problem afresh reject option posterior probabilities allow determine rejection criterion minimize misclassiﬁcation rate generally expected loss given fraction rejected data points compensating class priors consider medical ray problem suppose collected large number ray images general population use training data order build automated screening system cancer rare amongst general population might ﬁnd say every examples corresponds presence cer used data set train adaptive model could run severe difﬁculties due small proportion cancer class instance classiﬁer assigned every point normal class would already achieve accuracy would difﬁcult avoid trivial solution also even large data set contain examples ray images corre sponding cancer learning algorithm exposed broad range examples images hence likely generalize well balanced data set selected equal numbers exam ples classes would allow ﬁnd accurate model however compensate effects modiﬁcations training data suppose used modiﬁed data set found models posterior probabilities bayes theorem see posterior probabilities proportional prior probabilities interpret fractions points class therefore simply take posterior probabilities obtained artiﬁcially balanced data set ﬁrst divide class fractions data set multiply class fractions population wish apply model finally need normalize ensure new posterior probabilities sum one note procedure cannot applied learned discriminant function directly instead determining posterior probabilities combining models complex applications may wish break problem number smaller subproblems tackled sep arate module example hypothetical medical diagnosis problem may information available say blood tests well ray ages rather combine heterogeneous information one huge input space may effective build one system interpret ray images different one interpret blood data long two models gives posterior probabilities classes combine outputs systematically using rules probability one simple way assume class separately distributions inputs ray images denoted blood data denoted introduction independent example conditional independence property indepen section dence holds distribution conditioned class posterior probability given ray blood data given thus need class prior probabilities easily estimate fractions data points class need normalize resulting posterior probabilities sum one particular conditional independence assumption example naive bayes model section note joint marginal distribution typically factorize model shall see later chapters construct models combining data require conditional independence assumption 
"
16,"[decision, theory, loss, functions, regression]"," far discussed decision theory context classiﬁcation prob lems turn case regression problems curve ﬁtting example discussed earlier decision stage consists choosing speciﬁc esti section mate value input suppose incur loss average expected loss given common choice loss function regression problems squared loss given case expected loss written goal choose minimize assume completely ﬂexible function formally using calculus variations appendix give solving using sum product rules probability obtain decision theory figure regression function minimizes expected squared loss given mean conditional distri bution conditional average conditioned known regression function result illustrated figure readily extended mul tiple target variables represented vector case optimal solution conditional average exercise also derive result slightly different way also shed light nature regression problem armed knowledge optimal solution conditional expectation expand square term follows keep notation uncluttered use denote substituting loss function performing integral see cross term vanishes obtain expression loss function form function seek determine enters ﬁrst term minimized equal case term vanish simply result derived previously shows optimal least squares predictor given conditional mean second term variance distribution averaged represents intrinsic variability target data regarded noise independent represents irreducible minimum value loss function classiﬁcation problem either determine appropriate prob abilities use make optimal decisions build models make decisions directly indeed identify three distinct approaches solving regression problems given order decreasing complexity first solve inference problem determining joint density normalize ﬁnd conditional density ﬁnally marginalize ﬁnd conditional mean given introduction first solve inference problem determining conditional density subsequently marginalize ﬁnd conditional mean given find regression function directly training data relative merits three approaches follow lines classiﬁcation problems squared loss possible choice loss function regression indeed situations squared loss lead poor results need develop sophisticated approaches important example concerns situations conditional distribution multimodal often arises solution inverse problems consider brieﬂy one simple section generalization squared loss called minkowski loss whose expectation given reduces expected squared loss function plotted various values figure minimum given conditional mean conditional median conditional mode exercise 
"
17,"[information, theory]"," chapter discussed variety concepts probability theory decision theory form foundations much subsequent discussion book close chapter introducing additional concepts ﬁeld information theory also prove useful development pattern recognition machine learning techniques shall focus key concepts refer reader elsewhere detailed discussions viterbi omura cover thomas mackay begin considering discrete random variable ask much information received observe speciﬁc value variable amount information viewed degree surprise learning value told highly improbable event occurred received information told likely event occurred knew event certain happen would receive information measure information content therefore depend probability distribution therefore look quantity monotonic function probability expresses information content form found noting two events unrelated information gain observing sum information gained separately two unrelated events statistically independent two relationships easily shown must given logarithm exercise information theory figure plots quantity various values log negative sign ensures information positive zero note low probability events correspond high information content choice basis logarithm arbitrary moment shall adopt convention prevalent information theory using logarithms base case shall see shortly units bits binary digits suppose sender wishes transmit value random variable receiver average amount information transmit process obtained taking expectation respect distribution given log important quantity called entropy random variable note lim shall take whenever encounter value far given rather heuristic motivation deﬁnition informa introduction corresponding entropy show deﬁnitions indeed possess useful properties consider random variable possible states equally likely order communicate value receiver would need transmit message length bits notice entropy variable given log bits consider example cover thomas variable pos sible states respective probabilities given entropy case given log bits see nonuniform distribution smaller entropy uniform one shall gain insight shortly discuss interpretation entropy terms disorder moment let consider would transmit identity variable state receiver could using bit number however take advantage nonuniform distribution using shorter codes probable events expense longer codes less probable events hope getting shorter average code length done representing states using instance following set code strings average length code transmitted average code length bits entropy random variable note shorter code strings cannot used must possible disambiguate concatenation strings component parts instance decodes uniquely state sequence relation entropy shortest coding length general one noiseless coding theorem shannon states entropy lower bound number bits needed transmit state random variable shall switch use natural logarithms deﬁning tropy provide convenient link ideas elsewhere book case entropy measured units nats instead bits differ simply factor introduced concept entropy terms average amount information needed specify state random variable fact concept entropy much earlier origins physics introduced context equilibrium thermodynamics later given deeper interpretation measure disorder developments statistical mechanics understand alternative view entropy considering set identical objects divided amongst set bins objects bin consider information theory number different ways allocating objects bins ways choose ﬁrst object ways choose second object leading total ways allocate objects bins pronounced factorial denotes product however wish distinguish rearrangements objects within bin ways reordering objects total number ways allocating objects bins given called multiplicity entropy deﬁned logarithm multiplicity scaled appropriate constant consider limit fractions held ﬁxed apply stirling approximation gives lim used lim probability object assigned bin physics terminology speciﬁc rangements objects bins called microstate overall distribution occupation numbers expressed ratios called macrostate multiplicity also known weight macrostate interpret bins states discrete random variable entropy random variable distributions sharply peaked around values relatively low entropy whereas spread evenly across many values higher entropy illustrated figure entropy nonnegative equal minimum value one maximum entropy conﬁguration found maximizing using lagrange multiplier enforce normalization constraint appendix probabilities thus maximize introduction probabilities probabilities figure histograms two probability distributions bins illustrating higher value entropy broader distribution largest entropy would arise uniform distribution would give ﬁnd equal given total number states corresponding value entropy result also derived jensen inequality discussed shortly verify stationary point indeed maximum exercise evaluate second derivative entropy gives elements identity matrix extend deﬁnition entropy include distributions con tinuous variables follows first divide bins width assuming continuous mean value theorem weisstein tells bin must exist value quantize continuous variable assigning value value whenever falls bin probability observing value gives discrete distribution entropy takes form used follows omit second term right hand side consider limit information theory ﬁrst term right hand side approach integral limit lim quantity right hand side called differential entropy see discrete continuous forms entropy differ quantity diverges limit reﬂects fact specify continuous variable precisely requires large number bits density deﬁned multiple continuous variables denoted collectively vector differential entropy given case discrete distributions saw maximum entropy con ﬁguration corresponded equal distribution probabilities across possible states variable let consider maximum entropy conﬁguration continuous variable order maximum well deﬁned nec essary constrain ﬁrst second moments well preserving normalization constraint therefore maximize differential entropy 
"
18,"[ludwig, boltzmann]"," ludwig eduard boltzmann austrian physicist created ﬁeld statistical mechanics prior boltzmann concept tropy already known classical thermodynamics quantiﬁes fact take energy system energy typically available useful work boltzmann showed thermodynamic entropy macroscopic quantity could related statistical properties micro scopic level expressed famous equation represents number possible microstates macrostate units joules per kelvin known boltzmann constant boltzmann ideas disputed many scientists day one dif ﬁculty saw arose second law thermo dynamics states entropy closed system tends increase time contrast microscopic level classical newtonian equations physics reversible found difﬁcult see latter could explain mer fully appreciate boltzmann arguments statistical nature con cluded entropy could never decrease time simply overwhelming probability would generally increase boltzmann even long running dispute editor leading german physics journal refused let refer atoms molecules anything convenient oretical constructs continued attacks work lead bouts depression eventually committed suicide shortly boltzmann death new experiments perrin colloidal suspensions veri ﬁed theories conﬁrmed value boltzmann constant equation carved boltzmann tombstone introduction three constraints constrained maximization performed using lagrange multipliers appendix maximize following functional respect using calculus variations set derivative functional zero giving appendix exp lagrange multipliers found back substitution result three constraint equations leading ﬁnally result exercise exp distribution maximizes differential entropy gaussian note constrain distribution nonnegative maximized entropy however resulting distribution indeed nonnegative see hindsight constraint necessary evaluate differential entropy gaussian obtain exercise thus see entropy increases distribution becomes broader increases result also shows differential entropy unlike discrete entropy negative suppose joint distribution draw pairs values value already known additional information needed specify corresponding value given thus average additional information needed specify written information theory called conditional entropy given easily seen using product rule conditional entropy satisﬁes relation exercise differential entropy differential tropy marginal distribution thus information needed describe given sum information needed describe alone plus additional information required specify given 
"
19,"[ludwig, boltzmann, relative, entropy, mutual, information]"," far section introduced number concepts information theory including key notion entropy start relate ideas pattern recognition consider unknown distribution suppose modelled using approximating distribution use construct coding scheme purpose transmitting values receiver average additional amount information nats required specify value assuming choose efﬁcient coding scheme result using instead true distribution given known relative entropy kullback leibler divergence diver gence kullback leibler distributions note symmetrical quantity say show kullback leibler divergence satisﬁes equality ﬁrst introduce concept convex functions function said convex property every chord lies function shown figure value interval written form corresponding point chord given 
"
20,"[claude, shannon]"," graduating michigan mit shannon joined atamp bell telephone laboratories paper mathematical theory communication published bell system technical journal laid foundations modern information ory paper introduced word bit concept information could sent stream paved way communications revo lution said von neumann recommended shannon use term entropy cause similarity quantity used physics also nobody knows entropy really discussion always advan tage introduction figure convex function one chord corresponding value function convexity implies concave using technique proof induction show convex function satisﬁes set points result known jensen inequality interpret probability distribution discrete variable taking values written denotes expectation continuous variables jensen inequality takes form apply jensen inequality form kullback leibler divergence give information theory used fact convex function together malization condition fact strictly convex function equality hold thus terpret kullback leibler divergence measure dissimilarity two distributions see intimate relationship data compression density estimation problem modelling unknown probability distribution efﬁcient compression achieved know true distri bution use distribution different true one must necessarily less efﬁcient coding average additional information must transmitted least equal kullback leibler divergence tween two distributions suppose data generated unknown distribution wish model try approximate distribution using parametric distribution governed set adjustable parameters example multivariate gaussian one way determine minimize kullback leibler divergence respect cannot directly know suppose however observed ﬁnite set training points drawn expectation respect approximated ﬁnite sum points using second term right hand side independent ﬁrst term negative log likelihood function distribution evaluated using training set thus see minimizing kullback leibler divergence equivalent maximizing likelihood function consider joint distribution two sets variables given sets variables independent joint distribution factorize product marginals variables independent gain idea whether close indepen dent considering kullback leibler divergence joint distribution product marginals given called mutual information variables properties kullback leibler divergence see equality independent using sum product rules probability see mutual information related conditional entropy exercise introduction posterior distribu 
"
21,[exercises]," consider sum squares error function given function given polynomial show coefﬁcients minimize error function given solution following set linear equations sufﬁx denotes index component whereas denotes raised power minimize regularized sum squares error function given suppose three coloured boxes red blue green consider probability density deﬁned continuous vari maximum density general related location maximum density simple functional relation consequence jacobian factor shows maximum using deﬁnition show var satisﬁes exercises show two variables independent covariance zero exercise prove normalization condition univariate gaussian consider integral exp evaluate ﬁrst writing square form exp show performing integrals taking square root sides obtain finally use result show gaussian distribution normalized using change variables verify univariate gaussian respect verify gaussian satisﬁes finally show holds show mode maximum gaussian distribution suppose two variables statistically independent show mean variance sum satisﬁes var var var equal zero verify results introduction using results show denote data points sampled gaussian distribution mean variance satisﬁes otherwise hence prove results replaced true value show arbitrary square matrix elements written form symmetric anti symmetric matrices respectively satisfying show chosen symmetric elements matrix chosen indepen dently show number independent parameters matrix given exercise next explore number indepen order term polynomial dimensions form coefﬁcients comprise elements number independent begin showing redundancy coefﬁcients removed rewriting order term form exercises note precise relationship coefﬁcients coefﬁcients need made explicit use result show number independent parameters appear order satisﬁes following recursion relation next use proof induction show following result holds done ﬁrst proving result arbitrary making use result assuming correct dimension verifying correct dimension finally use two previous results together proof induction show ﬁrst show result true value comparison result exercise make use together show result holds order also hold order exercise proved result number independent parameters order term dimensional polynomial ﬁnd expression total number independent parameters terms including order first show satisﬁes number independent parameters term order make use result together proof induction show done ﬁrst proving result holds arbitrary assuming holds order hence showing holds order finally make use stirling approximation form large show consider cubic polynomial dimensions evaluate numerically total number independent parameters correspond typical small scale medium scale machine learning applications introduction gamma function deﬁned use result derive expression surface area volume sphere unit radius dimensions finally use results show reduce usual expressions consider sphere radius dimensions together concentric volume sphere volume cube make use stirling formula form valid therefore goes results see space high exercises exercise explore behaviour gaussian distribution high dimensional spaces consider gaussian distribution dimensions given exp wish ﬁnd density respect radius polar coordinates direction variables integrated show integral probability density thin shell radius thickness given exp surface area unit sphere dimensions show function single stationary point located large considering show large exp shows maximum radial probability density also decays exponentially away maximum length scale already seen large see probability mass concentrated thin shell large radius finally show probability density larger origin radius factor exp therefore see probability mass high dimensional gaussian distribution located different radius region high probability density property distributions spaces high dimensionality important consequences consider bayesian inference model parameters later chapters consider two nonnegative numbers show use result show decision regions two class classiﬁcation problem chosen minimize probability misclassiﬁcation probability satisfy mistake given loss matrix elements expected risk minimized choose class minimizes verify loss matrix given elements identity matrix reduces criterion choosing class largest posterior probability interpretation form loss matrix derive criterion minimizing expected loss general loss matrix general prior probabilities classes introduction consider classiﬁcation problem loss incurred input vector class classiﬁed belonging class given loss matrix loss incurred selecting reject option relationship rejection threshold consider generalization squared loss function single target variable case multiple target variables described vector given show result reduces case single target variable consider expected loss regression problems loss show solution represents conditional also show minimum expected loss hence induction positive integer hence show also positive integer implies positive rational number hence consider state discrete random variable use jensen evaluate kullback leibler divergence two gaussians exercises table joint distribution two binary variables used exercise consider two variables joint distribution show differential entropy pair variables satisﬁes equality statistically independent consider vector continuous variables distribution corre sponding entropy suppose make nonsingular linear transformation obtain new variable show corresponding entropy given denotes determinant suppose conditional entropy two discrete random variables zero show values variable must function words one value use calculus variations show stationary point functional given use constraints eliminate lagrange multipliers hence show maximum entropy solution given gaussian use results show entropy univariate gaussian given strictly convex function deﬁned one every chord lies function show equivalent condition second derivative function positive using deﬁnition together product rule probability prove result using proof induction show inequality convex functions implies result consider two binary variables joint distribution given table evaluate following quantities draw diagram show relationship various quantities introduction applying jensen inequality show arith metic mean set real numbers never less geometrical mean using sum product rules probability show mutual information satisﬁes relation 
"
22,"[probability, distributions]"," chapter emphasized central role played probability theory solution pattern recognition problems turn exploration particular examples probability distributions properties welling great interest right distributions form building blocks complex models used extensively throughout book distributions introduced chapter also serve another important purpose namely provide opportunity discuss key statistical concepts bayesian inference context simple models encounter complex situations later chapters one role distributions discussed chapter model prob ability distribution random variable given ﬁnite set observations problem known density estimation purposes chapter shall assume data points independent identically distributed emphasized problem density estimation fun probability distributions damentally ill posed inﬁnitely many probability distributions could given rise observed ﬁnite data set indeed distribution nonzero data points potential candidate issue choosing appropriate distribution relates problem model selection already encountered context polynomial curve ﬁtting chapter central issue pattern recognition begin considering binomial multinomial distributions discrete random variables gaussian distribution continuous random variables speciﬁc examples parametric distributions called governed small number adaptive parameters mean variance case gaussian example apply models problem density estimation need procedure determining suitable values parameters given observed data set frequentist treatment choose speciﬁc values parameters optimizing criterion likelihood function contrast bayesian treatment introduce prior distributions parameters use bayes theorem compute corresponding posterior distribution given observed data shall see important role played conjugate priors lead posterior distributions functional form prior fore lead greatly simpliﬁed bayesian analysis example conjugate prior parameters multinomial distribution called dirichlet distribution conjugate prior mean gaussian another gaussian distributions examples exponential family distributions possess number important properties discussed detail one limitation parametric approach assumes speciﬁc functional form distribution may turn inappropriate particular application alternative approach given nonparametric density estimation methods form distribution typically depends size data set models still contain parameters control model complexity rather form distribution end chapter considering three nonparametric methods based respectively histograms nearest neighbours kernels 
"
23,"[probability, distributions, binary, variables]"," begin considering single binary random variable example might describe outcome ﬂipping coin representing heads representing tails imagine damaged coin probability landing heads necessarily landing tails probability denoted parameter binary variables follows probability distribution therefore written form bern known bernoulli distribution easily veriﬁed distribution exercise normalized mean variance given var suppose data set observed values sum sum provides example section respect equal zero obtain maximum likelihood estimator 
"
24,"[probability, distributions, binary, variables, jacob, bernoulli]"," probability distributions figure suppose ﬂip coin say times happen observe heads case maximum likelihood result would also work distribution number observations order obtain bin number ways choosing objects total identical objects figure shows plot binomial distribution mean variance binomial distribution found using observation mean variance binary variables given respectively bin var bin results also proved directly using calculus exercise 
"
25,"[probability, distributions, binary, variables, beta, distribution]"," seen maximum likelihood setting parameter bernoulli distribution hence binomial distribution given fraction observations data set already noted give severely ﬁtted results small datasets order develop bayesian treatment problem need introduce prior distribution parameter consider form prior distribution simple interpretation well useful analytical properties motivate prior note likelihood function takes form product factors form choose prior proportional powers posterior distribution proportional product prior likelihood function functional form prior property called conjugacy see several examples later chapter therefore choose prior called beta distribution given beta gamma function deﬁned coefﬁcient ensures beta distribution normalized exercise beta mean variance beta distribution given exercise var parameters often called hyperparameters control distribution parameter figure shows plots beta distribution various values hyperparameters posterior distribution obtained multiplying beta prior binomial likelihood function normalizing keeping factors depend see posterior distribution form probability distributions figure plots beta distribution beta given function various values hyperparameters therefore corresponds number tails coin see effect observing data set observations binary variables prior likelihood function posterior figure illustration one step sequential bayesian inference prior given beta distribution parameters likelihood function given corresponds single observation posterior given beta distribution parameters see sequential approach learning arises naturally adopt section goal predict best outcome next trial must evaluate predictive distribution given observed data set sum product rules probability takes form using result posterior distribution together result mean beta distribution obtain result reduces probability distributions exercise figure see number observations increases fact might wonder whether address take frequentist view bayesian learning show observed data set scribed joint distribution following result exercise says posterior mean averaged distribution generating data equal prior mean similarly show var var var term left hand side prior variance right hand side ﬁrst term average posterior variance second term measures variance posterior mean variance positive quantity result shows average posterior variance smaller 
"
26,"[probability, distributions, multinomial, variables]"," equals remaining elements equal multinomial variables represented note vectors satisfy denote probability parameter distribution given parameters constrained satisfy represent probabilities distribution consider data set independent observations corresponding likelihood function takes form represent number observations called sufﬁcient statistics distribution order ﬁnd maximum likelihood solution need maximize respect taking account constraint must sum one achieved using lagrange multiplier maximizing setting derivative respect zero obtain probability distributions solve lagrange multiplier substituting constraint give thus obtain maximum likelihood solution form fraction observations consider joint distribution quantities conditioned parameters total number observations takes form mult given note variables subject constraint 
"
27,"[probability, distributions, multinomial, variables, dirichlet, distribution]"," introduce family prior distributions parameters parameters distribution denotes note summation constraint distribution space conﬁned simplex dimensionality illustrated figure normalized form distribution exercise dir multinomial variables figure dirichlet distribution three variables shown figure multiplying prior likelihood function obtain posterior distribution parameters form denoted case binomial distribution beta prior interpret parameters dirichlet prior effective number observations note two state quantities either represented binary variables johann peter gustav lejeune fourier series family originated richelet belgium name lejeune dirichlet comes dirichlet gave partial proof case although full proof fermat last theo rem arbitrary wait work andrew wiles closing years century probability distributions figure plots dirichlet distribution three variables two horizontal axes coordinates plane simplex vertical axis corresponds value density left plot centre plot right plot 
"
28,"[probability, distributions, gaussian, distribution]"," gaussian distribution written form exp mean variance dimensional vector multivariate gaussian distribution takes form exp dimensional mean vector covariance matrix denotes determinant gaussian distribution arises many different contexts motivated variety different perspectives example already seen another situation gaussian distribution arises consider gaussian distribution figure histogram plots mean uniformly distributed numbers various values observe increases distribution tends towards gaussian illustrate considering variables uniform large distribution tends gaussian illustrated figure practice convergence gaussian increases see figure case gaussian distribution many important analytical properties shall begin considering geometrical form gaussian distribution probability distributions functional dependence gaussian quadratic form appears exponent quantity called mahalanobis distance reduces euclidean distance identity matrix first note matrix taken symmetric without exercise exercise element identity matrix satisﬁes otherwise exercise similarly inverse covariance matrix expressed substituting quadratic form becomes deﬁned interpret new coordinate system deﬁned orthonormal vectors shifted rotated respect original coordinates forming vector gaussian distribution figure density exp value covari probability distributions product eigenvalues hence thus coordinate system gaussian distribution takes form exp product independent univariate gaussian distributions eigen vectors therefore deﬁne new set shifted rotated coordinates respect joint probability distribution factorizes product independent distributions integral distribution coordinate system exp used result normalization univariate gaussian conﬁrms multivariate gaussian indeed normalized look moments gaussian distribution thereby provide interpretation parameters expectation gaussian distribution given exponent changed variables using note exponent even function components integrals taken range term factor vanish symmetry thus refer mean gaussian distribution consider second order moments gaussian univariate case considered second order moment given multivariate gaussian second order moments given group together form matrix matrix written exponent gaussian distribution changed variables using note cross terms involving vanish symmetry term constant taken outside integral unity gaussian distribution normalized consider term involving make use eigenvector expansion covariance matrix given together completeness set eigenvectors write gives exponent made use eigenvector equation together fact integral right hand side middle line vanishes symmetry unless ﬁnal line made use results together thus single random variables subtracted mean taking second ments order deﬁne variance similarly multivariate case convenient subtract mean giving rise covariance random vector deﬁned covariance speciﬁc case gaussian distribution make use together result give covariance parameter matrix governs covariance gaussian distribution called covariance matrix although gaussian distribution widely used density model suffers signiﬁcant limitations consider number free parameters distribution general symmetric covariance matrix independent parameters another independent parameters giv exerciseing parameters total large total number parameters probability distributions figure contours constant total inde known isotropic limitation gaussian distribution intrinsically uni gaussian distribution 
"
29,"[probability, distributions, gaussian, distribution, conditional, gaussian, distributions]"," important property multivariate gaussian distribution two consider ﬁrst case conditional distributions suppose dimensional vector gaussian distribution partition two disjoint subsets without loss generality take form ﬁrst components comprising remaining components also deﬁne corresponding partitions mean vector given covariance matrix given note symmetry covariance matrix implies symmetric many situations convenient work inverse covariance matrix symmetric exercise stressed point instance simply given inverse fact shall shortly examine relation inverse partitioned matrix inverses partitions let begin ﬁnding expression conditional distribution product rule probability see conditional distribution probability distributions evaluated joint distribution simply ﬁxing observed value normalizing resulting expression obtain valid probability distribution instead performing normalization explicitly obtain solution efﬁciently considering quadratic form exponent gaussian distribution given reinstating normalization coefﬁcient end calculation make use partitioning obtain see function quadratic form hence cor responding conditional distribution gaussian distri bution completely characterized mean covariance goal identify expressions mean covariance inspection example rather common operation associated gaussian distributions sometimes called completing square given quadratic form deﬁning exponent terms gaussian distribution need determine corresponding mean covariance problems solved straightforwardly noting exponent general gaussian distribution written const const denotes terms independent made use symmetry thus take general quadratic form express form given right hand side immediately equate matrix coefﬁcients entering second order term inverse covariance matrix coefﬁcient linear term obtain let apply procedure conditional gaussian distribution quadratic form exponent given denote mean covariance distribution respectively consider functional dependence regarded constant pick terms second order immediately conclude covariance inverse precision given gaussian distribution consider terms linear used discussion general form coefﬁcient expression must equal hence made use results expressed terms partitioned precision matrix original joint distribution also express results terms corresponding partitioned covariance matrix make use following identity inverse partitioned matrix exercise mbd cmbd deﬁned quantity known schur complement matrix left hand side respect submatrix using deﬁnition making use obtain following expressions mean covariance conditional distribution comparing see conditional distribution takes simpler form expressed terms partitioned precision matrix expressed terms partitioned covariance matrix note mean conditional distribution given linear function covariance given independent represents example linear gaussian model section probability distributions 
"
30,"[probability, distributions, gaussian, distribution, marginal, gaussian, distributions]"," seen joint distribution gaussian conditional distribution gaussian turn discussion marginal distribution given shall see also gaussian strategy evaluating distribution efﬁciently focus quadratic form exponent joint distribution thereby identify mean covariance marginal distribution quadratic form joint distribution expressed using partitioned precision matrix form goal integrate easily achieved ﬁrst considering terms involving completing square order facilitate integration picking terms involve deﬁned see dependence cast standard quadratic form gaussian distribution corresponding ﬁrst term right hand side plus term depend depend thus take exponential quadratic form see integration required take form exp integration easily performed noting integral unnor malized gaussian result reciprocal normalization efﬁcient know form normalized gaussian given coefﬁcient independent mean depends determinant covariance matrix thus completing square respect integrate term remaining contributions left hand side depends last term right hand side given combining term remaining terms gaussian distribution depend obtain const const const denotes quantities independent comparison see covariance marginal distribution given similarly mean given used covariance expressed terms partitioned precision matrix given rewrite terms corresponding partitioning covariance matrix given conditional distribution partitioned matrices related making use thus obtain intuitively satisfying result marginal distribution mean covariance given covariance see marginal distribution mean covariance simply pressed terms partitioned covariance matrix contrast conditional distribution partitioned precision matrix gives rise simpler expressions results marginal conditional distributions partitioned gaussian summarized partitioned gaussians given joint gaussian distribution probability distributions figure plot left shows contours gaussian distribution two variables plot right shows marginal distribution blue curve conditional distribution red curve conditional distribution marginal distribution illustrate idea conditional marginal distributions associated multivariate gaussian using example involving two variables figure sections considered gaussian partitioned vector two subvectors found expressions conditional distribution marginal distribution noted mean conditional distribution linear function function covariance independent example gaussian distribution linear gaussian model roweis ghahramani shall study greater generality section wish ﬁnd marginal distribution conditional distribution problem arise frequently subsequent chapters prove convenient derive general results shall take marginal conditional distributions parameters governing means precision first ﬁnd expression joint distribution deﬁne consider log joint distribution const const denotes terms independent see quadratic function components hence gaussian distribution ﬁnd precision gaussian consider second order terms written lax gaussian distribution precision inverse covariance matrix given covariance matrix found taking inverse precision done using matrix inversion formula give exercise covariance probability distributions similarly ﬁnd mean gaussian distribution identifying linear terms given making use obtain exercise next ﬁnd expression marginal distribution section covariance finally seek expression conditional recall results section see conditional distribution mean covariance given covariance evaluation conditional seen example bayes theorem interpret distribution prior distribution variable form results summarized gaussian distribution marginal conditional gaussians 
"
31,"[probability, distributions, gaussian, distribution, maximum, likelihood, gaussian]"," given data set observations given probability distributions exercise involves result joint maximization respect note solution depend ﬁrst evaluate use evaluate evaluate expectations maximum likelihood solutions true distribution obtain following results exercise given clearly expectation equal 
"
32,"[probability, distributions, gaussian, distribution, sequential, estimation]"," discussion maximum likelihood solution parameters gaus consider result maximum likelihood estimator mean denote based observations gaussian distribution figure together regression function given con ditional expectation robbins functions dissect contribution ﬁnal data point obtain result nice interpretation follows observing estimated observe data point obtain revised estimate moving old estimate small amount proportional direction error signal note increases contribution successive data points gets smaller result clearly give answer batch result goal ﬁnd root large data set following general procedure solving problems given probability distributions robbins monro shall assume conditional variance ﬁnite shall also without loss generality consider case case figure robbins monro procedure deﬁnes sequence successive estimates root given observed value takes value coefﬁcients represent sequence positive numbers satisfy conditions lim shown robbins monro fukunaga sequence estimates given indeed converge root probability one note ﬁrst condition ensures successive corrections decrease magnitude process converge limiting value second con dition required ensure algorithm converge short root third condition needed ensure accumulated noise ﬁnite variance hence spoil convergence let consider general maximum likelihood problem solved sequentially using robbins monro algorithm deﬁnition maximum like lihood solution stationary point log likelihood function hence satisﬁes exchanging derivative summation taking limit lim see ﬁnding maximum likelihood solution corresponds ﬁnding root regression function therefore apply robbins monro procedure takes form gaussian distribution figure case gaussian distribution corresponding mean regression case random variable corresponds expectation root regres speciﬁc example consider sequential estimation mean gaussian distribution case parameter estimate mean gaussian random variable given thus distribution gaussian mean illustrated fig form note apply equally multivariate case blum maximum likelihood framework gave point estimates parameters known consider task inferring mean given set observations likelihood function probability observed data given viewed function given exp emphasize likelihood function probability distri bution normalized see likelihood function takes form exponential quad ratic form thus choose prior given gaussian probability distributions conjugate distribution likelihood function corresponding poste rior product two exponentials quadratic functions hence also gaussian therefore take prior distribution posterior distribution given simple manipulation involving completing square exponent shows exercise posterior distribution given maximum likelihood solution given sample mean worth spending moment studying form posterior mean variance first note mean posterior distribution given compromise prior mean maximum likelihood solution number observed data points reduces prior mean expected posterior mean given maximum likelihood solution similarly consider result variance posterior distribution see naturally expressed terms inverse variance called precision furthermore precisions additive precision posterior given precision prior plus one contribution data precision observed data points increase number observed data points precision steadily increases corresponding posterior distribution steadily decreasing variance observed data points prior variance whereas number data points variance goes zero posterior distribution becomes inﬁnitely peaked around maximum likelihood solution therefore see maximum likelihood result point estimate given recovered precisely bayesian formalism limit inﬁnite number observations note also ﬁnite take limit prior inﬁnite variance posterior mean reduces maximum likelihood result posterior variance given gaussian distribution figure gaussian distri curves show prior distribution curve labelled probability distributions figure plot gamma distribution gam deﬁned various values parameters gam exp distribution ﬁnite plotted various values figure mean variance gamma distribution given var consider prior distribution gam multiply likelihood function obtain posterior distribution exp recognize gamma distribution form gam maximum likelihood estimator variance note gaussian distribution see effect observing data points increase value coefﬁcient thus interpret parameter prior terms effective prior observations similarly see data points contribute parameter variance interpret parameter prior arising effective prior observations variance recall made analogous interpretation dirichlet prior distributions section examples exponential family shall see interpretation conjugate prior terms effective ﬁctitious data points general one exponential family distributions instead working precision consider variance conjugate prior case called inverse gamma distribution although shall discuss ﬁnd convenient work precision suppose mean precision unknown ﬁnd conjugate prior consider dependence likelihood function exponent wish identify prior distribution functional dependence likelihood function therefore take form exponent cλµ exponent constants since always write ﬁnd inspection particular see gaussian whose precision linear function gamma distri bution normalized prior takes form gam deﬁned new constants given distribution called normal gamma gaussian gamma distribution plotted figure note simply product independent gaussian prior gamma prior precision linear function even chose prior independent posterior distribution would exhibit coupling precision value probability distributions figure case multivariate gaussian distribution dimensional variable conjugate prior distribution mean exp called number degrees freedom distribution scale matrix denotes trace normalization constant given known normal wishart gaussian wishart distribution seen conjugate prior precision gaussian given gamma distribution univariate gaussian together gamma prior gam integrate precision obtain marginal distribution form gaussian distribution figure various values limit corresponds gaussian distribution mean precision probability distributions figure illustration robustness student distribution compared gaussian histogram back substitute alternative parameters see distribution written form gam generalize multivariate gaussian obtain cor responding multivariate student distribution form gam gaussian distribution dimensionality squared mahalanobis distance deﬁned exercise covariance mode corresponding results univariate case 
"
33,"[probability, distributions, gaussian, distribution, periodic, variables]"," although gaussian distributions great practical signiﬁcance example periodic variable would wind direction particular might tempted treat periodic variables choosing direction model using standard univariate gaussian distribution choose origin sample mean data set standard deviation whereas choose origin mean standard deviation clearly need develop special approach treatment periodic variables let consider problem evaluating mean set observations periodic variable shall assume measured radians already seen simple average illustrated figure average vectors probability distributions figure periodic variable two dimensional vectors living unit circle also shown average vectors gaussian distribution figure one must also periodic thus must satisfy three conditions follows integer easily obtain gaussian like distribution satisﬁes three prop erties follows consider gaussian distribution two variables mean covariance matrix identity matrix exp polar coordinates cos sin also map mean cos sin cos cos sin sin cos cos sin sin cos const probability distributions figure von mises distribution plotted two different parameter values shown cartesian plot left corresponding polar plot right cos sin cos cos sin sin cos deﬁne obtain ﬁnal expression distribution along unit circle form exp cos corresponds mean distribution known exp cos large distribution becomes approximately gaussian von mises distributionplotted figure function plotted figure consider maximum likelihood estimators parameters von mises distribution log likelihood function given cos gaussian distribution figure plot bessel function deﬁned together function deﬁned setting derivative respect equal zero gives sin solve make use trigonometric identity sin cos sin cos sin obtain tan sin cos similarly maximizing respect making use abramowitz stegun cos substituted maximum likelihood solution recalling performing joint optimization deﬁned cos cos sin sin probability distributions figure plots old faith full data blue curves show contours constant probability density left single gaussian distribution ﬁtted dataing maximum likelihood note distribution fails capture two clumps data indeed places much probability mass central region clumps data relatively sparse right distribution given linear combination two gaussians ﬁtted data maximum likelihood using techniques discussed chaptergives better rep resentation data right hand side easily evaluated function inverted numerically completeness mention brieﬂy alternative techniques con struction periodic distributions simplest approach use histogram observations angular coordinate divided ﬁxed bins virtue simplicity ﬂexibility also suffers signiﬁcant limitations shall see discuss histogram methods detail section another approach starts like von mises distribution gaussian distribution euclidean space marginalizes onto unit circle rather conditioning mardia jupp however leads complex forms distribution discussed finally valid distribution real axis gaussian turned periodic distribution mapping succes sive intervals width onto periodic variable corresponds wrapping real axis around unit circle resulting distribution complex handle von mises distribution one limitation von mises distribution unimodal forming mixtures von mises distributions obtain ﬂexible framework modelling periodic variables handle multimodality example machine learning application makes use von mises distributions see lawrence extensions modelling conditional densities regression problems see bishop nabney 
"
34,"[probability, distributions, gaussian, distribution, mixtures, gaussians]"," gaussian distribution important analytical properties suf fers signiﬁcant limitations comes modelling real datasets consider example shown figure known old faithful data set comprises measurements eruption old faithful geyser yel lowstone national park usa measurement comprises duration appendix gaussian distribution figure superpositions formed taking linear combinations basic dis therefore consider superposition gaussian densities form called mixture gaussians gaussian density called component mixture mean covariance section shall consider gaussian components illustrate frame parameters called mixing coefﬁcients integrate also requirement together implies combining condition obtain probability distributions figure illustration mixture gaussians two dimensional space contours constant mixture distribution surface plot distribution sum product rules marginal density given equivalent view prior prob ability picking component density form gaussian mixture distribution governed parameters maximum likelihood log likelihood function given exponential family immediately see situation much 
"
35,"[probability, distributions, exponential, family]"," exponential family distributions given parameters deﬁned set distributions form exp may scalar vector may discrete continuous interpreted coefﬁcient ensures distribution normalized therefore satisﬁes exp integration replaced summation discrete variable begin taking examples distributions introduced earlier bern expressing right hand side exponential logarithm exponent comparison allows identify probability distributions solve give exp called logistic sigmoid function thus write bernoulli distribution using standard representation form exp used easily proved comparison shows next consider multinomial distribution single observation takes form exp write standard representation exp deﬁned comparing note parameters independent parameters sub ject constraint given parameters value remaining parameter ﬁxed circumstances convenient remove constraint expressing distribution terms parameters achieved using relationship eliminate expressing terms remaining thereby leaving parameters note remaining parameters still subject constraints exponential family making use constraint multinomial distribution representation becomes exponent identify solve ﬁrst summing sides rearranging back substituting give exponent called softmax function normalized exponential represen tation multinomial distribution therefore takes form exponent standard form exponential family parameter vector exp finally let consider gaussian distribution univariate gaussian exponent probability distributions exercise exp 
"
36,"[probability, distributions, exponential, family, maximum, likelihood, sufﬁcient, statistics]"," let consider problem estimating parameter vector gen exponent rearranging making use gives exp used therefore obtain result similarly higher order moments thus provided normalize exercise consider set independent identically distributed data denoted likelihood function given exp setting gradient respect zero get following condition satisﬁed maximum likelihood estimator exponential family principle solved obtain see solution maximum likelihood estimator depends data therefore called sufﬁcient statistic distribution need store entire data set value sufﬁcient statistic bernoulli distribution example function given need keep sum data points whereas gaussian keep sum sum consider limit right hand side becomes comparing see limit equal true value fact sufﬁciency property holds also bayesian inference although shall defer discussion chapter equipped tools graphical models thereby gain deeper insight important concepts 
"
37,"[probability, distributions, exponential, family, conjugate, priors]"," already encountered concept conjugate prior several times example context bernoulli distribution conjugate prior beta distribution gaussian conjugate prior mean gaussian conjugate prior precision wishart distribution general given probability distribution seek prior conjugate likelihood function posterior distribution functional form prior member exponential family exists conjugate prior written form exp normalization coefﬁcient function pears see indeed conjugate let multiply prior likelihood function obtain posterior distribution malization coefﬁcient form exp takes functional form prior conﬁrming conjugacy furthermore see parameter interpreted effective number pseudo observations prior value sufﬁcient statistic given 
"
38,"[probability, distributions, exponential, family, noninformative, priors]"," applications probabilistic inference may prior knowledge conveniently expressed prior distribution example prior assigns zero probability value variable posterior distributionnecessarily also assign zero probability value irrespective probability distributions subsequent observations data many cases however may little idea form distribution take may seek form prior distribution called noninformative prior intended little inﬂu ence posterior distribution possible jeffries box tao bernardo smith sometimes referred letting data speak distribution governed parameter might tempted propose prior distribution const suitable prior discrete variable states simply amounts setting prior probability state case continuous parameters however two potential difﬁculties approach ﬁrst domain unbounded prior distribution cannot correctly normalized integral diverges priors called improper practice improper priors often used provided corresponding posterior distribution proper correctly normalized instance put uniform prior distribution mean gaussian posterior distribution mean observed least one data point proper second difﬁculty arises transformation behaviour probability density nonlinear change variables given function constant change variables also constant however choose density constant density given density constant issue arise use maximum likelihood likelihood function simple function free use convenient parameterization however choose prior distribution constant must take care use appropriate representation parameters consider two simple examples noninformative priors berger first density takes form parameter known location parameter family densities exhibits translation invariance shift constant give deﬁned thus density takes form new variable original one density independent choice origin would like choose prior distribution reﬂects translation invariance property choose prior assigns equal probability mass exponential family interval shifted interval implies must hold choices implies constant example location parameter would mean gaussian distribution seen conjugate prior distri bution case gaussian obtain noninformative prior taking limit indeed see gives posterior distribution contributions prior vanish second example consider density form note normalized density provided correctly normalized parameter known scale parameter density exhibits exercise scale invariance scale constant give deﬁned transformation corresponds change scale example meters kilometers length would like choose prior distribution reﬂects scale invariance consider interval scaled interval prior assign equal probability mass two intervals thus must hold choices hence note improper prior integral distribution divergent sometimes also convenient think prior distribution scale parameter terms density log parameter using transformation rule densities see const thus prior probability mass range range probability distributions example scale parameter would standard deviation gaussian distribution taken account location parameter exp discussed earlier often convenient work terms precision rather using transformation rule densities see distribution corresponds distribution form seen conjugate prior gamma distribution gam given noninformative prior obtained section special case examine results posterior distribution see posterior depends terms arising data prior 
"
39,"[probability, distributions, nonparametric, methods]"," throughout chapter focussed use probability distributions speciﬁc functional forms governed small number parameters whose values determined data set called parametric approach density modelling important limitation approach chosen density might poor model distribution generates data result poor predictive performance instance process generates data multimodal aspect distribution never captured gaussian necessarily unimodal ﬁnal section consider nonparametric approaches density timation make assumptions form distribution shall focus mainly simple frequentist methods reader aware however nonparametric bayesian methods attracting increasing interest walker neal uller quintana teh let start discussion histogram methods density estimation already encountered context marginal conditional distributions figure context central limit theorem figure explore properties histogram density models detail focussing case single continuous variable standard histograms simply partition distinct bins width count number observations falling bin order turn count normalized probability density simply divide total number observations width bins obtain probability values bin given easily seen gives model density nonparametric methods figure illustration histogram approach density estimation data set data points generated distribution shown green curve histogram density estimates based common bin width shown various values figure show example histogram density estimation note histogram method property unlike methods dis practice histogram technique useful obtaining quick visual exponential scaling example curse dimensionality space high dimensional section histogram approach density estimation however teach two probability distributions neighbourhood property deﬁned bins natural smoothing parameter describing spatial extent local region case bin width second value smoothing parameter neither large small order obtain good results reminiscent choice model complexity polynomial curve ﬁtting discussed chapter degree polynomial alternatively value regularization parameter optimal intermediate value neither large small armed insights turn discussion two widely used nonparametric tech niques density estimation kernel estimators nearest neighbours better scaling dimensionality simple histogram model 
"
40,"[probability, distributions, nonparametric, methods, kernel, density, estimators]"," let suppose observations drawn unknown probability density dimensional space shall take euclidean wish estimate value earlier discussion locality let consider small region containing probability mass associated region given suppose collected data set comprising observations drawn data point probability falling within total number points lie inside distributed according binomial distribution section bin using see mean fraction points falling inside region large distribution sharply peaked around mean however also assume region sufﬁciently small probability density roughly constant region volume combining obtain density estimate form note validity depends two contradictory assumptions namely region sufﬁciently small density approximately constant region yet sufﬁciently large relation value density number points falling inside region sufﬁcient binomial distribution sharply peaked nonparametric methods exploit result two different ways either determine value data gives rise nearest neighbour technique discussed shortly determine data giving rise kernel approach shown nearest neighbour density estimator kernel density estimator converge true probability density limit provided shrinks suitably grows duda hart begin discussing kernel method detail start take region small hypercube centred point wish determine probability density order count number points falling within region convenient deﬁne following function otherwise represents unit cube centred origin function example kernel function context also called parzen window quantity one data point lies inside cube side centred zero otherwise total number data points lying inside cube therefore substituting expression gives following result esti mated density used volume hypercube side mensions using symmetry function interpret equation single cube centred sum cubes centred data points stands kernel density estimator suffer one problems histogram method suffered namely presence artiﬁcial discontinuities case boundaries cubes obtain smoother density model choose smoother kernel function common choice gaussian gives rise following kernel density model exp represents standard deviation gaussian components thus density model obtained placing gaussian data point adding contributions whole data set dividing density correctly normalized figure apply model data probability distributions figure see acts middle panel choose kernel function subject conditions one difﬁculties kernel approach density estimation therefore return general result local density estimation nonparametric methods figure illustration nearest neighbour den see parameter governs degree smoothing small value exercise close chapter showing nearest neighbour technique points class points total points class provides estimate density associated class similarly unconditional density given class priors given probability distributions figure nearest closest training data points case nearest neighbour approach classiﬁcation thus classify new point identify nearest figure show results applying nearest neighbour algo figure plot data points oil data set showing values plotted nearest neighbour algorithm various values exercises interesting property nearest neighbour classiﬁer limit error rate never twice minimum achievable error discussed far nearest neighbour method kernel den 
"
41,"[probability, distributions, exercises]"," verify bernoulli distribution satisﬁes following properties var form bernoulli distribution given symmetric case distribution written show distribution normalized evaluate mean variance entropy exercise prove binomial distribution probability distributions use result prove induction following result known binomial theorem valid real values finally show binomial distribution normalized done ﬁrst pulling factor summation making use binomial theorem show mean binomial distribution given differentiate sides normalization condition respect rearrange obtain expression mean similarly differentiating twice respect making use result mean binomial distribution prove result variance binomial exercise prove beta distribution given correctly normalized holds equivalent showing deﬁnition gamma function exponent use expression prove follows first bring integral inside integrand integral next make change variable ﬁxed interchange order integrations ﬁnally make change variable ﬁxed make use result show mean variance mode beta distribution given respectively var mode exercises times maximum likelihood estimate illustrates con var var var denotes expectation conditional distribution similar notation conditional variance exercise prove normalization dirichlet dis variables eliminating dirichlet written goal ﬁnd expression integrate taking making use derive expression using property gamma function derive var covariance deﬁned probability distributions expressing expectation dirichlet distribution derivative respect show given digamma function uniform distribution continuous variable deﬁned evaluate kullback leibler divergence two gaussians exercise demonstrates multivariate distribution max show entropy multivariate gaussian given dimensionality exercises consider two random variables gaussian distri butions means precisions respectively derive expression differential entropy variable ﬁrst ﬁnd distribution using relation completing square exponent observe represents convolution two gaussian distributions gaussian ﬁnally make use result entropy univariate gaussian consider multivariate gaussian distribution given writing precision matrix inverse covariance matrix sum symmetric anti symmetric matrix show anti symmetric term appear exponent gaussian hence precision matrix may taken symmetric without loss generality inverse symmetric matrix also symmetric see exercise follows covariance matrix may also chosen symmetric without loss generality consider real symmetric matrix whose eigenvalue equation given taking complex conjugate equation subtracting original equation forming inner product eigenvector show eigenvalues real similarly use symmetry property show two eigenvectors orthogonal provided finally show without loss generality set eigenvectors chosen orthonormal satisfy even eigenvalues zero show real symmetric matrix eigenvector equation expressed expansion eigenvectors coefﬁcients given eigenvalues form similarly show inverse matrix representation form positive deﬁnite matrix deﬁned one quadratic form positive real value vector show necessary sufﬁcient condition positive deﬁnite eigenvalues deﬁned positive show real symmetric matrix size independent parameters show inverse symmetric matrix symmetric diagonalizing coordinate system using eigenvector expansion show volume contained within hyperellipsoid corresponding constant probability distributions mahalanobis distance given volume unit sphere dimensions mahalanobis distance deﬁned prove identity multiplying sides matrix making use deﬁnition corresponding partitioning mean vector covariance matrix form marginalized useful result linear algebra woodbury matrix inversion formula given bcd multiplying sides bcd prove correctness result let two independent random vectors consider joint distribution variable given exercises consider two multidimensional random vectors gaussian distributions respectively together exercise next provide practice manipulating consider joint distribution exercise use verify agree corre sponding expressions ﬁnd maximum likelihood solution covariance matrix denotes data point sampled gaussian distribution mean covariance denotes element identity matrix hence prove result using analogous procedure used obtain derive expression sequential estimation variance univariate gaussian probability distributions distribution starting maximum likelihood expression using analogous procedure used obtain derive starting results posterior distribution data points hence obtain expressions sequential update derive results starting posterior distribution multiplying likelihood function completing square normalizing obtain posterior distribution observations consider dimensional gaussian random variable distribution covariance known wish infer mean set observations given prior distribution ﬁnd corresponding posterior distribution evaluate mean variance mode gamma distribution following distribution exp exercises observed data set input vectors corresponding target variables given const const denotes terms independent note function error function considered section consider univariate gaussian distribution conjugate gaussian gamma prior given data set verify wishart distribution deﬁned indeed conjugate prior precision matrix multivariate gaussian verify evaluating integral leads result show limit distribution becomes following analogous steps used derive univariate student show limit multivariate student distribution reduces gaussian mean precision various trigonometric identities used discussion periodic variables chapter proven easily relation exp cos sin square root minus one considering identity exponent prove result similarly using identity cos probability distributions exp denotes imaginary part prove result large von mises distribution becomes sharply peaked around mode deﬁning making taylor pansion cosine function given cos show von mises distribution tends gaussian using trigonometric identity show solution given computing ﬁrst second derivatives von mises distribution using show maximum distribution occurs minimum occurs modmaking use result together trigonometric identity show maximum likelihood solution concentration von mises distribution satisﬁes radius mean observations viewed unit vectors two dimensional euclidean plane illustrated figure express beta distribution gamma distribution von mises distribution members exponential family thereby identify natural parameters verify multivariate gaussian distribution cast exponential family form derive expressions analogous result showed negative gradient exponential family given expectation taking second derivatives show covariance changing variables using show density correctly normalized provided correctly normalized consider histogram like density model space vided ﬁxed regions density takes constant value region volume region denoted suppose set observations observations fall region using lagrange multiplier enforce normalization constraint density derive expression maximum likelihood estimator show nearest neighbour density model deﬁnes improper distribution whose integral space divergent 
"
42,"[linear, models, regression]"," focus far book unsupervised learning including topics density estimation data clustering turn discussion supervised learning starting regression goal regression predict value one continuous target variables given value dimensional vector input variables already encountered example regression problem considered polynomial curve ﬁtting chapter polynomial speciﬁc example broad class functions called linear regression models share property linear functions adjustable parameters form focus chapter simplest form linear regression models also linear functions input variables however obtain much useful class functions taking linear combinations ﬁxed set nonlinear functions input variables known basis functions models linear functions parameters gives simple analytical properties yet nonlinear respect input variables linear models regression given training data set comprising observations together corresponding target values goal predict value expresses although linear models signiﬁcant limitations practical techniques 
"
43,"[linear, models, regression, linear, basis, function, models]"," often simply known linear regression key property model linear function parameters also however linear function input variables imposes signiﬁcant known basis functions denoting maximum value index total number parameters model parameter allows ﬁxed offset data sometimes called many practical plications pattern recognition apply form ﬁxed pre processing linear basis function models feature extraction original data variables original variables comprise vector features expressed terms basis functions using nonlinear basis functions allow function non linear function input vector functions form called linear models however function linear linearity parameters greatly simplify analysis class models however also leads signiﬁcant limitations discuss section example polynomial regression considered chapter particular example model single input variable basis functions take form powers one limitation polynomial basis functions global functions input variable changes one region input space affect regions resolved dividing input space regions different polynomial region leading spline functions hastie many possible choices basis functions example exp govern locations basis functions input space parameter governs spatial scale usually referred gaussian basis functions although noted required prob abilistic interpretation particular normalization coefﬁcient unimportant basis functions multiplied adaptive parameters another possibility sigmoidal basis function form logistic sigmoid function deﬁned exp equivalently use tanh function related logistic sigmoid tanh general linear combination logistic sigmoid functions equivalent general linear combination tanh functions various choices basis function illustrated figure yet another possible choice basis function fourier basis leads expansion sinusoidal functions basis function represents speciﬁc fre quency inﬁnite spatial extent contrast basis functions localized ﬁnite regions input space necessarily comprise spectrum different spatial frequencies many signal processing applications interest consider sis functions localized space frequency leading class functions known wavelets also deﬁned mutually orthogonal simplify application wavelets applicable input values live linear models regression figure examples basis functions showing polynomials left gaussians form centre sigmoidal form right discussion chapter however independent particular basis functions simply identity fur 
"
44,"[linear, models, regression, linear, basis, function, models, maximum, likelihood, least, squares]"," chapter ﬁtted polynomial functions datasets minimizing sum assume target variable given deterministic function additive gaussian noise case gaussian conditional distribution form conditional mean linear basis function models simply note gaussian noise assumption implies conditional distribution given unimodal may inappropriate applications tension mixtures conditional gaussian distributions permit multimodal conditional distributions discussed section consider data set inputs corresponding target values group target variables column vector denote typeface chosen distinguish single observation multivariate target would denoted making assumption data points drawn independently distribution obtain following expression likelihood function function adjustable parameters form used note supervised learning problems regression classiﬁcation seeking model distribution input variables thus always appear set conditioning variables drop explicit expressions der keep notation uncluttered taking logarithm likelihood function making use standard form univariate gaussian sum squares error function deﬁned written likelihood function use maximum likelihood determine consider ﬁrst maximization respect observed already section see maximization likelihood function conditional gaussian noise distribution linear model equivalent minimizing sum squares error function given gradient log likelihood function takes form linear models regression setting gradient zero gives solving obtain known normal equations least squares problem matrix called design matrix whose elements given quantity known moore penrose pseudo inverse matrix rao mitra golub van loan regarded generalization notion matrix inverse nonsquare matrices indeed square invertible using property see point gain insight role bias parameter make bias parameter explicit error function becomes setting derivative respect equal zero solving obtain deﬁned thus bias compensates difference averages training set target values weighted sum averages basis function values also maximize log likelihood function respect noise precision parameter giving linear basis function models figure dimensional space whose axes values least squares length elements linear models regression data points considered one time model parameters dated presentation sequential learning also appropriate real time applications data observations arriving continuous stream predictions must made data points seen obtain sequential learning algorithm applying technique stochastic gradient descent also known sequential gradient descent follows error function comprises sum data points presen tation pattern stochastic gradient descent algorithm updates parameter vector using denotes iteration number learning rate parameter shall discuss choice value shortly value initialized starting vector case sum squares error function gives known least mean squares lms algorithm value needs chosen care ensure algorithm converges bishop nabney 
"
45,"[linear, models, regression, linear, basis, function, models, regularized, least, squares]"," section introduced idea adding regularization term error function order control ﬁtting total error function minimized takes form regularization coefﬁcient controls relative importance data dependent error regularization term one simplest forms regularizer given sum squares weight vector elements also consider sum squares error function given total error function becomes particular choice regularizer known machine learning literature weight decay sequential learning algorithms encourages weight values decay towards zero unless supported data statistics provides ample parameter shrinkage method shrinks parameter values towards linear basis function models figure contours regularization term various values parameter zero advantage error function remains quadratic function represents simple extension least squares solution general regularizer sometimes used regularized error takes form case know lasso statistics literature tibshirani driven zero leading sparse model corresponding basis regularization allows complex models trained datasets limited size linear models regression figure plot contours left lasso regularizer right denoted bias variance decomposition maximize function respect giving examine result target variable dimensional column vector components shared vectors extension general gaussian noise distributions arbitrary covariance matrices straightforward leads decoupling inde exercise 
"
46,"[linear, models, regression, bias-variance, decomposition]"," seen earlier chapters phenomenon ﬁtting really section discussed decision theory regression problems popular choice linear models regression squared loss function optimal prediction given conditional expectation denote given point worth distinguishing squared loss function arising decision theory sum squares error function arose maxi mum likelihood estimation model parameters might use sophisticated techniques least squares example regularization fully bayesian approach determine conditional distribution combined squared loss function purpose making predictions showed section expected squared loss written form recall second term independent arises intrinsic noise data represents minimum achievable value expected loss ﬁrst term depends choice function seek lution makes term minimum nonnegative smallest hope make term zero unlimited supply data unlimited computational resources could principle ﬁnd regression function desired degree accuracy would represent optimal choice however practice data set containing ﬁnite number data points consequently know regression function exactly model using parametric function governed parameter vector bayesian perspective uncertainty model expressed posterior distribution frequentist treatment however involves making point estimate based data set tries instead interpret uncertainty estimate following thought experi ment suppose large number datasets size drawn independently distribution given data set run learning algorithm obtain prediction function different datasets ensemble give different functions consequently different values squared loss performance particular learning algorithm assessed taking average ensemble datasets consider integrand ﬁrst term particular data set takes form quantity dependent particular data set take aver age ensemble datasets add subtract quantity bias variance decomposition inside braces expand obtain take expectation expression respect note ﬁnal term vanish giving bias variance see expected squared difference regression sensitive far considered single input value substitute expansion back obtain following decomposition expected squared loss expected loss bias variance noise bias variance noise bias variance terms refer integrated quantities goal minimize expected loss decomposed linear models regression figure illustration dependence bias variance model complexity governed regularization parameter using sinusoidal data set chapter datasets data points gaussian basis functions model total number parameters including bias parameter left column shows result ﬁtting model datasets various values clarity ﬁts shown right column shows corresponding average ﬁts red along sinusoidal function datasets generated green bias variance decomposition figure bias variance occurs around bias variance bias variance test error shown figure also examine bias variance trade quantitatively example average prediction estimated integrated squared bias integrated variance given bias variance linear models regression data set leading large variance conversely large value pulls weight parameters towards zero leading large bias although bias variance decomposition may provide interesting sights model complexity issue frequentist perspective limited practical value bias variance decomposition based averages respect ensembles datasets whereas practice single observed data set large number independent training sets given size would better combining single large training set course would reduce level ﬁtting given model complexity given limitations turn next section bayesian treatment linear basis function models provides powerful insights issues ﬁtting also leads practical techniques addressing question model complexity 
"
47,"[linear, models, regression, bayesian, linear, regression]"," discussion maximum likelihood setting parameters linear gression model seen effective model complexity governed number basis functions needs controlled according size data set adding regularization term log likelihood function means effective model complexity controlled value regularization coefﬁcient although choice number form basis functions course still important determining overall behaviour model leaves issue deciding appropriate model complexity particular problem cannot decided simply maximizing likelihood function always leads excessively complex models ﬁtting dependent hold data used determine model complexity discussed section computationally expensive wasteful valu able data therefore turn bayesian treatment linear regression avoid ﬁtting problem maximum likelihood also lead automatic methods determining model complexity using training data alone simplicity focus case single target variable tension multiple target variables straightforward follows discussion section 
"
48,"[linear, models, regression, bayesian, linear, regression, parameter, distribution]"," begin discussion bayesian treatment linear regression troducing prior probability distribution model parameters ment shall treat noise precision parameter known constant first note likelihood function deﬁned exponential quadratic function corresponding conjugate prior therefore given gaussian distribution form mean covariance bayesian linear regression next compute posterior distribution proportional product likelihood function prior due choice conjugate gaussian prior distribution posterior also gaussian evaluate distribution usual procedure completing square exponential ﬁnding normalization coefﬁcient using standard result normalized gaussian however already done necessary work deriving gen exercise general result allows write posterior distribution directly form note posterior distribution gaussian mode coincides mean thus maximum posterior weight vector simply given map consider inﬁnitely broad prior mean posterior distribution reduces maximum likelihood value given similarly posterior distribution reverts prior furthermore data points arrive sequentially posterior distribution stage acts prior distribution subsequent data point new posterior distribution given exercise remainder chapter shall consider particular form gaussian prior order simplify treatment speciﬁcally consider zero mean isotropic gaussian governed single precision parameter corresponding posterior distribution given log posterior distribution given sum log likelihood log prior function takes form const maximization posterior distribution respect therefore equiva lent minimization sum squares error function addition quadratic regularization term corresponding illustrate bayesian learning linear basis function model well sequential update posterior distribution using simple example involving straight line ﬁtting consider single input variable single target variable linear models regression linear model form two adap tive parameters plot prior posterior distributions directly parameter space generate synthetic data function parameter values ﬁrst choosing values uniform distribution evaluating ﬁnally adding gaussian noise standard deviation obtain target values goal recover values data explore dependence size data set assume noise variance known hence set precision parameter true value similarly parameter shall shortly discuss strategies determining training data figure shows results bayesian learning model size data set increased demonstrates sequential nature bayesian learning current posterior distribution forms prior new data point observed worth taking time study ﬁgure detail illustrates several important aspects bayesian inference ﬁrst row ﬁgure corresponds situation data points observed shows plot prior distribution space together six samples function values drawn prior second row see situation observing single data point location data point shown blue circle right hand column left hand column plot likelihood function data point function note likelihood function provides soft constraint line must pass close data point close determined noise precision comparison true parameter values used generate data set shown white cross plots left column figure multiply likelihood function prior top row normalize obtain posterior distribution shown middle plot second row samples regression function obtained drawing samples posterior distribution shown right hand plot note sample lines pass close data point third row ﬁgure shows effect serving second data point shown blue circle plot right hand column corresponding likelihood function second data point alone shown left plot multiply likelihood function posterior distribution second row obtain posterior distribution shown middle plot third row note exactly posterior distribution would obtained combining original prior likelihood function two data points posterior inﬂuenced two data points two points sufﬁcient deﬁne line already gives relatively compact posterior distribution samples posterior distribution give rise functions shown red third column see functions pass close data points fourth row shows effect observing total data points left hand plot shows likelihood function data point alone middle plot shows resulting posterior distribution absorbed information observations note posterior much sharper third row limit inﬁnite number data points bayesian linear regression figure illustration sequential bayesian learning simple linear model form detailed description ﬁgure given text linear models regression forms prior parameters considered instance generalize gaussian prior give exp 
"
49,"[linear, models, regression, bayesian, linear, regression, predictive, distribution]"," practice usually interested value rather target vari exercise variance predictive distribution given limit second term goes zero variance exercise illustration predictive distribution bayesian linear regression models let return synthetic sinusoidal data set section figure bayesian linear regression figure examples predictive distribution model consisting gaussian basis functions form using synthetic sinusoidal data set section see text detailed discussion model comprising linear combination gaussian basis functions datasets various sizes look corresponding posterior distributions green curves correspond function sin data points generated addition gaussian noise datasets size shown four plots blue circles plot red curve shows mean corresponding gaussian predictive distribution red shaded region spans one standard deviation either side mean note predictive uncertainty depends smallest neighbourhood data points also note level uncertainty decreases data points observed plots figure show point wise predictive variance function order gain insight covariance predictions different values draw samples posterior distribution plot corresponding functions shown figure linear models regression figure plots function using samples posterior distributions corresponding plots figure used localized basis functions gaussians regions away thus note treated unknown introduce predictive distribution student distribution bayesian linear regression figure equivalent kernel gaussian basis versus together data set used generate kernel comprised values equally spaced interval 
"
50,"[linear, models, regression, bayesian, linear, regression, equivalent, kernel]"," posterior mean solution linear basis function model chapter mean written form deﬁned thus mean predictive distribution point given linear combination training set target variables write function data set appear deﬁnition equivalent kernel illustrated case gaussian basis functions figure kernel functions plotted function three different values see localized around mean predictive distribution given obtained forming linear models regression figure examples equiva lent kernels plotted function corre even though insight role equivalent kernel obtained considering covariance given covariance predictive distribution shown figure allows visualize point formulation linear regression terms kernel function suggests seen effective kernel deﬁnes weights training values intuitively pleasing result easily proven informally noting summation equivalent considering predictive mean set target data provided basis functions bayesian model comparison simply obtain note kernel function finally note equivalent kernel satisﬁes important property shared kernel functions general namely expressed form chapter inner product respect vector nonlinear functions 
"
51,"[linear, models, regression]"," shall see ﬁtting associated maximum likelihood bayesian view model comparison simply involves use probabilities model refers probability distribution observed data case polynomial curve ﬁtting problem section given training set wish evaluate posterior distribution expresses preference shown data linear models regression different models shall examine term detail shortly model evidence sometimes also called marginal likelihood viewed likelihood function space models parameters marginalized ratio model evidences two models known bayes factor kass raftery know posterior distribution models predictive distribution given sum product rules example mixture distribution overall predictive distribution obtained averaging predictive distributions individual models weighted posterior probabilities models stance two models posteriori equally likely one predicts narrow distribution around predicts narrow distribution around overall predictive distribution bimodal distribution modes single model simple approximation model averaging use single probable model alone make predictions known model selection model governed set parameters model evidence given sum product rules probability sampling perspective marginal likelihood viewed proba chapterbility generating data set model whose parameters sampled random prior also interesting note evidence precisely normalizing term appears denominator bayes theorem evaluating posterior distribution parameters obtain insight model evidence making simple approx imation integral parameters consider ﬁrst case model single parameter posterior distribution parameters proportional omit dependence model keep notation uncluttered assume posterior distribution sharply peaked around probable value map width posterior approximate tegral value integrand maximum times width peak assume prior ﬂat width prior prior map posterior prior bayesian model comparison figure map posterior prior map taking logs obtain term negative increases magnitude ratio gets smaller thus parameters ﬁnely tuned data posterior distribution penalty term large model set parameters make similar approximation obtain gain insight bayesian model comparison understand successively linear models regression figure simplest complex note distributions normalized model intermedi sim spread large region space datasets distributions normalized see particular data set highest value evidence model implicit bayesian model comparison framework assumption truth corresponds given ﬁnite data seen bayesian framework avoids problem ﬁtting allows models compared basis training data alone however evidence approximation bayesian approach like approach pattern recognition needs make sumptions form model invalid results misleading particular see figure model evidence sensitive many aspects prior behaviour tails indeed evidence deﬁned prior improper seen noting improper prior arbitrary scaling factor words normalization coefﬁcient deﬁned distribution cannot normalized consider proper prior take suitable limit order obtain improper prior example gaussian prior take limit inﬁnite variance evidence zero seen figure may however possible consider evidence ratio two models ﬁrst take limit obtain meaningful answer practical application therefore wise keep aside independent test set data evaluate overall performance ﬁnal system 
"
52,"[linear, models, regression, evidence, approximation]"," fully bayesian treatment linear basis function model would intro duce prior distributions hyperparameters make predictions marginalizing respect hyperparameters well respect parameters however although integrate analytically either hyperparameters complete marginalization variables analytically intractable discuss approximation set hyperparameters speciﬁc values determined maximizing marginal likelihood function obtained ﬁrst integrating parameters framework known statistics literature empirical bayes bernardo smith gelman type maximum likelihood berger generalized maximum likelihood wahba machine learning literature also called evidence approximation gull mackay introduce hyperpriors predictive distribution obtained marginalizing given given deﬁned respectively omitted dependence sharply peaked around values predictive distribution obtained simply marginalizing ﬁxed values linear models regression bayes theorem posterior distribution given prior relatively ﬂat evidence framework values obtained maximizing marginal likelihood function shall proceed evaluating marginal likelihood linear basis function model ﬁnding maxima allow determine values hyperparameters training data alone without recourse cross validation recall ratio analogous regularization parameter aside worth noting deﬁne conjugate gamma prior distri butions marginalization hyperparameters performed analytically give student distribution see section although resulting integral longer analytically tractable might thought approximating integral example using laplace approximation discussed section based local gaussian approximation centred mode posterior distribution might provide practical alternative evidence framework buntine weigend however integrand function typically strongly skewed mode laplace approximation fails capture bulk probability mass leading poorer sults obtained maximizing evidence mackay returning evidence framework note two approaches take maximization log evidence evaluate evidence function analytically set derivative equal zero obtain estimation equations shall section alternatively use technique called expectation maximization algorithm discussed section shall also show two approaches converge solution 
"
53,"[linear, models, regression, evidence, approximation, evaluation, evidence, function]"," marginal likelihood function obtained integrating weight parameters one way evaluate integral make use result conditional distribution linear gaussian model shall evaluate exercise integral instead completing square exponent making use standard form normalization coefﬁcient gaussian write evidence function form exercise exp evidence approximation dimensionality deﬁned recognize equal constant proportionality reg ularized sum squares error function complete square exercise giving introduced together note corresponds matrix second derivatives error function known hessian matrix also deﬁned given using see hence equivalent previous deﬁnition therefore represents mean posterior distribution integral evaluated simply appealing standard result normalization coefﬁcient multivariate gaussian giving exercise exponentonent using write log marginal likelihood form required expression evidence function returning polynomial regression problem plot model evidence order polynomial shown figure assumed prior form parameter ﬁxed form plot instructive referring back figure see polynomial poor data consequently gives relatively low value linear models regression figure polynomial 
"
54,"[linear, models, regression, evidence, approximation, maximizing, evidence, function]"," let ﬁrst consider maximization respect done ﬁrst deﬁning following eigenvector equation follows eigenvalues consider deriva tive term involving respect thus stationary points respect satisfy evidence approximation multiplying rearranging obtain since terms sum quantity written interpretation quantity discussed shortly see value maximizes marginal likelihood satisﬁes exercise note implicit solution depends also mode posterior distribution depends choice therefore adopt iterative procedure make initial choice use ﬁnd given also evaluate given values used estimate using process repeated convergence note matrix ﬁxed compute eigenvalues start simply multiply obtain emphasized value determined purely looking training data contrast maximum likelihood methods independent data set required order optimize model complexity similarly maximize log marginal likelihood respect note eigenvalues deﬁned proportional hence giving stationary point marginal likelihood therefore satisﬁes rearranging obtain exercise implicit solution solved choosing initial value using calculate estimate using repeating convergence determined data values estimated together update linear models regression figure contours likelihood function red hessian mode poste rior given maximum likelihood solution whereas nonzero mode map direction eigenvalue deﬁned small compared quantity close zero corresponding map value also close zero contrast direction eigenvalue large compared quantity close unity map value close maximum likelihood value map evidence approximation single variable given estimate biased maximum likelihood solution mean ﬁtted noise data effect used one degree freedom model corresponding unbiased estimate given takes form map shall see section result obtained bayesian treat ment marginalize unknown mean factor denominator bayesian result takes account fact one degree freedom used ﬁtting mean removes bias maximum likelihood consider corresponding results linear regression model mean target distribution given function contains parameters however parameters tuned data effective number parameters determined data remaining parameters set small values prior reﬂected bayesian result variance factor denominator thereby correcting bias maximum likelihood result illustrate evidence framework setting hyperparameters using sinusoidal synthetic data set section together gaussian basis function model comprising basis functions total number parameters model given including bias simplicity illustration set true value used evidence framework determine shown figure also see parameter controls magnitude parameters plotting individual parameters versus effective number parameters shown figure consider limit number data points large relation number parameters parameters well determined data involves implicit sum data points eigenvalues increase size data set case estimation equations become deﬁned respectively results used easy compute approximation full evidence estimation linear models regression figure left plot shows red curve blue curve versus sinusoidal synthetic data set intersection two curves deﬁnes optimum value given evidence procedure right plot shows corresponding graph log evidence versus red figure plot parameters hyperparameter varied range causing vary range exercises might appear therefore linear models constitute general purpose difﬁculty stems assumption basis functions ﬁxed fortunately two properties real datasets exploit help alleviate problem first data vectors typically lie close non 
"
55,"[linear, models, regression, exercises]"," show tanh function logistic sigmoid function related tanh equivalent linear combination tanh functions form tanh ﬁnd expressions relate new parameters original parameters linear models regression show matrix shown figure consider data set data point associated weighting factor sum squares error function becomes find expression solution minimizes error function give two consider linear model form together sum squares error function form suppose gaussian noise zero mean variance added dependently input variables making use show minimizing averaged noise distribution omitted regularizer using technique lagrange multipliers discussed appendix consider linear basis function regression model multivariate target variable gaussian distribution form exercises together training data set comprising input basis vectors corre sponding target vectors show maximum likelihood solution parameter matrix property column deﬁned respectively consider linear basis function model section suppose already observed data points posterior distribution completing replaced replaced repeat previous exercise instead completing square hand make use general result linear gaussian models given making use result evaluate integral show uncertainty associated linear regression function given satisﬁes saw section conjugate prior gaussian distribution linear regression model consider likelihood function conjugate prior given gam linear models regression gam ﬁnd expressions posterior parameters show predictive distribution model discussed ercise given student distribution form obtain expressions exercise explore detail properties equivalent kernel deﬁned deﬁned suppose basis functions linearly independent number data points taking suitable linear combinations basis functions construct new basis set spanning space orthonormal deﬁned otherwise take show equivalent kernel written use result show kernel satisﬁes summation constraint consider linear basis function model regression deﬁned satisﬁes relation derive result log evidence function linear regression model making use evaluate integral directly show evidence function bayesian linear regression model written form deﬁned completing square show error function bayesian linear regression written form exercises starting verify steps needed show maxi starting verify steps needed show maximiza show marginal probability data words model evidence model described exercise given ﬁrst marginalizing respect respect repeat previous exercise use bayes theorem form 
"
56,"[linear, models, classiﬁcation]"," previous chapter explored class regression models particularly simple analytical computational properties discuss analogous class models solving classiﬁcation problems goal classiﬁcation take input vector assign one discrete classes common scenario classes taken disjoint input assigned one one class input space thereby divided decision regions whose boundaries called decision boundaries decision surfaces chapter consider linear models classiﬁcation mean decision surfaces linear functions input vector hence deﬁned dimensional hyperplanes within dimensional input space datasets whose classes separated exactly linear decision surfaces said linearly separable regression problems target variable simply vector real numbers whose values wish predict case classiﬁcation various linear models classification ways using target values represent class labels probabilistic models convenient case two class problems binary representation single target variable represents class represents class interpret value probability class values probability taking extreme values classes convenient use coding scheme vector length class elements zero except element takes value instance classes pattern class would given target vector interpret value probability class nonprobabilistic models alternative choices target variable representation sometimes prove convenient chapter identiﬁed three distinct approaches classiﬁcation prob lem simplest involves constructing discriminant function directly assigns vector speciﬁc class powerful approach however models conditional probability distribution inference stage subse quently uses distribution make optimal decisions separating inference decision gain numerous beneﬁts discussed section two different approaches determining conditional probabilities one technique model directly example representing parametric models optimizing parameters using training set alternatively adopt generative approach model class conditional densities given together prior probabilities classes compute required posterior probabilities using bayes theorem shall discuss examples three approaches chapter linear regression models considered chapter model prediction given linear function parameters simplest case model also linear input variables therefore takes form real number classiﬁcation problems however wish predict discrete class labels generally posterior probabilities lie range achieve consider generalization model transform linear function using nonlinear function machine learning literature known activation function whereas inverse called link function statistics literature decision surfaces correspond constant constant hence decision surfaces linear functions even function nonlinear reason class models described called generalized linear models discriminant functions lead complex analytical computa algorithms discussed chapter equally applicable ﬁrst regression models chapter begin consider 
"
57,"[linear, models, classiﬁcation, discriminant, functions]"," chapter shall restrict attention linear discriminants 
"
58,"[linear, models, classiﬁcation, discriminant, functions, two, classes]"," simplest representation linear discriminant function obtained taking linear function input vector called weight vector bias confused bias class otherwise cor dimensional hyperplane within dimensional input space consider two points lie decision surface hence vector therefore see bias parameter determines location decision surface properties illustrated case figure furthermore note value gives signed measure per pendicular distance point decision surface see consider linear models classification figure illustration geometry displacement origin controlled bias parameter decision surface given discriminant functions figure attempting construct class discriminant set two class discriminants leads points class right example involving three discriminant functions used separate pair classes alternative introduce binary discriminant functions one avoid difﬁculties considering single class discriminant comprising linear functions form assigning point class decision boundary class class therefore given hence corresponds dimensional hyperplane deﬁned decision regions discriminant always singly connected convex see consider two points lie inside decision region illustrated figure point expressed form linear models classification figure lie inside decision gion point lies line connecting two points must also lie hence decision region must singly connected convex discriminant functions matrix whose column comprises dimensional vector corresponding augmented input vector dummy input representation discussed detail section new input assigned class output largest determine parameter matrix minimizing sum squares error function regression chapter consider training data set deﬁne matrix whose row vector together matrix whose row sum squares error function written setting derivative respect zero rearranging obtain solution form pseudo inverse matrix discussed section obtain discriminant function form interesting property least squares solutions multiple target variables every target vector training set satisﬁes linear constraint constants model prediction value satisfy constraint exercise thus use coding scheme classes predictions made model property elements sum value however summation constraint alone sufﬁcient allow model outputs interpreted probabilities constrained lie within interval least squares approach gives exact closed form solution discrimi nant function parameters however even discriminant function use make decisions directly dispense probabilistic interpretation suf fers severe problems already seen least squares solutions section lack robustness outliers applies equally classiﬁcation application illustrated figure see additional data points right hand ﬁgure produce signiﬁcant change location decision boundary even though point would correctly classiﬁed original decision boundary left hand ﬁgure sum squares error function penalizes predictions correct lie long way correct side decision linear models classification figure left plot shows data two classes denoted red crosses blue circles together decision boundary found least squares magenta curve also logistic regression model green curve discussed later section right hand plot shows corresponding results obtained extra data points added bottom left diagram showing least squares highly sensitive outliers unlike logistic regression boundary section shall consider several alternative error functions classiﬁcation shall see suffer difﬁculty however problems least squares severe simply lack robustness illustrated figure shows synthetic data set drawn three classes two dimensional input space property linear decision boundaries give excellent separation classes indeed technique logistic regression described later chapter gives satisfactory solution seen right hand plot however least squares solution gives poor results small region input space assigned green class failure least squares surprise recall cor responds maximum likelihood assumption gaussian conditional distribution whereas binary target vectors clearly distribution far gaussian adopting appropriate probabilistic models shall obtain clas siﬁcation techniques much better properties least squares moment however continue explore alternative nonprobabilistic methods setting parameters linear classiﬁcation models 
"
59,"[linear, models, classiﬁcation, discriminant, functions, fisher’s, linear, discriminant]"," one way view linear classiﬁcation model terms dimensionality reduction consider ﬁrst case two classes suppose take discriminant functions figure example synthetic data set comprising three classes training data points denoted red dimensional input vector project one dimension using place threshold classify class otherwise class obtain standard linear classiﬁer discussed previous section points class points class mean vectors two classes given linear models classification figure left plot shows samples two classes depicted red blue along histograms mean projected data class however expression using lagrange multiplier perform constrained maximization ﬁnd still problem approach however illustrated considerable overlap projected onto projection formula transforms set labelled data points therefore given deﬁne total within class variance whole data set simply fisher criterion deﬁned ratio class variance within class variance given discriminant functions class covariance matrix given total within class covariance matrix given differentiating respect ﬁnd maximized see always direction furthermore care magnitude direction drop scalar factors multiplying sides obtain note within class covariance isotropic proportional unit matrix ﬁnd proportional difference class means discussed result known fisher linear discriminant although strictly discriminant rather speciﬁc choice direction projection data one dimension however projected data subsequently used construct discriminant choosing threshold classify new point belonging classify belonging otherwise example model class conditional densities using gaussian distributions use techniques section ﬁnd parameters gaussian distributions maximum likelihood found gaussian proximations projected classes formalism section gives expression optimal threshold justiﬁcation gaussian assumption comes central limit theorem noting sum set random variables 
"
60,"[linear, models, classiﬁcation, discriminant, functions, relation, least, squares]"," least squares approach determination linear discriminant based goal making model predictions close possible set target values contrast fisher criterion derived requiring maximum class separation output space interesting see relationship two approaches particular shall show two class problem fisher criterion obtained special case least squares far considered coding target values however adopt slightly different target coding scheme least squares solution linear models classification weights becomes equivalent fisher solution duda hart particular shall take targets class number patterns class total number patterns target value approximates reciprocal prior probability class class shall take targets number patterns class sum squares error function written setting derivatives respect zero obtain respectively making use choice target coding scheme obtain expression bias form used mean total data set given straightforward algebra making use choice second equation becomes exercise deﬁned deﬁned substituted bias using using note always direction thus write ignored irrelevant scale factors thus weight vector coincides found fisher criterion addition also found expression bias value given tells new vector classiﬁed belonging class class otherwise discriminant functions 
"
61,"[linear, models, classiﬁcation, discriminant, functions, fisher’s, discriminant, multiple, classes]"," consider generalization fisher discriminant classes shall assume dimensionality input space greater number classes next introduce linear features feature values conveniently grouped together form vector similarly weight vectors considered columns matrix note including bias parameters deﬁnition generalization within class covariance matrix case classes follows give number patterns class order ﬁnd generalization class covariance matrix follow duda hart consider ﬁrst total covariance matrix mean total data set total number data points total covariance matrix decomposed sum within class covariance matrix given plus additional matrix identify measure class covariance linear models classification dimensional space criterion rewritten explicit function projection matrix form correspond largest eigenvalues one important result common criteria worth emphasizing ﬁrst note composed sum matrices independent result constraint thus rank equal nonzero eigenvalues shows projection onto dimensional subspace spanned eigenvectors alter value therefore unable ﬁnd linear features means fukunaga 
"
62,"[linear, models, classiﬁcation, discriminant, functions, perceptron, algorithm]"," another example linear discriminant model perceptron rosenblatt used construct generalized linear model form discriminant functions nonlinear activation function given step function form vector typically include bias component earlier appropriate context probabilistic class matches choice activation function algorithm used determine parameters perceptron therefore consider alternative error function known perceptron cri class whereas patterns class using target coding scheme follows would like patterns satisfy perceptron criterion tries minimize quantity perceptron criterion therefore given 
"
63,"[linear, models, classiﬁcation, frank, rosenblatt]"," linear models classification denotes set misclassiﬁed patterns contribution error associated particular misclassiﬁed pattern linear function regions space pattern misclassiﬁed zero regions correctly classiﬁed total error function therefore piecewise linear apply stochastic gradient descent algorithm error function section change weight vector given learning rate parameter integer indexes steps algorithm perceptron function unchanged multiply constant set learning rate parameter equal without generality note weight vector evolves training set patterns misclassiﬁed change perceptron learning algorithm simple interpretation follows cycle training patterns turn pattern evaluate perceptron function pattern correctly classiﬁed weight vector remains unchanged whereas incorrectly classiﬁed class add vector onto current estimate weight vector class subtract vector perceptron learning algorithm illustrated figure consider effect single update perceptron learning algorithm see contribution error misclassiﬁed pattern reduced set made use course imply contribution error function misclassiﬁed patterns reduced furthermore change weight vector may caused previously correctly classiﬁed patterns become misclassiﬁed thus perceptron learning rule guaranteed reduce total error function stage however perceptron convergence theorem states exists act solution words training data set linearly separable perceptron learning algorithm guaranteed ﬁnd exact solution ﬁnite number steps proofs theorem found example rosenblatt block nilsson minsky papert hertz bishop note however number steps required achieve con vergence could still substantial practice convergence achieved able distinguish nonseparable problem one simply slow converge even data set linearly separable may many solutions one found depend initialization parameters der presentation data points furthermore datasets linearly separable perceptron learning algorithm never converge discriminant functions figure illustration convergence perceptron learning algorithm showing data points two classes red blue two dimensional feature space top left plot shows initial parameter vector shown black arrow together corresponding decision boundary black line arrow points towards decision region classiﬁed belonging red class data point circled green misclassiﬁed feature vector added current weight vector giving new decision boundary shown top right plot bottom left plot shows next misclassiﬁed point considered indicated green circle feature vector added weight vector giving decision boundary shown bottom right plot data points correctly classiﬁed linear models classification figure illustration mark perceptron hardware photograph left shows inputs array cadmium sulphide photocells aside difﬁculties learning algorithm perceptron pro analogue hardware implementations perceptron built rosenblatt based motor driven variable resistors implement adaptive parameters could time perceptron developed closely related 
"
64,"[linear, models, classiﬁcation, probabilistic, generative, models]"," probabilistic generative models figure plot logistic sigmoid function shown dashed blue deﬁned scaling factor chosen approach model class conditional densities well class priors use compute posterior probabilities bayes theorem consider ﬁrst case two classes posterior probability class written exp deﬁned logistic sigmoid function deﬁned exp easily veriﬁed inverse logistic sigmoid given two classes also known log odds linear models classification note simply rewritten posterior probabilities equivalent form appearance logistic sigmoid may seem rather vacuous however signiﬁcance provided takes simple functional form shall shortly consider situations linear function case posterior probability governed generalized linear model case classes exponent known normalized exponential regarded multiclass generalization logistic sigmoid quantities deﬁned normalized exponential also known softmax function represents smoothed version max function investigate consequences choosing speciﬁc forms class conditional densities looking ﬁrst continuous input variables discussing brieﬂy case discrete inputs 
"
65,"[linear, models, classiﬁcation, probabilistic, generative, models, continuous, inputs]"," let assume class conditional densities gaussian explore resulting form posterior probabilities start shall assume classes share covariance matrix thus density class given exp consider ﬁrst case two classes deﬁned see quadratic terms exponents gaussian densities cancelled due assumption common covariance matrices leading linear function argument logistic sigmoid result illus trated case two dimensional input space figure resulting probabilistic generative models figure left hand plot shows class conditional densities two classes denoted red blue right corresponding posterior probability given logistic sigmoid linear function surface right hand plot coloured using proportion red ink given proportion blue ink given constant given linear functions therefore decision boundaries linear input space prior probabilities enter bias parameter changes priors effect general case classes deﬁned see linear functions consequence cancel relax assumption shared covariance matrix allow class conditional density covariance matrix earlier linear models classification figure left hand plot shows class conditional densities three classes gaussian 
"
66,"[linear, models, classiﬁcation, probabilistic, generative, models, maximum, likelihood, solution]"," speciﬁed parametric functional form class conditional densities determine values parameters together prior class probabilities using maximum likelihood requires data set comprising observations along corresponding class labels consider ﬁrst case two classes gaussian class conditional density shared covariance matrix suppose data set denotes class denotes class denote prior class probability data point class hence similarly class hence thus likelihood function given usual convenient maximize log likelihood function consider ﬁrst maximization respect terms probabilistic generative models log likelihood function depend setting derivative respect equal zero rearranging obtain denotes total number data points class denotes total number data points class thus maximum likelihood estimate simply fraction points class expected result easily generalized multiclass case maximum likelihood estimate prior probability associated class given fraction training set points assigned class exercise consider maximization respect pick log likelihood function terms depend giving const setting derivative respect zero rearranging obtain simply mean input vectors assigned class similar argument corresponding result given mean input vectors assigned class finally consider maximum likelihood solution shared covariance matrix picking terms log likelihood function depend linear models classification deﬁned result easily extended class problem obtain corresponding exercise section let consider case discrete feature values simplicity begin looking binary feature values discuss extension numbers class containing section treated independent conditioned class thus class conditional distributions form linear functions input values case classes exercise seen gaussian distributed discrete inputs posterior class probabilities given generalized linear models logistic sigmoid probabilistic discriminative models classes softmax classes activation functions particular cases members exponential family distributions using form members exponential family see distribution written form exponent note allowing class parameter vector assuming classes share scale parameter two class problem substitute expression class conditional linear function 
"
67,"[linear, models, classiﬁcation, probabilistic, discriminative, models]"," written logistic sigmoid acting linear function wide choice class conditional distributions similarly multiclass case posterior probability class given softmax transformation linear function speciﬁc choices class conditional densities used bayes theorem ﬁnd posterior class probabilities however alternative approach use functional form generalized indirect approach ﬁnding parameters generalized linear model ﬁtting class conditional densities class priors separately applying linear models classification figure illustration role nonlinear basis functions linear classiﬁcation models left plot shows original input space together data points two classes labelled red blue two gaussian basis functions deﬁned space centres shown green crosses contours shown green circles right hand plot shows corresponding feature space together linear decision boundary obtained given logistic regression model form far chapter considered classiﬁcation models work feature space space note discussion linear models regression one probabilistic discriminative models basis functions typically set constant say corresponding parameter plays role bias remainder chapter shall include ﬁxed basis function transformation highlight useful similarities regression models discussed chapter many problems practical interest signiﬁcant overlap class conditional densities corresponds posterior probabilities least values cases opti cannot remove class overlap indeed increase ﬁxed basis function models important limitations section 
"
68,"[linear, models, classiﬁcation, probabilistic, discriminative, models, logistic, regression]"," begin treatment generalized linear models considering problem written logistic sigmoid acting linear function feature vector logistic sigmoid function deﬁned dimensional feature space model adjustable parameters use maximum likelihood determine parameters logistic exercise linear models classification data set likelihood function written usual deﬁne error taking gradient error function respect obtain exercise target value prediction model times basis function vector furthermore comparison section desired could make use result give sequential algorithm term worth noting maximum likelihood exhibit severe ﬁtting furthermore typically continuum exercise probabilistic discriminative models 
"
69,"[linear, models, classiﬁcation, probabilistic, discriminative, models, iterative, reweighted, least, squares]"," case linear regression models discussed chapter maxi new old let ﬁrst apply newton raphson method linear regression design matrix whose row given newton section raphson update takes form new old old let apply newton raphson update cross entropy error function linear models classification made use also introduced diagonal matrix elements follows form logistic sigmoid function see arbitrary vector hessian matrix exercise newton raphson update formula logistic regression model comes new old rφw old dimensional vector elements old var used property fact interpret irls solution linearized problem space variable quantity corresponds element given simple old old old probabilistic discriminative models 
"
70,"[linear, models, classiﬁcation, probabilistic, discriminative, models, multiclass, logistic, regression]"," discussion generative models multiclass classiﬁcation section exponent activations given consider use maximum likelihood determine parameters model directly require derivatives respect activations given exercise elements identity matrix next write likelihood function easily done using coding scheme target vector feature vector belonging class binary vector elements zero except element equals one likelihood function given matrix target variables elements taking negative logarithm gives take gradient error function respect one parameter vectors making use result derivatives softmax function obtain exercise linear models classification made use see form arising times basis function could use seen derivative log likelihood function linear regres times feature vector similarly combination ﬁnd batch algorithm appeal newton raphson update block given seen broad range class conditional distributions described activation function one way motivate alternative choice link function consider noisy threshold model follows input evaluate set target value according otherwise probabilistic discriminative models figure schematic example probability density value exceeds threshold oth erwise takes value equivalent activation function given cumulative distribution function illustrated figure speciﬁc example suppose density given zero mean exp determine parameters model using maximum likelihood linear models classification however ﬁnd another use probit model discuss bayesian treatments logistic regression section one issue occur practical applications outliers arise instance errors measuring input vector misla belling target value points lie long way wrong side ideal decision boundary seriously distort classiﬁer note logistic probit regression models behave differently respect tails logistic sigmoid decay asymptotically like exp whereas probit activation function decay like exp probit model signiﬁcantly sensitive outliers however logistic probit models assume data correctly labelled effect mislabelling easily incorporated probabilistic model introducing probability target value ﬂipped wrong value opper winther leading target value distribution data point form activation function input vector may set advance may treated hyperparameter whose value inferred data 
"
71,"[linear, models, classiﬁcation, probabilistic, discriminative, models, canonical, link, functions]"," linear regression model gaussian noise distribution error function corresponding negative log likelihood given take derivative respect parameter vector contribution error function data point takes form error times feature vector similarly combination logistic sigmoid activation function cross entropy error function softmax activation function multiclass cross entropy error function obtain simple form show general result assuming conditional distribution target variable exponential family along corresponding choice activation function known canonical link function make use restricted form exponential family distributions note applying assumption exponential family distribution target variable contrast section applied input vector therefore consider conditional distributions target variable form exp using line argument led derivation result see conditional mean denote given laplace approximation thus must related denote relation following nelder wedderburn deﬁne generalized linear model known activation function machine learning literature known link function statistics consider log likelihood function model function given const used together result see considerable simpliﬁcation choose particular form link function given gives hence also hence case gradient error function reduces gaussian whereas logistic model 
"
72,"[linear, models, classiﬁcation]"," linear models classification parameter vector since posterior distribution longer gaussian therefore necessary introduce form approximation later book shall consider range techniques based analytical approximations chapter numerical sampling chapter introduce simple widely used framework called laplace proximation aims ﬁnd gaussian approximation probability density deﬁned set continuous variables consider ﬁrst case single continuous variable suppose distribution deﬁned normalization coefﬁcient shall suppose value unknown laplace method goal ﬁnd gaussian approx imation centred mode distribution ﬁrst step ﬁnd mode words point equivalently gaussian distribution property logarithm quadratic function variables therefore consider taylor expansion centred mode note ﬁrst order term taylor expansion appear since local maximum distribution taking exponential obtain exp obtain normalized distribution making use standard result normalization gaussian exp laplace approximation illustrated figure note gaussian approximation well deﬁned precision words stationary point must local maximum second derivative point negative laplace approximation figure illustration laplace approximation applied distribution exp logistic sigmoid function deﬁned left plot shows normalized distribution yellow together laplace approximation centred mode red right plot shows negative logarithms corresponding curves extend laplace method approximate distribution deﬁned dimensional space stationary point gradient vanish expanding around stationary point hessian matrix deﬁned gradient operator taking exponential sides obtain exponent denotes determinant gaussian distribution well must local maximum minimum saddle point order apply laplace approximation ﬁrst need ﬁnd mode linear models classification one major weakness laplace approximation since based consider laplace approximation 
"
73,"[linear, models, classiﬁcation, model, comparison, bic]"," well approximating distribution also obtain approximation normalization constant using approximation exp consider data set set models parameters model deﬁne likelihood function introduce prior parameters interested computing model evi dence various models omit conditioning keep notation uncluttered bayes theorem model evidence given identifying applying result obtain exercise map bayesian logistic regression map value mode posterior distribution hessian matrix second derivatives negative log posterior map assume gaussian prior distribution parameters broad hessian full rank approximate roughly using exercise map number data points number parameters complexity measures aic bic virtue easy section 
"
74,"[linear, models, classiﬁcation, bayesian, logistic, regression, laplace, approximation]"," recall section laplace approximation obtained ﬁnding seek gaussian representation posterior distribution natural begin gaussian prior write general form linear models classification ﬁxed hyperparameters posterior distribution given taking log sides substituting prior distribution using likelihood function using obtain const obtain gaussian approximation posterior distributionﬁrst maximize posterior distribution give map maximum posterior solution map deﬁnes mean gaussian covariance given inverse matrix second derivatives negative log likelihood takes form gaussian approximation posterior distribution therefore takes form map obtained gaussian approximation posterior distribution remains task marginalizing respect distribution order make predictions 
"
75,"[linear, models, classiﬁcation, bayesian, logistic, regression, predictive, distribution]"," predictive distribution class given new feature vector obtained marginalizing respect posterior distribution approximated gaussian distribution corresponding probability class given evaluate predictive distribution ﬁrst note function pends projection onto denoting dirac delta function obtain bayesian logistic regression evaluate noting delta function imposes linear constraint forms marginal distribution joint distribution inte grating directions orthogonal gaussian know section marginal distribution also gaussian evaluate mean covariance distribution taking moments interchanging order integration map used result variational posterior distribution similarly var note distribution takes form predictive distribution linear regression model noise variance set zero thus variational approximation predictive distribution becomes result also derived directly making use results marginal gaussian distribution given section exercise integral represents convolution gaussian logistic sigmoid cannot evaluated analytically however obtain good approx imation spiegelhalter lauritzen mackay barber bishop making use close similarity logistic sigmoid function deﬁned probit function deﬁned order obtain best approximation logistic function need scale hori zontal axis approximate ﬁnd suitable value requiring two functions slope origin gives similarity logistic sigmoid probit function exercise choice illustrated figure advantage using probit function convolution gaussian expressed analytically terms another probit function speciﬁcally show exercise linear models classification apply approximation probit functions appearing deﬁned applying result obtain approximate predictive distribution form deﬁned respectively ﬁned note decision boundary corresponding given decision boundary obtained using map 
"
76,"[linear, models, classiﬁcation, exercises]"," given set data points deﬁne convex hull set points given consider second set points together scalar show convex hulls intersect two consider minimization sum squares error function suppose target vectors training set satisfy linear constraint corresponds row matrix show exercises assume one basis functions corresponding parameter plays role bias extend result exercise show multiple linear constraints show maximization class separation criterion given respect using lagrange multiplier enforce constraint leads result show logistic sigmoid function satisﬁes property inverse given consider generative classiﬁcation model classes deﬁned prior class probabilities general class conditional densities input feature vector suppose given training data set binary target vector length uses coding scheme components pattern class number data points assigned class consider classiﬁcation model exercise suppose given linear models classification represents mean feature vectors assigned class similarly consider classiﬁcation problem classes feature vector components independent given appear argument note represents example naive bayes model discussed section verify relation derivative logistic sigmoid function deﬁned making use result derivative logistic sigshow linearly separable data set maximum likelihood solution separates classes taking magnitude inﬁnity show hessian matrix logistic regression model given positive deﬁnite diagonal matrix elements output logistic regression model input vector hence show error function concave function unique minimum consider binary classiﬁcation problem observation known instead value class label instead value representing probability given probabilistic model write log likelihood function appropriate data set exercises show derivatives softmax activation function deﬁned given using result derivatives softmax activation function show gradients cross entropy error given write expressions gradient log likelihood well corresponding hessian matrix probit regression model deﬁned section quantities would required train model using irls show hessian matrix multiclass logistic regression problem deﬁned positive semideﬁnite note full hessian matrix problem size number parameters number classes prove positive semideﬁnite property consider product arbitrary vector length apply jensen inequality show probit function function related using result derive expression log model evi dence laplace approximation exercise derive bic result starting laplace approximation model evidence given show prior parameters gaussian form log model evidence laplace approximation takes form map const matrix second derivatives log likelihood evaluated map assume prior broad small second term right hand side neglected furthermore consider case independent identically distributed data sum terms one data point show log model evidence written approximately form bic expression use results section derive result marginal ization logistic regression model respect gaussian posterior distribution parameters suppose wish approximate logistic sigmoid deﬁned scaled probit function deﬁned show chosen derivatives two functions equal linear models classification exercise prove relation convolution probit function gaussian distribution show derivative left hand side respect equal derivative right hand side integrate sides respect show constant integration vanishes note differentiating left hand side convenient ﬁrst introduce change variable given integral replaced integral differentiate left hand side relation obtain gaussian integral evaluated analytically 
"
77,"[neural, networks]"," chapters considered models regression classiﬁcation comprised linear combinations ﬁxed basis functions saw models useful analytical computational properties practical applicability limited curse dimensionality order apply models large scale problems necessary adapt basis functions data support vector machines svms discussed chapter address ﬁrst deﬁning basis functions centred training data points selecting subset training one advantage svms although training involves nonlinear optimization objective function convex solution optimization problem relatively straightforward number basis functions resulting models generally much smaller number training points although often still relatively large typically increases size training set relevance vector machine discussed section also chooses subset ﬁxed set basis functions typically results much neural networks sparser models unlike svm also produces probabilistic outputs although expense nonconvex optimization training alternative approach number basis functions advance allow adaptive words use parametric forms basis functions parameter values adapted training successful model type context pattern recognition feed forward neural network also known multilayer perceptron discussed chapter fact multilayer perceptron really misnomer model comprises multi ple layers logistic regression models continuous nonlinearities rather multiple perceptrons discontinuous nonlinearities many applications resulting model signiﬁcantly compact hence faster evaluate support vector machine generalization performance price paid compactness relevance vector machine like lihood function forms basis network training longer convex function model parameters practice however often worth investing substantial computational resources training phase order obtain compact model fast processing new data term neural network origins attempts ﬁnd mathematical rep resentations information processing biological systems mcculloch pitts widrow hoff rosenblatt rumelhart indeed used broadly cover wide range different models many subject exaggerated claims regarding biological plau sibility perspective practical applications pattern recognition ever biological realism would impose entirely unnecessary constraints focus chapter therefore neural networks efﬁcient models statistical pattern recognition particular shall restrict attention speciﬁc class neural networks proven greatest practical value namely multilayer perceptron begin considering functional form network model including speciﬁc parameterization basis functions discuss prob lem determining network parameters within maximum likelihood frame work involves solution nonlinear optimization problem requires evaluation derivatives log likelihood function respect net work parameters shall see obtained efﬁciently using technique error backpropagation shall also show backpropagation framework extended allow derivatives evaluated jacobian hessian matrices next discuss various approaches regularization neural network training relationships also consider extensions neural network model particular describe general framework modelling conditional probability distributions known mixture density networks finally discuss use bayesian treatments neural net works additional background neural network models found bishop feed forward network functions 
"
78,"[neural, networks, feed-forward, network, functions]"," take form nonlinear activation function case classiﬁcation depend parameters allow parameters adjusted along coefﬁcients training course leads basic neural network model described series form weights parameters biases following nomenclature chapter quantities known activations transformed using differentiable nonlinear activation function give exercise unit activations bias parameters choice activation function determined nature data assumed distribution target variables neural networks figure neural network corre sponding input arrows denote direc network forward propagation hidden units inputs outputs similarly multiple binary classiﬁcation problems output unit activation transformed using logistic sigmoid function exp combine various stages give overall network function sigmoidal output unit activation functions takes form adjustable parameters function represented form network diagram shown feed forward network functions notation two kinds model shall see later give probabilistic interpretation neural network discussed section bias parameters absorbed set weight parameters deﬁning additional input variable whose value clamped takes form similarly absorb second layer biases second layer weights overall network function becomes seen figure neural network model comprises two stages processing resembles perceptron model section reason neural network also known multilayer perceptron mlp key difference compared perceptron however neural net work uses continuous sigmoidal nonlinearities hidden units whereas per ceptron uses step function nonlinearities means neural network function differentiable respect network parameters property play central role network training activation functions hidden units network taken linear network always ﬁnd equivalent network without hidden units follows fact composition successive linear transformations linear transformation however number hidden units smaller either number input output units transformations network generate general possible linear trans formations inputs outputs information lost dimensionality reduction hidden units section show networks linear units give rise principal component analysis general however little interest multilayer networks linear units network architecture shown figure commonly used one practice however easily generalized instance considering additional layers processing consisting weighted linear combination form followed element wise transformation using nonlinear activation function note confusion literature regarding terminology counting number layers networks thus network figure may described layer network counts number layers units treats inputs units sometimes single hidden layer network counts number layers hidden units recommend terminology figure called two layer network number layers adap tive weights important determining network properties another generalization network architecture include skip layer con nections associated corresponding adaptive parameter neural networks figure inputs outputs furthermore network sparse possible connections within direct correspondence network diagram approximation properties feed forward networks widely stud feed forward network functions figure illustration pability multilayer perceptron approximate four different functions comprising heaviside step function case data points shown blue dots sam pled uniformly interval evaluated data points used train two layer network hidden units tanh activation functions linear output units resulting network functions shown red curves outputs three hidden units shown three dashed curves capability two layer network model broad range functions 
"
79,"[neural, networks, feed-forward, network, functions, weight-space, symmetries]"," one property feed forward networks play role consider bayesian model comparison multiple distinct choices weight vector tanh neural networks figure tanh activation functions single dashed blue lines show contours decision surface net symmetries thus given weight vector one set equivalent weight vectors similarly imagine interchange values weights turns factors account symmetries weight space 
"
80,"[neural, networks, network, training]"," together corresponding set network training target vectors minimize error function however provide much general view network training ﬁrst giving probabilistic interpretation network outputs already seen many advantages using probabilistic predictions section also provide clearer motivation choice output unit nonlinearity choice error function start discussing regression problems moment consider single target variable take real value following discussions section assume gaussian distribution dependent mean given output neural network precision inverse variance gaussian noise course somewhat restrictive assumption section shall see extend approach allow general conditional distributions conditional distribution given sufﬁcient take output unit activation function identity network approximate continuous function given data set independent identically distributed observations along corresponding target values construct corresponding likelihood function taking negative logarithm obtain error function used learn parameters section shall discuss bayesian treatment neural networks consider maximum likelihood approach note neural networks literature usual consider minimization error function rather maximization log likelihood shall follow convention consider ﬁrst determi nation maximizing likelihood function equivalent minimizing sum squares error function given neural networks discarded additive multiplicative constants value found minimizing denoted corresponds maximum likelihood solution practice nonlinearity network function causes error nonconvex practice local maxima likelihood may found corresponding local minima error function discussed section found value found minimizing negative log likelihood give note evaluated iterative optimization required ﬁnd completed multiple target variables assume independent conditional shared noise precision conditional distribution target values given following argument single target variable see maximum likelihood weights determined minimizing sum squares error function noise precision given exercise number target variables assumption independence dropped expense slightly complex optimization problem exercise recall section natural pairing error function given negative log likelihood output unit activation function regression case view network output activation function identity corresponding sum squares error function property shall make use discussing error backpropagation section consider case binary classiﬁcation single target variable denotes class denotes class following discussion canonical link functions section consider network single output whose activation function logistic sigmoid exp interpret conditional probability given conditional distribution targets given inputs bernoulli distribution form network training consider training set independent observations error function given negative log likelihood cross entropy error function form denotes note analogue noise precision target values assumed correctly labelled however model easily extended allow labelling errors simard found using exercise cross entropy error function instead sum squares classiﬁcation problem leads faster training well improved generalization separate binary classiﬁcations perform use net work outputs logistic sigmoid activation function associated output binary class label assume class labels independent given input vector conditional distribution targets taking negative logarithm corresponding likelihood function gives following error function exercise denotes derivative error function spect activation particular output unit takes form exercise regression case interesting contrast neural network solution problem corresponding approach based linear classiﬁcation model kind discussed chapter suppose using standard two layer network kind shown figure see weight parameters ﬁrst layer network shared various outputs whereas linear model classiﬁcation problem solved independently ﬁrst layer network viewed performing nonlinear feature extraction sharing features different outputs save computation also lead improved generalization finally consider standard multiclass classiﬁcation problem input assigned one mutually exclusive classes binary target variables coding scheme indicating class network outputs interpreted leading following error function neural networks figure geometrical view error function surface sitting weight space point local minimum global minimum point local gradient error surface given vector network training point weight space gradient error function vanishes otherwise could make small step direction thereby reduce error points gradient vanishes called stationary points may classiﬁed minima maxima saddle points goal ﬁnd vector takes smallest value ever error function typically highly nonlinear dependence weights bias parameters many points weight space gradient vanishes numerically small indeed discussion section see point local minimum points weight space equivalent minima instance two layer net work kind shown figure hidden units point weight space member family equivalent points section furthermore typically multiple inequivalent stationary points particular multiple inequivalent minima minimum corresponds smallest value error function weight vector said global minimum minima corresponding higher values error function said local minima successful application neural networks may necessary ﬁnd global minimum general known whether global minimum found may necessary compare several local minima order ﬁnd sufﬁciently good solution clearly hope ﬁnding analytical solution equation resort iterative numerical procedures optimization continuous nonlinear functions widely studied problem exists tensive literature solve efﬁciently techniques involve choosing initial value weight vector moving weight space succession steps form labels iteration step different algorithms involve different choices weight vector update many algorithms make use gradient information therefore require update value evaluated new weight vector order understand importance gradient information useful consider local approximation error function based taylor expansion 
"
81,"[neural, networks, network, training, local, quadratic, approximation]"," insight optimization problem various techniques solving obtained considering local quadratic approximation error function consider taylor expansion around point weight space neural networks cubic higher terms omitted deﬁned gradient evaluated hessian matrix elements corresponding local approximation gradient given points sufﬁciently close expressions give reasonable approximations error gradient consider particular case local quadratic approximation around point minimum error function case linear term becomes hessian evaluated order interpret geometrically consider eigenvalue equation hessian matrix eigenvectors form complete orthonormal set appendix expand linear combination eigenvectors form regarded transformation coordinate system origin translated point axes rotated align eigenvectors orthogonal matrix whose columns discussed detail appendix substituting using allows error function written form matrix said positive deﬁnite network training figure error function contours con hes neural networks evaluations would require steps thus computational effort needed ﬁnd minimum using approach would compare algorithm makes use gradient information evaluation brings items information might hope ﬁnd minimum function gradient evaluations shall see using error backpropagation evaluation takes steps minimum found steps reason use gradient information forms basis practical algorithms training neural networks 
"
82,"[neural, networks, network, training, gradient, descent, optimization]"," simplest approach using gradient information choose weight update comprise small step direction negative gradient parameter known learning rate update gradient evaluated new weight vector process repeated note error function deﬁned respect training set step requires entire training set processed order evaluate techniques use whole data set called batch methods step weight vector moved direction greatest rate decrease error function approach known gradient descent steepest descent although approach might intuitively seem reasonable fact turns poor algorithm reasons discussed bishop nabney batch optimization efﬁcient methods conjugate gra dients quasi newton methods much robust much faster simple gradient descent gill fletcher nocedal wright unlike gradient descent algorithms property error function always decreases iteration unless weight vector arrived local global minimum order ﬁnd sufﬁciently good minimum may necessary run gradient based algorithm multiple times time using different randomly cho sen starting point comparing resulting performance independent vali dation set however line version gradient descent proved useful practice training neural networks large datasets error functions based maximum likelihood set independent observations comprise sum terms one data point line gradient descent also known sequential gradient descent stochastic gradient descent makes update weight vector based one data point time error backpropagation update repeated cycling data either sequence selecting points random replacement course intermediate scenarios updates based batches data points one advantage line methods compared batch methods former handle redundancy data much efﬁciently see consider treme example take data set double size duplicating every data point note simply multiplies error function factor equivalent using original error function batch methods require double computational effort evaluate batch error function gradient whereas line methods unaffected another property line gradient descent possibility escaping local minima since stationary point respect error function whole data set generally stationary point data point individually nonlinear optimization algorithms practical application neural net work training discussed detail bishop nabney error backpropagation goal section ﬁnd efﬁcient technique evaluating gradient error function feed forward neural network shall see achieved using local message passing scheme information sent alternately forwards backwards network known error backpropagation sometimes simply backprop noted term backpropagation used neural computing literature mean variety different things instance multilayer perceptron architecture sometimes called backpropagation network term backpropagation also used describe training multilayer perceptroning gradient descent applied sum squares error function order clarify terminology useful consider nature training process care fully training algorithms involve iterative procedure minimization error function adjustments weights made sequence steps step distinguish two distinct stages ﬁrst stage derivatives error function respect weights must evaluated shall see important contribution backpropagation technique pro viding computationally efﬁcient method evaluating derivatives stage errors propagated backwards network shall use term backpropagation speciﬁcally describe evaluation derivatives second stage derivatives used compute adjustments made weights simplest technique one originally considered rumelhart involves gradient descent important recognize two stages distinct thus ﬁrst stage namely propagation rors backwards network order evaluate derivatives applied many kinds network multilayer perceptron also applied error functions simple sum squares eval neural networks uation derivatives jacobian hessian matrices shall see later chapter similarly second stage weight adjustment using calculated derivatives tackled using variety optimization schemes many substantially powerful simple gradient descent 
"
83,"[neural, networks, network, training, evaluation, error-function, derivatives]"," derive backpropagation algorithm general network bitrary feed forward topology arbitrary differentiable nonlinear activation functions broad class error function resulting formulae illustrated using simple layered network structure single layer sigmoidal hidden units together sum squares error many error functions practical interest instance deﬁned maxi mum likelihood set data comprise sum terms one data point training set shall consider problem evaluating one term error function may used directly sequential optimization results accumulated training set case batch methods consider ﬁrst simple linear model outputs linear combinations input variables together error function particular input pattern takes form gradient error function respect weight given interpreted local computation involving product error signal associated output end link variable associated input end link section saw similar formula arises logistic sigmoid activation function together cross entropy error function similarly softmax activation function together matching cross entropy error function shall see simple result extends complex setting multilayer feed forward networks general feed forward network unit computes weighted sum inputs form error backpropagation activation unit input sends connection unit weight associated connection section saw biases included sum introducing extra unit input activation ﬁxed thereforeneed deal biases explicitly sum transformed nonlinear activation function give activation unit form note one variables sum could input similarly unit could output pattern training set shall suppose supplied corresponding input vector network calculated activations hidden output units network successive application process often called forward propagation regarded forward ﬂow information network consider evaluation derivative respect weight outputs various units depend particular input pattern however order keep notation uncluttered shall omit subscript network variables first note depends weight via summed input unit therefore apply chain rule partial derivatives give introduce useful notation often referred errors reasons shall see shortly using write substituting obtain equation tells required derivative obtained simply multiplying value unit output end weight value unit input end weight case bias note takes form simple linear model considered start section thus order evaluate derivatives need calculate value hidden output unit network apply seen already output units neural networks figure illustration calculation hidden unit backpropagation units unit sends connections blue arrow denotes give rise variations error function variations variables substitute deﬁnition corresponding backward propagation information network backpropagation procedure therefore summarized follows apply input vector network forward propagate evaluate output units using backpropagate using obtain hidden unit network use evaluate required derivatives error backpropagation batch methods derivative total error obtained repeating steps pattern training set summing patterns derivation implicitly assumed hidden output unit network activation function derivation easily general ized however allow different units individual activation functions simply keeping track form goes unit 
"
84,"[neural, networks, network, training, simple, example]"," derivation backpropagation procedure allowed general forms error function activation functions network topology order illustrate application algorithm shall consider particular example chosen simplicity practical importance cause many applications neural networks reported literature make use type network speciﬁcally shall consider two layer network form illustrated figure together sum squares error output units linear activation functions hidden units logistic sigmoid activation functions given tanh tanh useful feature function derivative expressed particularly simple form also consider standard sum squares error function pattern error given activation output unit corresponding target particular input pattern pattern training set turn ﬁrst perform forward propagation using tanh neural networks next compute output unit using backpropagate obtain hidden units using finally derivatives respect ﬁrst layer second layer weights given 
"
85,"[neural, networks, network, training, efﬁciency, backpropagation]"," one important aspects backpropagation computational efﬁ ciency understand let examine number computer operations required evaluate derivatives error function scales total number weights biases network single evaluation error function given input pattern would require operations sufﬁciently large follows fact except network sparse connections number weights typically much greater number units bulk computational effort forward propagation concerned evaluating sums evaluation activation functions representing small overhead term sum requires one multiplication one addition leading overall computational cost alternative approach backpropagation computing derivatives error function use ﬁnite differences done perturbing weight turn approximating derivatives expression derivatives improved making smaller numerical roundoff problems arise accuracy ﬁnite differences method improved signiﬁcantly using symmetrical central differences form case corrections cancel veriﬁed taylor expansion exercise right hand side residual corrections number computational steps however roughly doubled compared main problem numerical differentiation highly desirable scaling lost forward propagation requires steps error backpropagation figure however numerical differentiation plays important role practice 
"
86,"[neural, networks, network, training, jacobian, matrix]"," seen derivatives error function respect weights jacobian matrix provides measure local sensitivity outputs changes input variables also allows known errors neural networks associated inputs propagated trained network order estimate contribution errors outputs relation valid provided small general network mapping rep resented trained neural network nonlinear elements jacobian matrix constants depend particular input vector used thus valid small perturbations inputs jacobian must evaluated new input vector jacobian matrix evaluated using backpropagation procedure similar one derived earlier evaluating derivatives error function respect weights start writing element form made use sum runs units input unit sends connections example units ﬁrst hidden layer layered topology considered earlier write recursive backpropagation formula determine derivatives sum runs units unit sends connections corresponding ﬁrst index made use backpropagation starts output units required derivatives found directly functional form output unit activation function instance individual sigmoidal activation functions output unit whereas softmax outputs summarize procedure evaluating jacobian matrix follows apply input vector corresponding point input space cobian matrix found forward propagate usual way obtain hessian matrix activations hidden output units network next row jacobian matrix corresponding output unit backpropagate using recursive relation starting hidden units network finally use backpropagation inputs jacobian also evaluated using alternative forward propagation formalism derived analogous way backpropagation approach given exercise implementation algorithms checked using numerical differentiation form involves forward propagations network inputs 
"
87,"[neural, networks, hessian, matrix]"," shown technique backpropagation used obtain ﬁrst derivatives error function respect weights network back propagation also used evaluate second derivatives error given note sometimes convenient consider weight bias parameters elements single vector denoted case second derivatives form elements hessian matrix total number weights biases hessian plays important role many aspects neural computing including following several nonlinear optimization algorithms used training neural networks based considerations second order properties error surface controlled hessian matrix bishop nabney hessian forms basis fast procedure training feed forward network following small change training data bishop inverse hessian used identify least signiﬁcant weights network part network pruning algorithms hessian plays central role laplace approximation bayesian neural network see section inverse used determine predic tive distribution trained network eigenvalues determine values hyperparameters determinant used evaluate model evidence various approximation schemes used evaluate hessian matrix neural network however hessian also calculated exactly using extension backpropagation technique neural networks important consideration many applications hessian efﬁciency evaluated parameters weights biases network hessian matrix dimensions computational effort needed evaluate hessian scale like pattern data set shall see efﬁcient methods evaluating hessian whose scaling indeed 
"
88,"[neural, networks, hessian, matrix, diagonal, approximation]"," applications hessian matrix discussed require inverse hessian rather hessian reason interest using diagonal approximation hessian words one simply replaces diagonal elements zeros inverse trivial evaluate shall consider error function consists sum terms one pattern data set hessian obtained considering one pattern time summing results patterns diagonal elements hessian pattern written using second derivatives right hand side found recursively using chain rule differential calculus give backprop agation equation form neglect diagonal elements second derivative terms obtain becker cun note number computational steps required evaluate approximation total number weight bias parameters network compared full hessian ricotti also used diagonal approximation hessian retained terms evaluation obtained exact expressions diagonal terms note longer scaling major problem diagonal approximations however practice hessian typically found strongly nondiagonal approximations driven mainly computational convenience must treated care hessian matrix outer product approximation neural networks applied regression problems common use sum squares error function form considered case single output order keep notation simple extension several outputs straightforward write exercise hessian matrix form network trained data set outputs happen close target values second term small neglected generally however may appropriate neglect term following argument recall section optimal function minimizes sum squares loss conditional average target data quantity random variable zero mean assume value uncorrelated value second derivative term right hand side whole term average zero summation exercise neglecting second term arrive levenberg marquardt approximation outer product approximation hessian matrix built sum outer products vectors given activation function output units simply identity evaluation outer product approximation hessian straightforward involves ﬁrst derivatives error function evaluated efﬁciently steps using standard backpropagation elements matrix found steps simple multiplication important emphasize approximation likely valid network trained appropriately general network mapping second derivative terms right hand side typically negligible case cross entropy error function network logistic sigmoid output unit activation functions corresponding approximation given exercise analogous result obtained multiclass networks softmax output unit activation functions exercise neural networks 
"
89,"[neural, networks, hessian, matrix, inverse, hessian]"," use outer product approximation develop computationally contribution gradient output unit activation order evaluate inverse hessian consider matrix identity obtain chosen small quantity algorithm actually exercise note hessian matrix sometimes calculated indirectly 
"
90,"[neural, networks, hessian, matrix, finite, differences]"," case ﬁrst derivatives error function ﬁnd second hessian matrix using symmetrical central differences formulation ensure residual errors rather elements hessian matrix evaluation element requires four forward propagations needing operations per pattern see approach require operations evaluate complete hessian therefore poor scaling properties although practice useful check soft ware implementation backpropagation methods efﬁcient version numerical differentiation found applying central differences ﬁrst derivatives error function calculated using backpropagation gives weights perturbed gradients evaluated steps see method gives hessian operations 
"
91,"[neural, networks, hessian, matrix, exact, evaluation, hessian]"," far considered various approximation schemes evaluating hessian matrix inverse hessian also evaluated exactly net work arbitrary feed forward topology using extension technique back propagation used evaluate ﬁrst derivatives shares many desirable features including computational efﬁciency bishop bishop applied differentiable error function expressed function network outputs networks arbitrary differentiable activation functions number computational steps needed evaluate hessian scales like similar algorithms also considered buntine weigend consider speciﬁc case network two layers weights required equations easily derived shall use indices exercise denote inputs indices denoted hidden units indices denote outputs ﬁrst deﬁne contribution error data point hessian matrix network considered three separate blocks follows weights second layer neural networks weights ﬁrst layer one weight layer element identity matrix one weights exercise 
"
92,"[neural, networks, hessian, matrix, fast, multiplication, hessian]"," many applications hessian quantity interest hessian operations also requires storage vector wish calculate however elements directly way requires operations ﬁrst note denotes gradient operator weight space write apply equations give set forward propagation backpropagation equations evaluation møller pearlmutter pearlmutter used notation denote operator shall follow convention technique best illustrated simple example choose hessian matrix usual summing contributions patterns separately two layer network forward propagation equations given act equations using operator obtain set forward propagation equations form element vector corresponds weight quantities form regarded new variables whose values found using equations considering sum squares error function fol lowing standard backpropagation expressions act equations operator obtain set backprop agation equations form finally usual equations ﬁrst derivatives error neural networks acting operator obtain expressions elements vector implementation algorithm involves introduction additional variables hidden units output units input pattern values quantities found using results elements given elegant aspect technique equations evaluating mirror closely standard forward backward propagation extension existing software compute product typically straightforward desired technique used evaluate full hessian matrix choosing vector given successively series unit vectors form picks one column hessian leads formalism analytically equivalent backpropagation procedure bishop described section though loss efﬁciency due redundant calculations 
"
93,"[neural, networks, regularization, neural, networks]"," number input outputs units neural network generally determined dimensionality data set whereas number hidden units free parameter adjusted give best predictive performance note controls number parameters weights biases network might expect maximum likelihood setting optimum value gives best generalization performance corresponding optimum balance ﬁtting ﬁtting figure shows example effect different values sinusoidal regression problem generalization error however simple function due presence local minima error function illustrated figure see effect choosing multiple random initializations weight vector range values overall best validation set performance case occurred particular solution practice one approach choosing fact plot graph kind shown figure choose speciﬁc solution smallest validation set error however ways control complexity neural network model order avoid ﬁtting discussion polynomial curve ﬁtting chapter see alternative approach choose relatively large value control complexity addition regularization term error function simplest regularizer quadratic giving regularized error regularization neural networks figure examples two layer networks trained data points drawn sinusoidal data set graphs show result ﬁtting networks hidden units respectively minimizing sum squares error function using scaled conjugate gradient algorithm form one limitations simple weight decay form set output variables activations hidden units ﬁrst hidden layer figure random starts neural networks take form activations output units given suppose perform linear transformation input data form arrange mapping performed network unchanged making corresponding linear transformation weights biases inputs units hidden layer form exercise similarly linear transformation output variables network form achieved making transformation second layer weights biases using train one network using original data one network using data input target variables transformed one linear transformations consistency requires obtain equivalent networks differ linear transformation weights given regularizer consistent property otherwise arbitrarily favours one solution another equivalent one clearly simple weight decay treats weights biases equal footing satisfy property therefore look regularizer invariant linear trans formations require regularizer invariant scaling weights shifts biases regularizer given denotes set weights ﬁrst layer denotes set weights second layer biases excluded summations regularizer regularization neural networks regularizer corresponds prior form exp generally consider priors weights divided number groups exp obtain automatic relevance determination discussed section 
"
94,"[neural, networks, regularization, neural, networks, early, stopping]"," alternative regularization way controlling effective complexity behaviour network case sometimes explained qualitatively neural networks figure illustration effect hyperparameters governing prior distribution weights biases two layer network single input single linear output hidden units tanh activation functions priors governed four hyperparameters represent governs vertical scale functions note different vertical axis ranges top two diagrams governs horizontal scale variations function values governs horizontal range variations occur parameter whose effect illustrated governs range vertical offsets functions case quadratic error function verify insight show axis point corresponding roughlytion follows shape error surface widely differing eigenvalues hessian stopping point near learning rate parameter plays role reciprocal regularization regularization neural networks figure illustration behaviour training set error left validation set error right typical training session function iteration step sinusoidal data set goal achieving best generalization performance suggests training stopped point shown vertical dashed lines corresponding minimum validation set error parameter effective number parameters network therefore grows course training invariances many applications pattern recognition known predictions unchanged invariant one transformations input variables example classiﬁcation objects two dimensional images handwritten digits particular object assigned classiﬁcation irrespective position within image translation invariance size scale invariance transformations produce signiﬁcant changes raw data expressed terms intensities pixels image yet give rise output classiﬁcation system similarly speech recognition small levels nonlinear warping along time axis preserve temporal ordering change interpretation signal sufﬁciently large numbers training patterns available adaptive model neural network learn invariance least approximately involves including within training set sufﬁciently large number examples effects various transformations thus translation invariance age training set include examples objects many different positions approach may impractical however number training examples limited several invariants number combinations transformations grows exponentially number transformations therefore seek alternative approaches encouraging adaptive model exhibit required invariances broadly divided four categories training set augmented using replicas training patterns trans formed according desired invariances instance digit recog nition example could make multiple copies example neural networks figure found qual digit shifted different position image regularization term added error function penalizes changes invariance built pre processing extracting features invari ﬁnal option build invariance properties structure neu approach leaves data set unchanged modiﬁes error function regularization neural networks figure illustration synthetic warping handwritten digit original image shown one advantage approach correctly extrapolate well beyond 
"
95,"[neural, networks, regularization, neural, networks, tangent, propagation]"," use regularization encourage models invariant transformations provided within swept figure one applied causes sweep one dimensional manifold neural networks one dimensional parameterized let vector results acting transformation denoted deﬁned tangent curve given directional derivative tangent vector point given transformation input vector network output vector general change derivative output respect given element jacobian matrix discussed section result used modify standard error function encour age local invariance neighbourhood data points addition original error function regularization function give total error function form regularization coefﬁcient nki regularization function zero network mapping function variant transformation neighbourhood pattern vector value parameter determines balance ﬁtting training data learning invariance property practical implementation tangent vector approximateding ﬁnite differences subtracting original vector corresponding vector transformation using small value dividing illustrated figure regularization function depends network weights jaco bian backpropagation formalism computing derivatives regularizer respect network weights easily obtained extension exercise techniques introduced section transformation governed parameters case translations combined plane rotations two dimensional image manifold dimensionality corresponding regularizer given sum terms form one transformation several transformations considered time network mapping made invariant separately locally invariant combinations transformations simard regularization neural networks figure illustration showing original image hand written digit tangent vector xcτ degrees related technique called tangent distance used build invariance 
"
96,"[neural, networks, regularization, neural, networks, training, transformed, data]"," seen one way encourage invariance model set trans section shall consider transformation governed single neural networks parameter drawn distribution error function deﬁned expanded data set written assume distribution zero mean small variance considering small transformations original input vectors expand transformation function taylor series powers give denotes second derivative respect evaluated allows expand model function give substituting mean error function expanding distribution transformations zero mean also shall denote omitting terms average error function becomes original sum squares error regularization term takes form performed integration regularization neural networks simplify regularization term follows section target values see regularized equivalent tangent propagation regularizer consider special case transformation inputs simply consists addition random noise regularizer takes form exercise 
"
97,"[neural, networks, regularization, neural, networks, convolutional, networks]"," another approach creating models invariant certain transformation consider speciﬁc task recognizing handwritten digits input image however approach ignores key property images nearby neural networks input image convolutional layer sub sampling layer figure diagram illustrating part convolutional neural network showing layer convolutional units followed layer subsampling units several successive pairs layers may used ultimately yield information image whole also local features useful one region image likely useful regions image instance object interest translated notions incorporated convolutional neural networks three mechanisms local receptive ﬁelds weight sharing iii subsampling structure convolutional network illustrated figure convolutional layer units organized planes called feature map units feature map take inputs small subregion image units feature map constrained share weight values instance feature map might consist units arranged grid unit taking inputs pixel patch image whole feature map therefore adjustable weight parameters plus one adjustable bias parameter input values patch linearly combined using weights bias result transformed sigmoidal nonlinearity using think units feature detectors units feature map detect pattern different locations input image due weight sharing evaluation activations units equivalent convolution image pixel intensities kernel comprising weight parameters input image shifted activations feature map shifted amount otherwise unchanged provides basis approximate invariance regularization neural networks network outputs translations distortions input image typically need detect multiple features order build effective model generally multiple feature maps convolutional layer set weight bias parameters outputs convolutional units form inputs subsampling layer network feature map convolutional layer plane units subsampling layer unit takes inputs small receptive ﬁeld corresponding feature map convolutional layer units perform subsampling instance subsampling unit might take inputs unit region corresponding feature map would compute average inputs multiplied adaptive weight addition adaptive bias parameter transformed using sigmoidal nonlinear activation function receptive ﬁelds chosen contiguous nonoverlapping half number rows columns subsampling layer compared convolutional layer way response unit subsampling layer relatively insensitive small shifts image corresponding regions input space practical architecture may several pairs convolutional sub sampling layers stage larger degree invariance input trans formations compared previous layer may several feature maps given convolutional layer plane units previous subsampling layer gradual reduction spatial resolution compensated increasing number features ﬁnal layer network would typically fully connected fully adaptive layer softmax output nonlinearity case multiclass classiﬁcation whole network trained error minimization using backpropagation evaluate gradient error function involves slight modiﬁcation usual backpropagation algorithm ensure shared weight constraints satisﬁed due use local receptive ﬁelds number weights exercise network smaller network fully connected furthermore number independent parameters learned data much smaller still due substantial numbers constraints weights 
"
98,"[neural, networks, regularization, neural, networks, soft, weight, sharing]"," one way reduce effective complexity network large number weights constrain weights within certain groups equal technique weight sharing discussed section way building translation invariance networks used image interpretation appli cable however particular problems form constraints speciﬁed advance consider form soft weight sharing nowlan hinton hard constraint equal weights replaced form regularization groups weights encouraged similar values furthermore division weights groups mean weight value group spread values within groups determined part learning process neural networks recall simple weight decay regularizer given viewed negative log gaussian prior distribution weights encour age weight values form several groups rather one group considering instead probability distribution mixture gaussians centres section variances gaussian components well mixing coefﬁcients considered adjustable parameters determined part learning process thus probability density form mixing coefﬁcients taking negative logarithm leads regularization function form total error function given regularization coefﬁcient error minimized respect weights respect parameters mixture model weights constant parameters mixture model could determined using algorithm discussed chapter however distributionweights evolving learning process avoid merical instability joint optimization performed simultaneously weights mixture model parameters done using standard optimization algorithm conjugate gradients quasi newton methods order minimize total error function necessary able evaluate derivatives respect various adjustable parameters convenient regard prior probabilities introduce corresponding posterior probabilities following given bayes theorem form derivatives total error function respect weights given exercise regularization neural networks effect regularization term therefore pull weight towards centre gaussian force proportional posterior probability gaussian given weight precisely kind effect seeking derivatives error respect centres gaussians also easily computed give exercise simple intuitive interpretation pushes towards aver age weight values weighted posterior probabilities respective weight parameters generated component similarly derivatives respect variances given exercise drives towards weighted average squared deviations weights around corresponding centre weighting coefﬁcients given posterior probability weight generated component note practical implementation new variables deﬁned exp introduced minimization performed respect sures parameters remain positive also effect discouraging pathological solutions one goes zero corresponding gaussian component collapsing onto one weight parameter values solutions discussed detail context gaussian mixture models section derivatives respect mixing coefﬁcients need take account constraints follow interpretation prior probabilities done expressing mixing coefﬁcients terms set auxiliary variables using softmax function given exponent derivatives regularized error function respect take form exercise neural networks figure left ﬁgure shows two link robot arm cartesian coordinates end fector determined uniquely two joint angles ﬁxed lengths arms know forward kinematics arm prac inverse kinematics two solutions corresponding elbow elbow corresponding target values mixture density networks figure left data corresponding net computing function sin adding uniform noise interval inverse problem obtained keeping therefore seek general framework modelling conditional probability distributions achieved using mixture model provided consider sufﬁciently ﬂexible shall develop model explicitly gaussian components assume factorization respect take various parameters mixture model namely mixing coefﬁcients means variances governed neural networks figure mixture density network represent general conditional probability densities considering parametric mixture model distribution determined outputs neural network takes mixture density networks directly network output activations adaptive parameters mixture density network comprise vector weights biases neural network set maximum likelihood equivalently minimizing error function deﬁned negative logarithm likelihood independent data error function takes form made dependencies explicit order minimize error function need calculate derivatives error respect components evaluated using standard backpropagation procedure provided obtain suitable expressions derivatives error respect output unit activations represent error signals pattern output unit back propagated hidden units error function derivatives evaluated usual way error function composed sum terms one training data point consider derivatives particular pattern ﬁnd derivatives summing patterns dealing mixture distributions convenient view mixing coefﬁcients dependent prior probabilities introduce corresponding posterior probabilities given denotes derivatives respect network output activations governing mixing coefﬁcients given exercise similarly derivatives respect output activations controlling component means given exercise finally derivatives respect output activations controlling component variances given exercise neural networks figure plot mixing coefﬁcients function tanh sigconditional conditional den using plot illustrate use mixture density network returning toy means conditional density contours corresponding shown figure outputs neural network hence mixture density network trained predict conditional bayesian neural networks similarly evaluate variance density function conditional average give exercise seen multimodal distributions conditional mean give 
"
99,"[neural, networks]"," section developed bayesian solution simple linear regression technique variational inference discussed chapter applied bayesian neural networks using factorized gaussian approximation neural networks posterior distribution hinton van camp also using full covariance gaussian barber bishop barber bishop complete treatment however based laplace approximation mackay mackay forms basis discussion given approximate posterior distribution gaussian centred mode true posterior furthermore shall assume covariance gaussian small network function approximately linear respect parameters region parameter space posterior probability signiﬁcantly nonzero two approximations obtain models analogous linear regression classiﬁcation models discussed earlier chapters exploit results obtained make use evidence framework provide point estimates hyperparameters compare alternative models example networks different numbers hidden units start shall discuss regression case later consider modiﬁcations needed solving classiﬁcation tasks 
"
100,"[neural, networks, posterior, parameter, distribution]"," consider problem predicting single continuous target variable vector inputs extension multiple targets straightforward shall suppose conditional distribution gaussian dependent mean given output neural network model precision inverse variance similarly shall choose prior distribution weights gaussian form data set observations corresponding set target values likelihood function given resulting posterior distribution consequence nonlinear dependence non gaussian ﬁnd gaussian approximation posterior distribution using laplace approximation must ﬁrst ﬁnd local maximum posterior must done using iterative numerical optimization usual convenient maximize logarithm posterior written bayesian neural networks form const map standard nonlinear optimization algorithms conjugate gradients using error backpropagation evaluate required derivatives found mode map build local gaussian approximation map similarly predictive distribution obtained marginalizing respect posterior distribution map retain linear terms map deﬁned map whose mean linear function form map therefore make use general result marginal give exercise map neural networks input dependent variance given see predictive distribution gaussian whose mean given network function map parameter set map value variance two terms ﬁrst arises intrinsic noise target variable whereas second dependent term expresses uncertainty interpolant due uncertainty model parameters compared corresponding predictive distribution linear regression model given 
"
101,"[neural, networks, hyperparameter, optimization]"," far assumed hyperparameters ﬁxed known make use evidence framework discussed section together gaussian approximation posterior obtained using laplace approximation obtain practical procedure choosing values hyperparameters marginal likelihood evidence hyperparameters obtained integrating network weights easily evaluated making use laplace approximation result exercise taking logarithms gives map total number parameters regularized error function deﬁned map see takes form corresponding result linear regression model evidence framework make point estimates maximizing consider ﬁrst maximization respect done analogy linear regression case discussed section ﬁrst deﬁne eigenvalue equation hessian matrix comprising second derivatives sum squares error function evaluated map analogy obtain map bayesian neural networks represents effective number parameters deﬁned section note result exact linear regression case nonlinear neural network however ignores fact changes cause changes hessian turn change eigenvalues therefore implicitly ignored terms involving derivatives respect similarly see maximizing evidence respect gives estimation formula map linear model need alternate estimation hyper parameters updating posterior distribution situation neural network model complex however due multimodality posterior distribution consequence solution map found maximizing log posterior depend initialization solutions differ consequence interchange sign reversal symmetries hidden units section identical far predictions concerned irrelevant equivalent solutions found however may inequivalent solutions well generally yield different values optimized hyperparameters order compare different models example neural networks different numbers hidden units need evaluate model evidence approximated taking substituting values obtained iterative optimization hyperparameters careful evaluation obtained marginalizing making gaussian approximation mackay bishop either case necessary evaluate determinant hessian matrix problematic practice determinant unlike trace sensitive small eigenvalues often difﬁcult determine accurately laplace approximation based local quadratic expansion around mode posterior distribution weights seen section given mode two layer network member set equivalent modes differ interchange sign change symmetries number hidden units comparing networks different numbers hidden units taken account multiplying evidence factor 
"
102,"[neural, networks, bayesian, neural, networks, classiﬁcation]"," far used laplace approximation develop bayesian treat ment neural network regression models discuss modiﬁcations neural networks framework arise applied classiﬁcation shall consider network single logistic sigmoid output corresponding two class classiﬁcation problem extension networks multiclass softmax outputs straightforward shall build extensively analogous results linear exercise classiﬁcation models discussed section encourage reader familiarize material studying section log likelihood function model given target values note hyperparameter data points assumed correctly labelled prior taken isotropic gaussian form ﬁrst stage applying laplace framework model initialize hyperparameter determine parameter vector maximizing log posterior distribution equivalent minimizing regularized error function achieved using error backpropagation combined standard optimization algorithms discussed section found solution map weight vector next step evaluate hessian matrix comprising second derivatives negative log likelihood function done instance using exact method section using outer product approximation given second derivatives negative log posterior written form gaussian approximation posterior given optimize hyperparameter maximize marginal likelihood easily shown take form exercise map const regularized error function deﬁned map maximizing evidence function respect leads estimation equation given use evidence procedure determine illustrated figure synthetic two dimensional data discussed appendix finally need predictive distribution deﬁned integration intractable due nonlinearity network function bayesian neural networks figure map vector map found backpropagation gaussian approximation posterior distribution map gaussian approximation posterior distribution given map variance finally obtain predictive distribution must marginalize using neural networks figure illustration laplace approximation bayesian neural network hidden units tanh activation functions single logistic sigmoid output unit weight parameters found using scaled conjugate gradients hyperparameter optimized using evidence framework left result using simple approximation based point estimate map parameters green curve shows decision boundary contours correspond output probabilities right corresponding result obtained using note posterior probabilities shifted towards contour unaffected deﬁned recall functions figure shows example framework applied synthetic classi ﬁcation data set described appendix given logistic sigmoid functions form exp show maximizing likelihood function conditional exercises consider regression problem involving multiple target variables assumed distribution targets conditioned input vector gaussian form output neural network input vector weight vector covariance assumed gaussian noise targets given set independent observations write error function must minimized order ﬁnd maximum likelihood solution assume ﬁxed known assume also determined data write expression maximum likelihood solution note optimizations coupled contrast case independent target variables discussed section consider binary classiﬁcation problem target values network output represents suppose probability class label training data point incorrectly set assuming independent identically distributed data write error function corresponding negative log likelihood verify error function obtained note error function makes model robust incorrectly labelled data contrast usual error function show maximizing likelihood multiclass neural network model network outputs interpretation equivalent minimization cross entropy error function show derivative error function respect activation output unit logistic sigmoid activation function satisﬁes show derivative error function respect activation output units softmax activation function satisﬁes saw derivative logistic sigmoid activation function expressed terms function value derive corresponding result tanh activation function deﬁned error function binary classiﬁcation problems rived network logistic sigmoid output activation function data target values derive corresponding error function consider network output target values class class would appropriate choice output unit activation function consider hessian matrix eigenvector equation setting vector equal eigenvectors turn show positive deﬁnite eigenvalues positive neural networks consider quadratic error function deﬁned hessian matrix eigenvalue equation given show con tours constant error ellipses whose axes aligned eigenvectors lengths inversely proportional square root corresponding eigenvalues considering local taylor expansion error function stationary point show necessary sufﬁcient condition stationary point local minimum error function hessian matrix deﬁned positive deﬁnite show consequence symmetry hessian matrix number independent elements quadratic error function given making taylor expansion verify terms cancel right hand side section derived procedure evaluating jacobian matrix neural network using backpropagation procedure derive alternative formalism ﬁnding jacobian based forward propagation equations outer product approximation hessian matrix neural network using sum squares error function given extend result case multiple outputs consider squared loss function form parametric function neural network result shows function minimizes error given conditional expectation given use result show second derivative respect two elements vector given note ﬁnite sample obtain consider two layer network form shown figure addition extra parameters corresponding skip layer connections directly inputs outputs extending discussion section write equations derivatives error function respect additional parameters derive expression outer product approximation hessian matrix network single output logistic sigmoid output unit activation function cross entropy error function corresponding result sum squares error function exercises derive expression outer product approximation hessian matrix network outputs softmax output unit activation function cross entropy error function corresponding result sum squares error function extend expression outer product approximation hessian matrix case output units hence derive recursive expression analogous incrementing number patterns similar expression incrementing number outputs use results together identity ﬁnd sequential update expressions analogous ﬁnding inverse hessian incrementally including extra patterns extra outputs derive results elements hessian matrix two layer feed forward network application chain rule calculus extend results section exact hessian two layer network include skip layer connections directly inputs outputs verify network function deﬁned invariant der transformation applied inputs provided weights biases simultaneously transformed using similarly show network outputs transformed according applying transformation second layer weights biases consider quadratic error function form represents minimum hessian matrix positive deﬁnite constant suppose initial weight vector chosen origin updated using simple gradient descent denotes step number learning rate assumed small show steps components weight vector parallel eigenvectors written eigenvectors eigenvalues respectively show gives expected provided suppose training halted ﬁnite number steps show neural networks components weight vector parallel eigenvectors hessian satisfy compare result discussion section regularization simple weight decay hence show analogous regularization parameter results also show effective number parameters network deﬁned grows training progresses consider multilayer perceptron arbitrary feed forward topology trained minimizing tangent propagation error function regularizing function given show regularization term written sum patterns terms form differential operator deﬁned acting forward propagation equations operator show evaluated forward propagation using following equations deﬁned new variables show derivatives respect weight network written form deﬁned write backpropagation equations hence derive set back propagation equations evaluation exercises consider framework training transformed data gaussian distribution zero mean unit consider neural network convolutional network discussed verify result verify result verify result show derivatives mixing coefﬁcients deﬁned respect auxiliary parameters given hence making use constraint derive result write pair equations express cartesian coordinates robot arm shown figure terms joint angles lengths links assume origin coordinate system derive result derivative error function derive result derivative error function respect using general result derive predictive distribution laplace approximation bayesian neural network model neural networks make use laplace approximation result show evidence function hyperparameters bayesian neural network model approximated outline modiﬁcations needed framework bayesian neural networks discussed section handle multiclass problems using networks softmax output unit activation functions following analogous steps given section regression networks derive result marginal likelihood case net work cross entropy error function logistic sigmoid output unit activation function 
"
103,"[kernel, methods]"," chapters considered linear parametric models regression classiﬁcation form mapping input output governed vector adaptive parameters learning phase set training data used either obtain point estimate parameter vector determine posterior distribution vector training data discarded predictions new inputs based purely learned parameter vector approach also used nonlinear parametric models neural networks chapter however class pattern recognition techniques training data points subset kept used also prediction phase instance parzen probability density model comprised linear combination section kernel functions one centred one training data points similarly section introduced simple technique classiﬁcation called nearest neighbours involved assigning new test vector label kernel methods closest example training set examples memory based methods involve storing entire training set order make predictions future data points typically require metric deﬁned measures similarity two vectors input space generally fast train slow making predictions test data points many linear parametric models cast equivalent dual represen tation predictions also based linear combinations kernel function evaluated training data points shall see models based ﬁxed nonlinear feature space mapping kernel function given relation deﬁnition see kernel symmetric function arguments kernel concept introduced ﬁeld pattern recognition aizerman context method potential functions called analogy electrostatics although neglected many years introduced machine learning context large margin classiﬁers boser giving rise technique support vector machines since considerable interest topic chapter terms theory applications one signiﬁcant developments extension kernels handle symbolic objects thereby greatly expanding range problems addressed simplest example kernel function obtained considering identity mapping feature space case shall refer linear kernel concept kernel formulated inner product feature space allows build interesting extensions many well known algorithms making use kernel trick also known kernel substitution general idea algorithm formulated way input vector enters form scalar products replace scalar product choice kernel instance technique kernel substitution applied principal component analysis order develop nonlinear variant pca section examples kernel substitution include nearest neighbour classiﬁers kernel fisher discriminant mika roth steinhage baudat anouar numerous forms kernel functions common use shall counter several examples chapter many property function difference arguments known stationary kernels invariant translations input space specialization involves homogeneous kernels also known dial basis functions depend magnitude distance typically section euclidean arguments recent textbooks kernel methods see smola brich shawe taylor cristianini dual representations 
"
104,"[kernel, methods, dual, representations]"," set gradient respect equal zero see solution takes form linear combination vectors coefﬁcients functions form design matrix whose row given vector deﬁned obtain deﬁne gram matrix symmetric matrix elements introduced kernel function deﬁned terms gram matrix sum squares error function written kka kernel methods deﬁned vector elements thus known dual recover original formulation terms parameter vector note prediction given linear combination exercise dual formulation determine parameter vector inverting matrix whereas original parameter space formulation invert matrix order determine typically much larger therefore work directly terms kernels avoid explicit introduction feature vector allows implicitly use feature spaces high even inﬁnite dimensionality existence dual representation based gram matrix property many linear models including perceptron section develop dual exercise 
"
105,"[kernel, methods, constructing, kernels]"," use basis functions alternative approach construct kernel functions directly case constructing kernels figure illustration construction kernel functions starting corresponding set basis functions column lower plot shows kernel function deﬁned plotted function upper plot shows corresponding basis functions given polynomials left column gaussians centre column logistic sigmoids right column take particular case two dimensional input space see feature mapping takes form generally however need simple way test whether function con stitutes valid kernel without construct function necessary sufﬁcient condition function valid kernel shawe positive semideﬁnite possible choices set one powerful technique constructing new kernels build simpler kernels building blocks done using following properties kernel methods techniques constructing new kernels given valid kernels following new kernels also valid exp constant function polynomial nonneg ative coefﬁcients function valid kernel symmetric positive semideﬁnite matrix variables necessarily disjoint valid kernel functions respective spaces equipped properties embark construction complex kernels appropriate speciﬁc applications require kernel symmetric positive semideﬁnite expresses appropriate form similarity according intended application consider common examples kernel functions extensive discussion kernel engineering see shawe taylor cristianini saw simple polynomial kernel contains terms degree two consider slightly generalized kernel corresponding feature mapping contains con stant linear terms well terms order two similarly contains monomials order instance two images kernel represents particular weighted sum possible products pixels ﬁrst image pixels second image similarly gener alized include terms degree considering using results combining kernels see valid kernel functions another commonly used kernel takes form exp often called gaussian kernel note however context interpreted probability density hence normalization coefﬁcient constructing kernels omitted see valid kernel expanding square give exponent making use together validity linear kernel note feature vector corresponds gaussian kernel inﬁnite dimensionality exercise gaussian kernel restricted use euclidean distance use kernel substitution replace nonlinear kernel obtain exp important contribution arise kernel viewpoint extension inputs symbolic rather simply vectors real numbers kernel functions deﬁned objects diverse graphs sets strings text doc uments consider instance ﬁxed set deﬁne nonvectorial space consisting possible subsets set two subsets one simple choice kernel would denotes intersection sets denotes number subsets valid kernel function shown correspond inner product feature space exercise one powerful approach construction kernels starts probabilistic generative model haussler allows apply generative models discriminative setting generative models deal naturally missing data case hidden markov models handle sequences varying length contrast discriminative models generally give better performance discriminative tasks generative models therefore interest combine two approaches lasserre one way combine use generative model deﬁne kernel use kernel discriminative approach given generative model deﬁne kernel clearly valid kernel function interpret inner product one dimensional feature space deﬁned mapping says two inputs similar high probabilities use extend class kernels considering sums products different probability distributions positive weighting coefﬁcients form kernel methods give large value kernel function section continuous latent variable suppose data consists ordered sequences length observation given popular generative model sequences hidden markov model expresses distribution section marginalization corresponding sequence hidden states extending mixture representation give alternative technique using generative models deﬁne kernel functions denotes vector parameters goal ﬁnd kernel measures similarity two input vectors induced deﬁnes vector feature space dimensionality particular consider fisher score fisher kernel deﬁned fisher information matrix given expectation respect distribution exercise practice often infeasible evaluate fisher information matrix one radial basis function networks section information matrix altogether use noninvariant kernel application fisher kernels document retrieval given hofmann ﬁnal example kernel function sigmoidal kernel given tanh section 
"
106,"[kernel, methods]"," historically radial basis functions introduced purpose exact function interpolation powell given set input vectors along corresponding target values goal ﬁnd smooth function ﬁts every target value exactly values coefﬁcients found least squares expansions radial basis functions also arise regularization theory pog kernel methods point differential operator isotropic green functions depend radial distance corresponding data point due presence regularizer solution longer interpolates training data exactly another motivation radial basis functions comes consideration interpolation problem input rather target variables noisy webb bishop noise input variable described variable distribution sum squares error function becomes using calculus variations optimize respect function appendix give exercise basis functions given see one basis function centred every data point known nadaraya watson model derived different perspective section noise distribution isotropic function basis functions radial note basis functions normalized value effect normalization shown figure normalization sometimes used practice avoids regions input space basis functions take small values would necessarily lead predictions regions either small controlled purely bias parameter another situation expansions normalized radial basis functions arise application kernel density estimation problem regression shall discuss section one basis function associated every data point corre sponding model computationally costly evaluate making predictions new data points models therefore proposed broomhead lowe moody darken poggio girosi retain expansion radial basis functions number basis functions smaller number data points typically number basis functions locations centres determined based input data alone basis functions kept ﬁxed coefﬁcients determined least squares solving usual set linear equations discussed section radial basis function networks figure plot set gaussian basis functions left together corresponding normalized basis functions right one simplest ways choosing basis function centres use randomly section longer coincide training data points 
"
107,"[kernel, methods, nadaraya-watson, model]"," section saw prediction linear regression model motivate kernel regression model different perspective starting kernel density estimation suppose training set use parzen density estimator model joint distribution section kernel methods input variable given assume simplicity component density functions zero mean values using simple change variable obtain kernel function given deﬁned result known nadaraya watson model kernel regression nadaraya watson localized kernel function prop erty giving weight data points close note kernel satisﬁes summation constraint gaussian processes figure illustration nadaraya watson kernel fact model deﬁnes conditional expectation also full conditional distribution given expectations evaluated illustration consider case single input variable corresponding conditional distribution given gaussian mixture shown together conditional mean sinusoidal synthetic data set figure obvious extension model allow ﬂexible forms gaus latter case longer representation terms kernel func kernel methods logistic discriminative models leading framework gaussian processes shall thereby see kernels arise naturally bayesian setting chapter considered linear regression models form vector parameters vector ﬁxed nonlinear basis functions depend input vector showed prior distribution induced corresponding prior distribution functions given training data set evaluated posterior distribution thereby obtained corresponding posterior distribution regression functions turn addition noise implies predictive distribution new input vectors gaussian process viewpoint dispense parametric model instead deﬁne prior probability distribution functions directly ﬁrst sight might seem difﬁcult work distribution uncountably inﬁnite space functions however shall see ﬁnite training set need consider values function discrete set input values corresponding training set test set data points practice work ﬁnite space models equivalent gaussian processes widely studied many dif ferent ﬁelds instance geostatistics literature gaussian process regression known kriging cressie similarly arma autoregressive moving aver age models kalman ﬁlters radial basis function networks viewed forms gaussian process models reviews gaussian processes machine learning perspective found mackay williams mackay comparison gaussian process models alternative approaches given rasmussen see also rasmussen williams recent textbook gaussian processes 
"
108,"[kernel, methods, linear, regression, revisited]"," order motivate gaussian process viewpoint let return linear regression example derive predictive distribution working terms distributions functions provide speciﬁc example gaussian process consider model deﬁned terms linear combination ﬁxed basis functions given elements vector input vector dimensional weight vector consider prior distribution given isotropic gaussian form governed hyperparameter represents precision inverse variance distribution given value deﬁnition deﬁnes partic ular function probability distribution deﬁned therefore induces probability distribution functions practice wish evaluate function speciﬁc values example training data points gaussian processes therefore interested joint distribution function values denote vector elements vector given design matrix elements ﬁnd probability distribution follows first note linear combination gaussian distributed variables given elements hence gaussian thereforeneed ﬁnd mean covariance given exercise covariance gram matrix elements kernel function model provides particular example gaussian process general gaussian process deﬁned probability distribution functions set values evaluated arbitrary set points jointly gaussian distribution cases input vector two mensional may also known gaussian random ﬁeld generally stochastic process speciﬁed giving joint probability distribution ﬁnite set values consistent manner key point gaussian stochastic processes joint distribution variables speciﬁed completely second order statistics namely mean covariance applications prior knowledge mean symmetry take zero equivalent choosing mean prior weight values zero basis function viewpoint speciﬁcation gaussian process completed giving covariance evaluated two values given kernel function speciﬁc case gaussian process deﬁned linear regression model weight prior kernel function given also deﬁne kernel function directly rather indirectly choice basis function figure shows samples functions drawn gaussian processes two different choices kernel function ﬁrst gaussian kernel form second exponential kernel given exp corresponds ornstein uhlenbeck process originally introduced lenbeck ornstein describe brownian motion kernel methods figure samples gaus 
"
109,"[kernel, methods, gaussian, processes, regression]"," order apply gaussian process models problem regression need take account noise observed target values given random noise variable whose value chosen inde conditioned values given isotropic gaussian form denotes unit matrix deﬁnition gaussian process similar corresponding values strongly correlated dissimilar points notion similarity depend application order ﬁnd marginal distribution conditioned input values need integrate done making use gaussian processes covariance matrix elements result reﬂects fact two gaussian sources randomness namely associated associated independent covariances simply add one widely used kernel function gaussian process regression given exponential quadratic form addition constant linear terms give exp note term involving corresponds parametric model linear function input variables samples prior plotted various values parameters figure figure shows set points sam pled joint distribution along corresponding values deﬁned far used gaussian process viewpoint build model joint distribution sets data points goal regression however make predictions target variables new inputs given set training data let suppose corresponding input values comprise observed training set goal predict target variable new input vector requires evaluate predictive distri bution note distribution conditioned also variables however keep notation simple show conditioning variables explicitly ﬁnd conditional distribution begin writing joint distribution denotes vector apply results section obtain required conditional distribution illustrated figure joint distribution given covariance matrix elements given joint distribution gaussian apply results section ﬁnd conditional gaussian distribution partition covariance matrix follows covariance matrix elements given vector elements scalar kernel methods figure samples gaussian process prior deﬁned covariance function title plot denotes using results see con ditional distribution gaussian distribution mean covariance given key results deﬁne gaussian process regression vector see predictive distribution gaussian whose mean variance depend example gaussian process regression shown figure restriction kernel function covariance matrix given must positive deﬁnite eigenvalue corresponding eigenvalue therefore sufﬁcient kernel matrix positive semideﬁnite pair points eigenvalue zero still give rise positive eigenvalue gaussian processes figure gaussian process obtained corresponding values shown green suitable kernels note mean predictive distribution written function form component thus kernel function depends distance basis functions results deﬁne predictive distribution gaussian pro cess regression arbitrary kernel function particular case kernel function deﬁned terms ﬁnite set basis functions models therefore obtain predictive distribution either central computational operation using gaussian processes involve inversion matrix size computations contrast basis function model invert matrix size computational complexity note gaussian process case linear basis func kernel methods figure correspond shown function green curve large training datasets however direct application gaussian process introduced gaussian process regression case single tar figure green curve notice gaussian processes learning hyperparameters predictions gaussian process model depend part choice techniques learning hyperparameters based evaluation likelihood function denotes hyperparameters gaussian pro cess model simplest approach make point estimate maximizing log likelihood function represents set hyperparameters section log likelihood function gaussian process regression model easily evaluated using standard form multivariate gaussian distribution giving shall assume evaluation derivatives straightforward would case covariance functions considered chapter making use result derivative together result derivative obtain general nonconvex function multiple maxima straightforward introduce prior maximize log poste weighted product prior likelihood function general however exact marginalization intractable must resort approximations gaussian process regression model gives predictive distribution whose kernel methods figure samples ard right plot cor responds 
"
110,"[kernel, methods, automatic, relevance, determination]"," previous section saw maximum likelihood could used consider gaussian process two dimensional input space kernel function form exp figure see particu lar parameter becomes small function becomes relatively insensitive corresponding input variable adapting parameters data set using gaussian evaluating function sin adding gaussian processes figure illustration automatic relevant curves red green blue func details given text note logarithmic scale vertical axis gaussian noise values given copying corresponding values adding noise values sampled independent gaussian distributionthus good predictor noisy predictor optimized using scaled conjugate gradients algorithm see figure converges relatively large value converges much smaller value becomes small indicating irrelevant predicting ard framework easily incorporated exponential quadratic kernel exp dimensionality input space probabilistic approach classiﬁcation goal model posterior consider ﬁrst two class problem target variable illustrated case one dimensional input space figure probability distri kernel methods figure left plot shows sample gaussian process prior functions right plot shows result transforming sample using logistic sigmoid function bution target variable given bernoulli distribution usual denote training set inputs corresponding observed target variables also consider single test point target value goal determine predictive distribution left conditioning input variables implicit introduce gaussian process prior vector components turn deﬁnes non gaussian process conditioning training data obtain required predictive distri bution gaussian process prior takes form elements given positive semideﬁnite kernel function kind considered governed vector later discuss two class problems sufﬁcient predict value given required gaussian processes predictive distribution given integral analytically intractable may approximated using sam usual justiﬁcation section three different approaches obtaining gaussian approximation considered one technique based variational inference gibbs mackay section performed analytically approach also yields lower bound likelihood function second approach uses expectation propagation opper winther section 
"
111,"[kernel, methods, laplace, approximation]"," third approach gaussian process classiﬁcation based laplace approximation consider detail order evaluate predictive section using bayes theorem given kernel methods used conditional distribution obtained invoking results gaussian pro cess regression give using standard result convolution two gaussian distributions prior given zero mean gaussian process covariance matrix data term assuming independence data points given additive normalization constant given quantity const given vector elements cannot simply ﬁnd mode setting gradient zero depends nonlinearly section derivatives also require laplace approximation anyway given diagonal matrix elements used positive deﬁnite matrix hence inverse positive deﬁnite construction sum two positive deﬁnite matrices also positive deﬁnite see exercise hessian matrix positive deﬁnite posterior distribution log convex therefore single mode global gaussian processes maximum posterior distribution gaussian however hessian function using newton raphson formula iterative update equation given exercise new equations iterated converge mode denote mode gradient vanish hence satisfy found mode posterior evaluate hessian matrix given elements evaluated using deﬁnes gaussian proximation posterior distribution given combine hence evaluate integral corresponds linear gaussian model use general result give exercise var gaussian distribution approximate integral using result bayesian logistic regression model section interested decision boundary corresponding need consider mean ignore effect variance also need determine parameters covariance function one approach maximize likelihood function given need expressions log likelihood gradient desired suitable regularization terms also added leading penalized maximum likelihood solution likelihood function deﬁned integral analytically intractable make use laplace approx imation using result obtain following approximation log likelihood function kernel methods also need evaluate gradient respect parameter vector note changes cause changes leading additional terms gradient thus differentiate respect obtain two sets terms ﬁrst arising dependence covariance matrix rest arising dependence terms arising explicit dependence found using together results given compute terms arising dependence note laplace approximation constructed zero gradient gives contribution gradient result dependence leaves following contribution derivative respect component used result together deﬁnition evaluate derivative respect differ entiating relation respect give rearranging gives combining evaluate gradient log likelihood function used standard nonlinear optimization algo rithms order determine value illustrate application laplace approximation gaussian pro cesses using synthetic two class data set shown figure extension appendix laplace approximation gaussian processes involving classes using softmax activation function straightforward williams barber gaussian processes figure illustration use gaussian process classiﬁcation showing data left together optimal decision boundary true distribution green decision boundary gaussian process classiﬁer black right predicted posterior probability blue red classes together gaussian process decision boundary 
"
112,"[kernel, methods, connection, neural, networks]"," seen range functions represented neural bayesian neural network prior distribution parameter vector noted however limit output variables seen gaussian process determined covariance kernel nonstationary cannot expressed function difference consequence gaussian weight prior centred zero breaks translation invariance weight space kernel methods working directly covariance function implicitly marginal 
"
113,"[kernel, methods, exercises]"," consider dual formulation least squares linear regression problem given section show solution components vector expressed linear combination elements vector denoting coefﬁcients vector show dual dual formulation given original representation terms parameter vector exercise develop dual formulation perceptron learning denote coefﬁcients linear combination derive show feature vector enters form kernel function nearest neighbour classiﬁer section assigns new input vector class nearest input vector training set simplest case distance deﬁned euclidean metric matrix positive eigenvalues yet least one negative element verify results constructing valid kernels verify results constructing valid kernels verify results constructing valid kernels verify results constructing valid kernels verify results constructing valid kernels show excellent choice kernel learning function given showing linear learning machine based kernel always ﬁnd solution proportional exercises consider space possible subsets given ﬁxed set deﬁned mapping subset element indexed subset given otherwise denotes either subset equal show fisher kernel deﬁned remains invariant make nonlinear transformation parameter vector function invertible differentiable write form fisher kernel deﬁned case distribution gaussian mean ﬁxed covariance considering determinant gram matrix show positive deﬁnite kernel function satisﬁes cauchy schwartz inequality consider parametric model governed parameter vector together data set input values nonlinear feature mapping suppose dependence error function takes form monotonically increasing function writing form consider sum squares error function data noisy inputs distribution noise use calculus vari kernel methods consider nadaraya watson model one input variable one target unit matrix write expressions conditional density conditional mean variance var terms kernel function another viewpoint kernel regression comes consideration generated usual taking function evaluated point adding gaussian noise value directly observed however noise corrupted version random variable governed distribution consider set observations together cor verify results consider gaussian process regression model kernel consider regression problem training set input vectors test set input vectors suppose deﬁne gaussian given values show marginal distribution one test observations given usual gaussian process regression result consider gaussian process regression model target variable dimensionality write conditional distribution test input vector given training set input vectors corresponding target observations show diagonal matrix whose elements satisfy positive deﬁnite show sum two positive deﬁnite matrices positive deﬁnite exercises using newton raphson formula derive iterative update formula ﬁnding mode posterior distribution gaussian process classiﬁcation model using result derive expressions mean variance posterior distribution gaussian process clas siﬁcation model derive result log likelihood function laplace approx imation framework gaussian process classiﬁcation similarly derive results terms gradient log likelihood 
"
114,"[sparse, kernel, machines]"," previous chapter explored variety learning algorithms based non linear kernels one signiﬁcant limitations many algorithms kernel function must evaluated possible pairs training points computationally infeasible training lead excessive computation times making predictions new data points chapter shall look kernel based algorithms sparse solutions predictions new inputs depend kernel function evaluated subset training data points begin looking detail support vector machine svm became popular years ago solving problems classiﬁcation regression novelty detection important property support vector machines determination model parameters corresponds convex optimization prob lem local solution also global optimum discussion support vector machines makes extensive use lagrange multipliers reader sparse kernel machines svm decision machine provide posterior probabilities section bilistic outputs well typically much sparser solutions svm 
"
115,"[sparse, kernel, machines, maximum, margin, classiﬁers]"," denotes ﬁxed feature space transformation made corresponding target values new data points classiﬁed according sign shall assume moment training data set linearly separable feature space deﬁnition exists least one choice parameters function form satisﬁes points points training data points may course exist many solutions separate classes exactly support vector machines decision boundary chosen one section maximum margin classiﬁers margin figure margin deﬁned perpendicular distance decision boundary closest common parameter together class priors deﬁnes opti optimal hyperplane reduced hyperplane increasingly dominated nearby data points shall see figure marginalization respect prior distri recall figure perpendicular distance point hyper plane deﬁned takes form given thus distance point decision surface given margin given perpendicular distance closest point arg max min taken factor outside optimization sparse kernel machines distance point decision surface given unchanged use freedom set equivalent minimizing solve optimization problem arg min compensated changes shall see works shortly order solve constrained optimization problem introduce lagrange multipliers one multiplier constraints giving appendix lagrangian function note minus sign front lagrange multiplier maximum margin classiﬁers respect subject constraints kernel function deﬁned takes solution quadratic programming problem variables general computational complexity going dual formulation positive deﬁnite ensures lagrangian function bounded giving rise well deﬁned optimization problem order classify new data points using trained model evaluate sign deﬁned expressed terms parameters kernel function substituting using give 
"
116,"[sparse, kernel, machines, joseph-louis, lagrange]"," sparse kernel machines appendix show constrained optimization form satisﬁes karush kuhn tucker kkt conditions case require following three properties hold thus every data point either data point appear sum hence plays role making predictions new data points remaining data points called support vectors satisfy correspond points lie maximum margin hyperplanes feature space illustrated figure property central practical applicability support vector machines model trained signiﬁcant proportion data points discarded support vectors retained solved quadratic programming problem found value determine value threshold parameter noting support vector satisﬁes using gives denotes set indices support vectors although solve equation using arbitrarily chosen support vector numerically stable solution obtained ﬁrst multiplying making use averaging equations support vectors solving give total number support vectors later comparison alternative models express maximum margin classiﬁer terms minimization error function simple quadratic regularizer form function zero otherwise ensures constraints satisﬁed note long regularization parameter satisﬁes precise value plays role figure shows example classiﬁcation resulting training support vector machine simple synthetic data set using gaussian kernel maximum margin classiﬁers figure example also provides geometrical insight origin sparsity 
"
117,"[sparse, kernel, machines, joseph-louis, lagrange, overlapping, class, distributions]"," far assumed training data points linearly separable feature space resulting support vector machine give exact separation thereforeneed way modify support vector machine allow data points inside correct margin boundary points thus data point decision boundary points sparse kernel machines figure illustration slack variables replaced slack variables constrained satisfy data points correctly classiﬁed either margin correct side margin points lie inside margin cor rect side decision boundary data points goal maximize margin softly penalizing points lie wrong side margin boundary therefore minimize upper bound number misclassiﬁed points parameter data wish minimize subject constraints together corresponding lagrangian given maximum margin classiﬁers lagrange multipliers corresponding set kkt conditions given appendix optimize making use deﬁnition give using results eliminate lagrangian obtain dual lagrangian form identical separable case except constraints somewhat different see constraints note required lagrange multipliers furthermore together implies therefore minimize respect dual variables subject known box constraints represents quadratic programming problem substitute see predictions new data points made using interpret resulting solution subset data points may case contribute predictive sparse kernel machines model remaining data points constitute support vectors hence must satisfy implies requires hence points lie margin points lie inside margin either correctly classiﬁed misclassiﬁed determine parameter note support vectors hence satisfy numerically stable solution obtained averaging give denotes set indices data points alternative equivalent formulation support vector machine known svm proposed involves maximizing subject constraints approach advantage parameter replaces interpreted upper bound fraction margin errors points hence lie wrong side margin boundary may may misclassiﬁed lower bound fraction support vectors example svm applied synthetic data set shown figure gaussian kernels form exp used although predictions new inputs made using support vectors training phase determination parameters makes use whole data set important efﬁcient algorithms solving maximum margin classiﬁers figure illustration svm applied nonseparable data set two dimensions support vectors indicated circles quadratic programming problem ﬁrst note objective function seen kernel functions correspond inner products feature spaces sparse kernel machines mensionality case however constraints amongst section however coefﬁcients weighting different already highlighted fact support vector machine address issue platt proposed ﬁtting logistic sigmoid data used sigmoid needs independent 
"
118,"[sparse, kernel, machines, joseph-louis, lagrange, relation, logistic, regression]"," separable case cast svm nonseparable distri section seen data points correct side margin boundary therefore satisfy maximum margin classiﬁers figure passes point shown red remaining points thus objective function written overall multiplicative constant form hinge error function deﬁned denotes positive part hinge error function called considered logistic regression model section found convenient work target variable comparison support note used properties logistic sigmoid function write exp sparse kernel machines comparison error functions divide error function passes point rescaled error function also plotted figure see similar form support vector error function key difference ﬂat region leads sparse solutions logistic error hinge loss viewed continuous approx imations misclassiﬁcation error another continuous error function sometimes used solve classiﬁcation problems squared error plotted figure property however placing increasing emphasis data points correctly classiﬁed long way decision boundary correct side points strongly weighted expense misclassiﬁed points objective minimize mis classiﬁcation rate monotonically decreasing error function would better choice 
"
119,"[sparse, kernel, machines, joseph-louis, lagrange, multiclass, svms]"," support vector machine fundamentally two class classiﬁer practice however often tackle problems involving classes various meth ods therefore proposed combining multiple two class svms order build multiclass classiﬁer one commonly used approach vapnik construct separate svms model trained using data class positive examples data remaining classes negative examples known one versus rest approach however figure saw using decisions individual classiﬁers lead inconsistent results input assigned multiple classes simultaneously problem sometimes addressed making predictions new inputs using max unfortunately heuristic approach suffers problem different classiﬁers trained different tasks guarantee real valued quantities different classiﬁers appropriate scales another problem one versus rest approach training sets imbalanced instance ten classes equal numbers training data points individual classiﬁers trained datasets comprising negative examples positive examples symmetry original problem lost variant one versus rest scheme proposed lee modify target values positive class target negative class target weston watkins deﬁne single objective function training svms simultaneously based maximizing margin remaining classes however result much slower training instead solving separate optimization problems data points overall cost single optimization problem size must solved giving overall cost maximum margin classiﬁers another approach train different class svms possible pairs classes classify test points according class high est number votes approach sometimes called one versus one saw figure lead ambiguities resulting classiﬁcation also large approach requires signiﬁcantly training time one versus rest approach similarly evaluate test points signiﬁcantly computation required latter problem alleviated organizing pairwise classiﬁers directed acyclic graph confused probabilistic graphical model leading dagsvm platt classes dagsvm total classiﬁers classify new test point pairwise classiﬁers need evaluated particular classiﬁers used depending path graph traversed different approach multiclass classiﬁcation based error correcting put codes developed dietterich bakiri applied support vector machines allwein viewed generalization voting scheme one versus one approach general partitions classes used train individual classiﬁers classes represented particular sets responses two class classiﬁers chosen together suitable decoding scheme gives robustness errors ambiguity outputs individual classiﬁers although application svms multiclass classiﬁcation problems remains open issue practice one versus rest approach widely used spite hoc formulation practical limitations also single class support vector machines solve unsupervised learning problem related probability density estimation instead modelling density data however methods aim ﬁnd smooth boundary enclosing region high density boundary chosen represent quantile density probability data point drawn distribution land inside region given ﬁxed number speciﬁed advance restricted problem estimating full density may sufﬁcient speciﬁc applications two approaches problem using support vector machines proposed algorithm tries ﬁnd hyperplane separates ﬁxed fraction training data origin time maximizing distance margin hyperplane origin tax duin look smallest sphere feature space contains fraction data points kernels functions two algorithms equivalent 
"
120,"[sparse, kernel, machines, joseph-louis, lagrange, svms, regression]"," extend support vector machines regression problems time preserving property sparseness simple linear regression section sparse kernel machines figure plot insensitive error function minimize regularized error function given illustrated figure therefore minimize regularized error function given express optimization problem introducing slack variables data point need two slack variables corresponds point condition target point lie inside tube introducing slack variables allows points lie outside maximum margin classiﬁers figure points tube points tube points inside tube ξgt error function support vector regression written must minimized subject constraints well achieved introducing lagrange multipliers optimizing lagrangian substitute using set derivatives grangian respect zero giving sparse kernel machines respect introduced kernel constrained maximization ﬁnd constraints note required lagrange multipliers also together require box constraints together condition substituting see predictions new inputs made using expressed terms kernel function corresponding karush kuhn tucker kkt conditions state solution product dual variables constraints must vanish given obtain several useful results first note coefﬁcient nonzero implies data point either lies upper boundary tube lies upper boundary similarly nonzero value implies points must lie either lower boundary tube furthermore two constraints incompatible easily seen adding together noting nonnegative strictly positive every data point either must zero support vectors data points contribute predictions given words either points lie boundary tube outside tube points within tube maximum margin classiﬁers sparse solution terms evaluated predictive model involve support vectors parameter found considering data point must must therefore satisfy using solving obtain used obtain analogous result considering point practice better average estimates classiﬁcation case alternative formulation svm regression parameter governing complexity intuitive interpretation particular instead ﬁxing width insensitive region instead parameter bounds fraction points lying outside tube involves maximizing subject constraints shown data points falling outside insensitive tube least data points support vectors lie either tube outside use support vector machine solve regression problem illustrated using sinusoidal data set figure parameters appendix chosen hand practice values would typically determined cross validation sparse kernel machines figure illustration svm insensitive tube corresponds comple
"
121,"[sparse, kernel, machines, joseph-louis, lagrange, computational, learning, theory]"," historically support vector machines largely motivated analysed suppose data set size drawn joint distribution drawn space functions basis training set indicator function expectation respect dis pac framework requires holds probability greater data set drawn randomly error rate small less given choice model space bounds derived within pac framework often described worst relevance vector machines case apply choice distribution long training test examples drawn independently distribution choice function long belongs real world applications machine learning deal distributions signiﬁcant reg ularity example large regions input space carry class label consequence lack assumptions form distribution pac bounds conservative words strongly estimate size datasets required achieve given generalization performance reason pac bounds found practical applications one attempt improve tightness pac bounds pac bayesian framework mcallester considers distribution space functions somewhat analogous prior bayesian treatment still considers possible choice although bounds tighter still conservative 
"
122,"[sparse, kernel, machines, relevance, vector, machines]"," support vector machines used variety classiﬁcation regression applications nevertheless suffer number limitations several highlighted already chapter particular outputs svm represent decisions rather posterior probabilities also svm originally formulated two classes extension classes prob lematic complexity parameter well parameter case regression must found using hold method cross validation finally predictions expressed linear combinations kernel functions centred training data points required positive deﬁnite relevance vector machine rvm tipping bayesian sparse kernel technique regression classiﬁcation shares many characteristics svm whilst avoiding principal limitations additionally typically leads much sparser models resulting correspondingly faster performance test data whilst maintaining comparable generalization error contrast svm shall ﬁnd convenient introduce regression form rvm ﬁrst consider extension classiﬁcation tasks 
"
123,"[sparse, kernel, machines, relevance, vector, machines, rvm, regression]"," relevance vector machine regression linear model form studied chapter modiﬁed prior results sparse solutions model deﬁnes conditional distribution real valued target variable given input vector takes form sparse kernel machines noise precision inverse noise variance mean given linear model form ﬁxed nonlinear basis functions typically include constant term corresponding weight parameter represents bias relevance vector machine speciﬁc instance model tended mirror structure support vector machine particular basis functions given kernels one kernel associated data points training set general expression takes svm like form bias parameter number parameters case form predictive model svm except coefﬁcients denoted emphasized subsequent analysis valid arbitrary choices basis function generality shall work form contrast svm restriction positive deﬁnite kernels basis functions tied either number location training data points suppose given set observations input vector denote collectively data matrix whose row corresponding target values given thus likelihood function given next introduce prior distribution parameter vector chapter shall consider zero mean gaussian prior however key differ ence rvm introduce separate hyperparameter weight parameters instead single shared hyperparameter thus weight prior takes form represents precision corresponding parameter denotes shall see maximize evidence respect hyperparameters signiﬁcant proportion inﬁnity corresponding weight parameters posterior distributions concentrated zero basis functions associated parameters therefore play role relevance vector machines predictions made model effectively pruned resulting sparse model using result linear regression models see posterior distribution weights gaussian takes form mean covariance given design matrix elements diag note speciﬁc case model symmetric kernel matrix elements values determined using type maximum likelihood also known evidence approximation maximize marginal section likelihood function obtained integrating weight parameters represents convolution two gaussians readily evaluated exercise give log marginal likelihood form deﬁned matrix given goal maximize respect hyperparameters requires small modiﬁcation results obtained section evidence approximation linear regression model identify two approaches ﬁrst simply set required derivatives marginal likelihood zero obtain following estimation equations exercise new new component posterior mean deﬁned quantity measures well corresponding parameter determined data deﬁned section sparse kernel machines diagonal component posterior covariance given learning therefore proceeds choosing initial values evalu ating mean covariance posterior using respectively alternately estimating hyperparameters using estimating posterior mean covariance using suitable convergence criterion satisﬁed second approach use algorithm discussed section two approaches ﬁnding values hyperparameters maximize evidence formally equivalent numerically however found exercise direct optimization approach corresponding gives faster convergence tipping result optimization ﬁnd proportion hyperparameters driven large principle inﬁnite values weight parameters section corresponding hyperparameters posterior distributions mean variance zero thus parameters corresponding basis functions removed model play role making predictions new inputs case models form inputs corresponding remaining nonzero weights called relevance vectors identiﬁed mechanism automatic relevance determination analogous support vectors svm worth emphasizing however mechanism achieving sparsity probabilistic models automatic relevance determination quite general applied model expressed adaptive linear combination basis functions found values hyperparameters maximize marginal likelihood evaluate predictive distribution new input using given exercise thus predictive mean given set equal posterior mean variance predictive distribution given given set optimized values familiar result obtained context linear regression recall localized basis functions predictive variance linear regression models becomes small regions input space basis functions case rvm basis functions centred data points model therefore become increasingly certain predictions extrapolating outside domain data rasmussen qui nonero candela course undesirable predictive distribution gaussian process regression section relevance vector machines figure svm regression model svm figure figure shows example rvm applied sinusoidal regression principal disadvantage rvm compared svm training general requires computation 
"
124,"[sparse, kernel, machines, relevance, vector, machines, analysis, sparsity]"," noted earlier mechanism automatic relevance determination causes subset parameters driven zero examine detail sparse kernel machines figure illustration mechanism sparsity bayesian linear regression model showing training set vector target values given indicated cross model one basis vector poorly aligned target data vector left see model isotropic noise corresponding set probable value right see model ﬁnite value case red ellipse corresponds unit mahalanobis distance taking value plots dashed green circle shows contrition arising noise term see ﬁnite value reduces probability observed data probable solution basis vector removed relevance vector machines basis vectors similar intuition holds namely particular basis vector poorly aligned data vector likely pruned model investigate mechanism sparsity mathematical per spective general case involving basis functions motivate analysis ﬁrst note result estimating parameter terms right hand side also functions results therefore rep resent implicit solutions iteration would required even determine single ﬁxed suggests different approach solving optimization problem rvm make explicit dependence marginal likelihood particular determine stationary points explicitly faul tipping tipping faul ﬁrst pull contribution matrix deﬁned give denotes column words dimensional vector elements contrast denotes row matrix represents matrix contribution basis function removed using matrix identities determinant inverse written using results write log marginal likelihood function form exercise simply log marginal likelihood basis function omitted quantity deﬁned contains dependence introduced two quantities called sparsity known quality shall see large value relative value means basis function sparse kernel machines figure plots log marginal likelihood versus showing left single maximum ﬁnite right maximum overlaps basis vectors model quality represents measure alignment basis vector error training set values vector predictions would result model vector excluded tipping faul stationary points marginal likelihood respect occur derivative equal zero two possible forms solution recalling see solve obtain note approach yielded closed form solution given solving regression problem initialize initialize using one basis function hyperparameter set using remaining hyperparameters inﬁnity included model relevance vector machines evaluate along basis functions select candidate basis function basis vector already included model update using add model evaluate hyperparameter using remove basis function model set solving regression problem update converged terminate otherwise note basis function already excluded model action required practice convenient evaluate quantities quality sparseness variables expressed form note using write exercise φσφ φσφ stage required computations therefore scale like 
"
125,"[sparse, kernel, machines, relevance, vector, machines, rvm, classiﬁcation]"," extend relevance vector machine framework classiﬁcation prob model takes form linear combination basis functions transformed logistic sigmoid function sparse kernel machines logistic sigmoid function deﬁned introduce gaussian prior weight vector obtain model considered already chapter difference rvm model uses ard prior separate precision hyperparameter associated weight parameter contrast regression model longer integrate analytically parameter vector follow tipping use laplace proximation applied closely related problem bayesian logistic section regression section begin initializing hyperparameter vector given value build gaussian approximation posterior distribution thereby obtain approximation marginal likelihood maximization approxi mate marginal likelihood leads estimated value process repeated convergence let consider laplace approximation model detail ﬁxed value mode posterior distribution obtained maximizing const diag done using iterative reweighted least squares irls discussed section need gradient vector hessian matrix log posterior distribution given exercise diagonal matrix elements vector design matrix elements used property derivative logistic sigmoid function convergence irls algorithm negative hessian represents inverse covariance matrix gaussian approximation posterior distribution mode resulting approximation posterior distribution corre sponding mean gaussian approximation obtained setting zero giving mean covariance laplace approximation form use laplace approximation evaluate marginal likelihood using general result integral evaluated using laplace approxi relevance vector machinesmation substitute set derivative marginal likelihood respect equal zero obtain exercise deﬁning rearranging gives new identical estimation formula obtained regression rvm deﬁne write approximate log marginal likelihood form φaφ takes form regression case apply analysis sparsity obtain fast learning algorithm fully optimize single hyperparameter step figure shows relevance vector machine applied synthetic classiﬁ cation data set see relevance vectors tend lie region appendix decision boundary contrast support vector machine consistent earlier discussion sparsity rvm basis function centred data point near boundary vector poorly aligned training data vector one potential advantages relevance vector machine compared svm makes probabilistic predictions example allows rvm used help construct emission density nonlinear extension linear dynamical system tracking faces video sequences williams section far considered rvm binary classiﬁcation problems sparse kernel machines figure example relevance vector machine applied synthetic data set left hand plot combined using softmax function give outputs exponent log likelihood function given target values coding data point matrix elements laplace approximation used optimize number active basis functions gives additional factor computational cost training compared two class rvm principal disadvantage relevance vector machine relatively long exercises 
"
126,"[sparse, kernel, machines, exercises]"," suppose data set input vectors corresponding target values suppose model density input vec write minimum misclassiﬁcation rate classiﬁcation rule reduces classiﬁcation based closest mean feature space show irrespective dimensionality data space data set show value margin maximum margin hyper plane given given maximizing subject constraints show values previous exercise also satisfy deﬁned similarly show consider logistic regression model target variable deﬁne given show negative zero sparse kernel machines regression support vector machine considered section show training data points similarly points verify results mean covariance posterior distribution weights regression rvm derive result marginal likelihood function regression rvm performing gaussian integral using technique completing square exponential repeat exercise time make use general result show direct maximization log marginal likelihood regression relevance vector machine leads estimation equations deﬁned evidence framework rvm regression obtained estimation formulae maximizing marginal likelihood given extend approach inclusion hyperpriors given gamma distributions form obtain corresponding estimation formulae maximizing corresponding posterior probability respect derive result predictive distribution relevance vector machine regression show predictive variance given using results show marginal likelihood written form deﬁned sparsity quality factors deﬁned respectively taking second derivative log marginal likelihood regression rvm respect hyperparameter show stationary point given maximum marginal likelihood using together matrix identity show quantities deﬁned written form show gradient vector hessian matrix log poste rior distribution classiﬁcation relevance vector machine given verify maximization approximate log marginal likelihood function classiﬁcation relevance vector machine leads result estimation hyperparameters 
"
127,"[graphical, models]"," probabilities play central role modern pattern recognition seen chapter probability theory expressed terms two simple equations corresponding sum rule product rule probabilistic infer ence learning manipulations discussed book matter complex amount repeated application two equations could therefore proceed formulate solve complicated probabilistic models purely algebraic nipulation however shall ﬁnd highly advantageous augment analysis using diagrammatic representations probability distributions called probabilistic graphical models offer several useful properties provide simple way visualize structure probabilistic model used design motivate new models insights properties model including conditional independence properties obtained inspection graph graphical models complex computations required perform inference learning sophisticated models expressed terms graphical manipulations underlying mathematical expressions carried along implicitly graph comprises nodes also called vertices connected links also known edges arcs probabilistic graphical model node represents random variable group random variables links express probabilistic relation ships variables graph captures way joint distribution random variables decomposed product factors depending subset variables shall begin discussing bayesian networks also known directed graphical models links graphs particular directionality indicated arrows major class graphical models markov random ﬁelds also known undirected graphical models links carry arrows directional signiﬁcance directed graphs useful expressing causal relationships random variables whereas undirected graphs better suited expressing soft con straints random variables purposes solving inference problems often convenient convert directed undirected graphs different representation called factor graph chapter shall focus key aspects graphical models needed applications pattern recognition machine learning general treat ments graphical models found books whittaker lauritzen jensen castillo jordan cowell jordan 
"
128,"[graphical, models, bayesian, networks]"," order motivate use directed graphs describe probability distributions consider ﬁrst arbitrary joint distribution three variables note stage need specify anything variables whether discrete continuous indeed one powerful aspects graphical models speciﬁc graph make probabilistic statements broad class distributions application product rule probability write joint distribution form second application product rule time second term right hand side gives note decomposition holds choice joint distribution represent right hand side terms simple graphical model follows first introduce node random variables associate node corresponding conditional distribution right hand side bayesian networks figure directed graphical model representing joint probability distribution three variables corresponding decomposition right hand side conditional distribution add directed links arrows graph nodes corresponding variables distribution conditioned thus factor links nodes node whereas factor incoming links result graph shown figure link going node node say node parent node say node child node note shall make formal distinction node variable corresponds simply use symbol refer interesting point note left hand side symmetrical respect three variables whereas right hand side indeed making decomposition implicitly chosen particular ordering namely chosen different ordering would obtained different decomposition hence different graphical representation shall return point later moment let extend example figure considering joint distribution variables given repeated application product rule probability joint distribution written product conditional distributions one variables given choice represent directed graph nodes one conditional distribution right hand side node incoming links lower numbered nodes say graph fully connected link every pair nodes far worked completely general joint distributions decompositions representations fully connected graphs applica ble choice distribution shall see shortly absence links graph conveys interesting information properties class distributions graph represents consider graph shown figure fully connected graph instance link shall graph corresponding representation joint probability distribution written terms product set conditional distributions one node graph conditional distribution conditioned parents corresponding node graph stance conditioned joint distribution variables graphical models figure example directed acyclic graph describing joint distribution variables corresponding decomposition joint distribution given therefore given reader take moment study carefully correspondence figure state general terms relationship given directed graph corresponding distribution variables joint distribution deﬁned graph given product nodes graph conditional distribution node conditioned variables corresponding parents node graph thus graph nodes joint distribution given denotes set parents key equation expresses factorization properties joint distribution directed graphical model although considered node correspond single variable equally well associate sets variables vector valued variables nodes graph easy show representation right hand side always correctly normalized provided individual conditional distributions normalized exercise directed graphs considering subject important restriction namely must directed cycles words closed paths within graph move node node along links following direction arrows end back starting node graphs also called directed acyclic graphs dags equivalent statement exercise exists ordering nodes links node lower numbered node 
"
129,"[graphical, models, bayesian, networks, example, polynomial, regression]"," illustration use directed graphs describe probability distri butions consider bayesian polynomial regression model introduced sec bayesian networks figure directed graphical model representing joint distribution corresponding bayesian polynomial regression model introduced sectiontion random variables model vector polynomial coefﬁcients observed data addition model contains input data noise variance hyperparameter representing precision gaussian prior parameters model rather random variables focussing random variables moment see joint distribution given product prior conditional distributions joint distribution represented graphical model shown figure start deal complex models later book shall ﬁnd inconvenient write multiple nodes form explicitly figure therefore introduce graphical notation allows multiple nodes expressed compactly draw single representative node surround box called plate labelled indicating nodes kind writing graph figure way obtain graph shown figure shall sometimes ﬁnd helpful make parameters model well stochastic variables explicit case becomes correspondingly make explicit graphical representation shall adopt convention random variables denoted open circles deterministic parameters denoted smaller solid circles take graph figure include deterministic parameters obtain graph shown figure apply graphical model problem machine learning pattern recognition typically set random variables speciﬁc observed figure alternative compact representation graph shown figure introduced plate box labelled represents nodes single example shown explicitly graphical models figure shows model figure deterministic parameters shown explicitly smaller solid nodes values example variables training set case polynomial curve ﬁtting graphical model denote observed variables shading corresponding nodes thus graph corresponding figure variables observed shown figure note value observed example latent variable also known hidden variable variables play crucial role many probabilistic models form focus chapters observed values desired evaluate posterior distributionpolynomial coefﬁcients discussed section moment note involves straightforward application bayes theorem omitted deterministic parameters order keep notation uncluttered general model parameters little direct interest ultimate goal make predictions new input values suppose given new input value wish ﬁnd corresponding probability distributionconditioned observed data graphical model describes problem shown figure corresponding joint distribution random variables model conditioned deterministic parameters given figure figure nodes shaded indicate corresponding random variables set observed training set values bayesian networks figure polynomial regression model corresponding figure showing also new input value together corresponding model prediction required predictive distribution obtained sum rule probability integrating model parameters 
"
130,"[graphical, models, bayesian, networks, generative, models]"," many situations wish draw samples given prob variables factorizes according joint distribution start lowest numbered node draw sample distribution call work nodes der node draw sample conditional distribution achieved objective obtaining simply sample full joint distribution retain values discard remaining values graphical models figure graphical model representing process images objects created identity object discrete variable position orientation object continuous variables independent prior probabilities image vector pixel intensities probability distribution dependent identity object well position orientation image object orientation position practical applications probabilistic models typically higher numbered variables corresponding terminal nodes graph represent observations lower numbered nodes corresponding latent variables primary role latent variables allow complicated distribution observed variables represented terms model constructed simpler typically exponential family conditional distributions interpret models expressing processes observed data arose instance consider object recognition task observed data point corresponds image comprising vector pixel intensities one objects case latent variables might interpretation position orientation object given particular observed image goal ﬁnd posterior distribution objects integrate possible positions orientations represent problem using graphical model form show figure graphical model captures causal process pearl served data generated reason models often called generative models contrast polynomial regression model described figure generative probability distribution associated input variable possible generate synthetic data points model could make generative introducing suitable prior distribution expense complex model hidden variables probabilistic model need however plicit physical interpretation may introduced simply allow complex joint distribution constructed simpler components either case technique ancestral sampling applied generative model mimics creation observed data would therefore give rise fantasy data whose probability distribution model perfect representation reality would observed data practice producing synthetic observations generative model prove informative understanding form probability distribution represented model 
"
131,"[graphical, models, bayesian, networks, discrete, variables]"," discussed importance probability distributions members exponential family seen family includes many well section known distributions particular cases although distributions relatively simple form useful building blocks constructing complex probability bayesian networks figure fully connected graph describes general distribution two state discrete variables total parameters dropping link nodes number parameters reduced models particularly nice properties choose relationship probability distribution single discrete variable possible states using representation given governed parameters due constraint values need speciﬁed order deﬁne distribution suppose two discrete variables parameter denotes component similarly joint distribution written parameters subject constraint distri bution governed parameters easily seen total number therefore grows exponentially number variables using product rule factor joint distribution form corresponds two node graph link going node node shown figure marginal distribution governed parameters similarly conditional distribution requires speciﬁcation parameters possible values total number parameters must speciﬁed joint distribution therefore suppose variables independent corresponding graphical model shown figure variable described graphical models figure chain discrete nodes states requires speciﬁcation parameters grows linearly length chain contrast fully con nected graph nodes would parameters grows exponentially separate multinomial distribution total number parameters would distribution independent discrete variables states total number parameters would therefore grows lineacomplerly number variables graphical perspective reduced number parameters dropping links graph expense restricted class distributions generally discrete variables model joint distribution using directed graph one variable corresponding node conditional distribution node given set nonnegative parameters subject usual normalization constraint graph fully connected completely general distribution parameters whereas links graph joint distribution factorizes product marginals total number parameters graphs termediate levels connectivity allow general distributions fully factorized one requiring fewer parameters general joint distribution illustration consider chain nodes shown figure marginal distribution requires parameters whereas conditional distributions requires parameters gives total parameter count quadratic grows linearly rather exponentially length chain alternative way reduce number independent parameters model sharing parameters also known tying parameters instance chain example figure arrange conditional distributions governed set parameters together parameters governing distribution gives total parameters must speciﬁed order deﬁne joint distribution turn graph discrete variables bayesian model introducing dirichlet priors parameters graphical point view node acquires additional parent representing dirichlet distribution parameters associated corresponding discrete node illustrated chain model figure corresponding model tie parame ters governing conditional distributions shown figure another way controlling exponential growth number parameters models discrete variables use parameterized models conditional distributions instead complete tables conditional probability values illus trate idea consider graph figure nodes represent binary variables parent variables governed single parame bayesian networks figure shared representing probability giving parameters total parent nodes conditional distribution however would require parameters representing probability possible section exp logistic sigmoid whose value clamped vector figure graph comprising parents single child used illustrate idea parameterized conditional distributions discrete variables graphical models 
"
132,"[graphical, models, bayesian, networks, linear-gaussian, models]"," previous section saw construct joint probability distributions set discrete variables expressing variables nodes directed acyclic graph show multivariate gaussian expressed directed graph corresponding linear gaussian model component variables allows impose interesting structure distribution general gaussian diagonal covariance gaussian representing opposite tremes several widely used techniques examples linear gaussian models probabilistic principal component analysis factor analysis linear namical systems roweis ghahramani shall make extensive use results section later chapters consider techniques detail consider arbitrary directed acyclic graph variables node represents single continuous random variable gaussian distribution mean distribution taken linear combination states parent nodes node parameters governing mean variance conditional distribution log joint distribution log product conditionals nodes graph hence takes form const const denotes terms independent see quadratic function components hence joint distribution multivariate gaussian determine mean covariance joint distribution recursively follows variable conditional states parents gaussian distribution form zero mean unit variance gaussian random variable satisfying element identity matrix taking expectation bayesian networks figure directed graph three gaussian variables one missing link thus ﬁnd components starting lowest numbered node working recursively graph assume nodes numbered node higher number parents similarly use obtain element covariance matrix form recursion relation covariance covariance similarly evaluated recursively starting lowest numbered node let consider two extreme cases first suppose links graph therefore comprises isolated nodes case parameters parameters parameters recursion relations see mean given covariance matrix diagonal form diag joint distribution total parameters represents set independent univariate gaussian distributions consider fully connected graph node lower numbered nodes parents matrix entries row hence lower triangular matrix entries leading diagonal total number parameters obtained taking number elements matrix subtracting account absence elements leading diagonal dividing matrix elements diagonal giving total total number independent parameters covariance matrix therefore corresponding general symmetric covariance matrix section graphs intermediate level complexity correspond joint gaussian distributions partially constrained covariance matrices consider ample graph shown figure link missing variables using recursion relations see mean covariance joint distribution given exercise graphical models readily extend linear gaussian graphical model case nodes graph represent multivariate gaussian variables case write conditional distribution node form matrix nonsquare different dimensionalities easy verify joint distribution variables gaussian note already encountered speciﬁc example linear gaussian relationship saw conjugate prior mean gaussian section variable gaussian distribution joint distribution therefore gaussian corresponds simple two node graph node representing parent node representing mean distribution parameter controlling prior viewed hyperparameter value hyperparameter may unknown treat bayesian perspective introducing prior hyperparameter sometimes called hyperprior given gaussian distribution type construction extended principle level illustration hierarchical bayesian model shall encounter examples later chapters 
"
133,"[graphical, models, conditional, independence]"," important concept probability distributions multiple variables conditional independence dawid consider three variables suppose conditional distribution given depend value say conditionally independent given expressed slightly different way consider joint distribution conditioned write form used product rule probability together thus see conditioned joint distribution factorizes prod uct marginal distribution marginal distribution conditioned says variables statistically independent given note deﬁnition conditional independence require conditional independence figure ﬁrst three examples graphs three variables used discuss conditional independence properties directed graphical models equivalently must hold every possible value values shall sometimes use shorthand notation conditional independence dawid denotes conditionally independent given equivalent conditional independence properties play important role using probabilistic models pattern recognition simplifying structure model computations needed perform inference learning model shall see examples shortly given expression joint distribution set variables terms product conditional distributions mathematical representation underlying directed graph could principle test whether potential conditional independence property holds repeated application sum product rules probability practice approach would time con suming important elegant feature graphical models conditional independence properties joint distribution read directly graph without perform analytical manipulations general framework achieving called separation stands directed pearl shall motivate concept separation give general state ment separation criterion formal proof found lauritzen 
"
134,"[graphical, models, conditional, independence, three, example, graphs]"," begin discussion conditional independence properties directed graphs considering three simple examples involving graphs three nodes together motivate illustrate key concepts separation ﬁrst three examples shown figure joint distribution corresponding graph easily written using general result give none variables observed investigate whether independent marginalizing sides respect give general factorize product graphical models figure figure conditioned value variable denotes empty set symbol means conditional independence property hold general course may hold particular distribution virtue speciﬁc numerical values associated various conditional probabilities follow general structure graph suppose condition variable represented graph figure easily write conditional distribution given form obtain conditional independence property provide simple graphical interpretation result considering path node node via node said tail tail spect path node connected tails two arrows presence path connecting nodes causes nodes pendent however condition node figure conditioned node blocks path causes become conditionally independent similarly consider graph shown figure joint distribution corresponding graph obtained general formula give first suppose none variables observed test see independent marginalizing give figure second three examples node graphs used motivate conditional indepen dence framework directed graphical models conditional independence figure figure conditioning node general factorize suppose condition node shown figure using bayes theorem together obtain obtain conditional independence property interpret results graphically node said head tail respect path node node path connects nodes renders dependent observe figure observation blocks path obtain conditional independence property finally consider third node examples shown graph figure shall see subtle behaviour two previous graphs joint distribution written using general result give consider ﬁrst case none variables observed marginalizing sides obtain figure last three examples node graphs used explore conditional independence properties graphical models graph rather different properties two previous examples graphical models figure figure conditioning value node graph act conditioning induces depen dence independent variables observed contrast two previous examples write result suppose condition indicated figure conditional distri bution given general factorize product thus third example opposite behaviour ﬁrst two graphically say node head head respect path connects heads two arrows node unobserved blocks path variables independent however conditioning unblocks path renders dependent one subtlety associated third example need consider first introduce terminology say node scendant node path step path follows directions arrows shown head head path become unblocked either node descendants observed exercise summary tail tail node head tail node leaves path unblocked unless observed case blocks path contrast head head node blocks path unobserved node least one descendants observed path becomes unblocked worth spending moment understand unusual behaviour graph figure consider particular instance graph corresponding problem three binary random variables relating fuel system car shown figure variables called representing state battery either charged ﬂat representing state fuel tank either full fuel empty state electric fuel gauge indicates either full empty conditional independence figure example node graph used illustrate phenomenon explaining away three nodes represent state battery state fuel tank reading electric fuel gauge see text details battery either charged ﬂat independently fuel tank either full empty prior probabilities given state fuel tank battery fuel gauge reads full proba bilities given rather unreliable fuel gauge remaining probabilities determined requirement probabilities sum one complete speciﬁ cation probabilistic model observe data prior probability fuel tank empty suppose observe fuel gauge discover reads empty corresponding middle graph figure use bayes theorem evaluate posterior probability fuel tank empty first evaluate denominator bayes theorem given similarly evaluate using results graphical models thus observing gauge reads empty makes likely tank indeed empty would intuitively expect next suppose also check state battery ﬁnd ﬂat observed states fuel gauge battery shown right hand graph figure posterior probability fuel tank empty given observations fuel gauge battery state given prior probability cancelled numerator denom inator thus probability tank empty decreased result observation state battery accords intuition ﬁnding battery ﬂat explains away observation fuel gauge reads empty see state fuel tank battery indeed become dependent result observing reading fuel gauge fact would also case instead observing fuel gauge directly observed state descendant note probability greater prior probability observation fuel gauge reads zero still provides evidence favour empty fuel tank 
"
135,"[graphical, models, conditional, independence, d-separation]"," give general statement separation property pearl directed graphs consider general directed graph arbi trary nonintersecting sets nodes whose union may smaller complete set nodes graph wish ascertain whether particular conditional independence statement implied given directed acyclic graph consider possible paths node node path said blocked includes node either arrows path meet either head tail tail tail node node set arrows meet head head node neither node descendants set paths blocked said separated joint distribution variables graph satisfy concept separation illustrated figure graph path blocked node tail tail node path observed blocked node although latter head head node descendant conditioning set thus conditional independence statement follow graph graph path blocked node tail tail node observed conditional independence property conditional independence figure illustration concept separation see text details satisﬁed distribution factorizes according graph note path also blocked node head head node neither descendant conditioning set purposes separation parameters figure indicated small ﬁlled circles behave observed nodes ever marginal distributions associated nodes consequently parameter nodes never parents paths nodes always tail tail hence blocked consequently play role separation another example conditional independence separation provided concept independent identically distributed data introduced section consider problem ﬁnding posterior distribution mean univariate gaussian distribution represented directed graph section shown figure joint distribution deﬁned prior gether set conditional distributions practice observe goal infer suppose moment condition consider joint distribution observations using separation note unique path path tail tail respect observed node every path blocked observations independent given figure directed graph corre sponding problem inferring mean univariate gaussian distributionobservations graph drawn using plate notation graphical models figure conditioned class label components observed vector assumed independent latent variable value observed another example model representing data graph figure see node tail tail respect path one nodes following conditional independence property thus conditioned polynomial coefﬁcients predictive distribution independent training data therefore ﬁrst use new input observations related graphical structure arises approach classiﬁcation called wish assign observed values one class labels component prior probability class together conditional distribution paths tail tail node conditionally independent longer blocked tells general marginal given labelled training set comprising inputs together class labels naive bayes model training data conditional independence using maximum likelihood assuming data drawn independently model solution obtained ﬁtting model class separately using correspondingly labelled data example suppose probability density within class chosen gaussian case naive bayes assumption implies covariance matrix gaussian diagonal contours constant density within class axis aligned ellipsoids marginal density however given superposition diagonal gaussians weighting coefﬁcients given class priors longer factorize respect components naive bayes assumption helpful dimensionality input space high making density estimation full dimensional space chal lenging also useful input vector contains discrete continuous variables since represented separately using appropriate models bernoulli distributions binary observations gaussians real valued variables conditional independence assumption model clearly strong one may lead rather poor representations class conditional densities nevertheless even assumption precisely satisﬁed model may still give good classiﬁcation performance practice decision boundaries insensitive details class conditional densities illustrated figure seen particular directed graph represents speciﬁc decomposition joint probability distribution product conditional probabilities graph also expresses set conditional independence statements obtained separation criterion separation theorem really expression equivalence two properties order make clear helpful think directed graph ﬁlter suppose consider particular joint probability distribution variables corresponding nonobserved nodes graph ﬁlter allow distribution pass expressed terms factorization implied graph present ﬁlter set possible distributions set variables subset distributions passed ﬁlter denoted directed factorization illustrated figure alternatively use graph different kind ﬁlter ﬁrst listing conditional independence properties obtained applying separation criterion graph allowing distribution pass satisﬁes properties present possible distributions second kind ﬁlter separation theorem tells set distributions allowed precisely set emphasized conditional independence properties obtained separation apply probabilistic model described particular rected graph true instance whether variables discrete continuous combination see particular graph scribing whole family probability distributions one extreme fully connected graph exhibits conditional dependence properties represent possible joint probability distribution given variables set contain possible distribu graphical models figure view graphical model case directed graph ﬁlter prob ability distribution allowed ﬁlter satisﬁes directed factorization property set possible probability distributions pass ﬁlter denoted alternatively use graph ﬁlter distributions according whether respect conditional independencies implied separation properties graph separation theorem says set distributions allowed second kind ﬁltertions extreme fully disconnected graph one links corresponds joint distributions factorize product marginal distributions variables comprising nodes graph note given graph set distributions include distributions additional independence properties beyond described graph instance fully factorized distribution always passed ﬁlter implied graph corresponding set variables end discussion conditional independence properties exploring concept markov blanket markov boundary consider joint distribution represented directed graph nodes consider conditional distribution particular node variables conditioned remaining variables using factorization property express conditional distribution form integral replaced summation case discrete variables observe factor functional dependence taken outside integral therefore cancel numerator denominator factors remain conditional distribution node together conditional distributions nodes node conditioning set words parent conditional depend parents node whereas conditionals depend children markov random fields figure markov blanket node comprises set parents children parents node property conditional distribution conditioned remaining variables graph dependent variables markov blanket well parents words variables corresponding parents node node set nodes comprising parents children parents called markov blanket illustrated figure think markov blanket node minimal set nodes isolates rest graph note sufﬁcient include parents children node phenomenon explaining away means observations child nodes block paths parents must therefore observe parent nodes also 
"
136,"[graphical, models, markov, random, fields]"," seen directed graphical models specify factorization joint distributionset variables product local conditional distributions also deﬁne set conditional independence properties must satisﬁed distribution factorizes according graph turn second jor class graphical models described undirected graphs specify factorization set conditional independence relations markov random ﬁeld also known markov network undirected graphical model kindermann snell set nodes corresponds variable group variables well set links connects pair nodes links undirected carry arrows case undirected graphs convenient begin discussion conditional independence properties 
"
137,"[graphical, models, markov, random, fields, conditional, independence, properties]"," case directed graphs saw possible test whether section particular conditional independence property holds applying graphical test called separation involved testing whether paths connecting two sets nodes blocked deﬁnition blocked however somewhat subtle due presence paths head head nodes might ask whether possible deﬁne alternative graphical semantics probability distributions conditional independence determined simple graph separation indeed case corresponds undirected graphical models removing graphical models figure example undirected graph every path node set node set passes least one node set conse quently conditional independence property holds probability distribution described graph directionality links graph asymmetry parent child nodes removed subtleties associated head head nodes longer arise suppose undirected graph identify three sets nodes denoted consider conditional independence property test whether property satisﬁed probability distribution deﬁned graph consider possible paths connect nodes set nodes set paths pass one nodes set paths blocked conditional independence property holds however least one path blocked property necessarily hold precisely exist least distributions corresponding graph satisfy conditional independence relation illustrated example figure note exactly separation crite rion except explaining away phenomenon testing conditional independence undirected graphs therefore simpler directed graphs alternative way view conditional independence test imagine moving nodes set graph together links connect nodes ask exists path connects node node paths conditional independence property must hold markov blanket undirected graph takes particularly simple form node conditionally independent nodes conditioned neighbouring nodes illustrated figure 
"
138,"[graphical, models, markov, random, fields, factorization, properties]"," seek factorization rule undirected graphs correspond conditional independence test involve expressing joint distribution product functions deﬁned sets variables local graph thereforeneed decide appropriate notion locality case markov random fields figure undirected graph markov blanket node consists set neighbouring nodes property conditional distribution conditioned remaining variables graph dependent variables markov blanket consider two nodes connected link variables must conditionally independent given nodes graph follows fact direct path two nodes paths pass nodes observed hence paths blocked conditional independence property expressed denotes set variables removed factor ization joint distribution must therefore appear factor order conditional independence property hold possible distributions belonging graph leads consider graphical concept called clique deﬁned subset nodes graph exists link pairs nodes subset words set nodes clique fully connected furthermore maximal clique clique possible include nodes graph set without ceasing clique concepts illustrated undirected graph four variables shown figure graph ﬁve cliques two nodes given well two maximal cliques given set clique missing link therefore deﬁne factors decomposition joint distribution functions variables cliques fact consider functions maximal cliques without loss generality cliques must subsets maximal cliques thus maximal clique deﬁne arbitrary function clique including another factor deﬁned subset variables would redundant let denote clique set variables clique figure four node undirected graph showing clique outlined green maximal clique outlined blue graphical models joint distribution written product potential functions maximal cliques graph quantity sometimes called partition function normalization con stant given ensures distribution given correctly normalized considering potential functions satisfy ensure assumed comprises discrete variables framework equally applicable continuous variables combination two summation replaced appropriate combination summation integration note restrict choice potential functions speciﬁc probabilistic interpretation marginal conditional distributions contrast directed graphs factor represents conditional distribution corresponding variable conditioned state parents however special cases instance undirected graph constructed starting directed graph potential functions may indeed interpretation shall see shortly one consequence generality potential functions product general correctly normalized therefore troduce explicit normalization factor given recall directed graphs joint distribution automatically normalized consequence normalization conditional distributions factorization presence normalization constant one major limitations undirected graphs model discrete nodes states evaluation normalization term involves summing states worst case exponential size model partition function needed parameter learning function parameters govern potential functions however evaluation local conditional distributions partition function needed conditional ratio two marginals partition function cancels numerator denom inator evaluating ratio similarly evaluating local marginal probabil ities work unnormalized joint distribution normalize marginals explicitly end provided marginals involves small number variables evaluation normalization coefﬁcient feasible far discussed notion conditional independence based simple graph separation proposed factorization joint distribution intended correspond conditional independence structure however made formal connection conditional independence factorization undirected graphs need restrict attention potential functions strictly positive never zero negative markov random fields choice given restriction make precise relationship factorization conditional independence return concept graphical model ﬁlter corre sponding figure consider set possible distributions deﬁned ﬁxed set variables corresponding nodes particular undirected graph deﬁne set distributions consistent set conditional independence statements read graph using graph separation similarly deﬁne set distributions expressed factorization form respect maximal cliques graph hammersley clifford theorem clifford states sets identical restricted potential functions strictly positive convenient express exponentials exp called energy function exponential representation called boltzmann distribution joint distribution deﬁned product potentials total energy obtained adding energies maximal cliques contrast factors joint distribution directed graph tentials undirected graph speciﬁc probabilistic interpretation although gives greater ﬂexibility choosing potential functions normalization constraint raise question motivate choice potential function particular application done viewing potential function expressing conﬁgurations local variables preferred others global conﬁgurations relatively high probability ﬁnd good balance satisfying possibly conﬂicting inﬂuences clique potentials turn speciﬁc example illustrate use undirected graphs 
"
139,"[graphical, models, markov, random, fields, illustration, image, de-noising]"," illustrate application undirected graphs using example noise removal binary image besag geman geman besag although simple example typical sophisticated applications let observed noisy image described array binary pixel values index runs pixels shall suppose image obtained taking unknown noise free image described binary pixel values randomly ﬂipping sign pixels small probability example binary image together noise corrupted image obtained ﬂipping sign pixels probability shown figure given noisy image goal recover original noise free image noise level small know strong correlation also know neighbouring pixels image strongly correlated prior knowledge captured using markov graphical models figure illustration image noising using markov random ﬁeld top row shows original binary image left corrupted image randomly changing pixels right bottom row shows restored images obtained using iterated conditional models icm left using graph cut algorithm right icm produces image pixels agree original image whereas corresponding number graph cut random ﬁeld model whose undirected graph shown figure graph two types cliques contains two variables cliques form associated energy function expresses correlation variables choose simple energy function cliques form positive constant desired effect giving lower energy thus encouraging higher probability sign higher energy opposite sign remaining cliques comprise pairs variables indices neighbouring pixels want energy lower pixels sign opposite sign choose energy given positive constant potential function arbitrary nonnegative function maximal clique multiply nonnegative functions subsets clique markov random fields figure undirected graphical model representing markov random ﬁeld image noising binary variable denoting state pixel unknown noise free image denotes corresponding value pixel observed noisy image equivalently add corresponding energies example allows add extra term pixel noise free image term effect biasing model towards pixel values one particular sign preference complete energy function model takes form deﬁnes joint distribution given exp elements observed values given pixels noisy image implicitly deﬁnes conditional distribution noise free images example ising model widely studied statistical physics purposes image restoration wish ﬁnd image high probability ideally maximum probability shall use simple iterative technique called iterated conditional modes icm kittler oglein simply application coordinate wise gradient ascent idea ﬁrst initialize variables simply setting take one node time evaluate total energy two possible states keeping node variables ﬁxed set whichever state lower energy either leave probability unchanged unchanged increase one variable changed simple local computation performed exercise efﬁciently repeat update another site suitable stopping criterion satisﬁed nodes may updated systematic way instance repeatedly raster scanning image choosing nodes random sequence updates every site visited least changes variables made deﬁnition algorithm graphical models figure example directed graph equivalent undirected graph converged local maximum probability need however correspond global maximum purposes simple illustration ﬁxed parameters note leaving simply means prior probabilities two states equal starting observed noisy image initial conﬁguration run icm convergence leading noised image shown lower left panel figure note set effectively removes links neighbouring pixels global probable solution given corresponding observed noisy image exercise later shall discuss effective algorithm ﬁnding high probability lutions called max product algorithm typically leads better solutions section although still guaranteed ﬁnd global maximum posterior distributionhowever certain classes model including one given exist efﬁcient algorithms based graph cuts guaranteed ﬁnd global maximum greig boykov kolmogorov zabih lower right panel figure shows result applying graph cut algorithm noising problem 
"
140,"[graphical, models, markov, random, fields, relation, directed, graphs]"," introduced two graphical frameworks representing probability distributions corresponding directed undirected graphs instructive discuss relation consider ﬁrst problem taking model speciﬁed using directed graph trying convert undirected graph cases straightforward simple example figure joint distribution directed graph given product conditionals form let convert undirected graph representation shown figure undirected graph maximal cliques simply pairs neigh bouring nodes wish write joint distribution form markov random fields figure example simple directed graph corre sponding moral graph easily done identifying absorbed marginal ﬁrst node ﬁrst potential function note case partition function let consider generalize construction convert distribution speciﬁed factorization directed graph one speciﬁed factorization undirected graph achieved clique potentials undirected graph given conditional distributions directed graph order valid must ensure set variables appears conditional distributions member least one clique undirected graph nodes directed graph one parent achieved simply replacing directed link undirected link however nodes directed graph one parent sufﬁcient nodes head head paths encountered discussion conditional independence consider simple directed graph nodes shown figure joint distribution directed graph takes form see factor involves four variables must belong single clique conditional distribution absorbed clique potential ensure add extra links pairs parents node anachronistically process marrying parents become known moralization resulting undirected graph dropping arrows called moral graph important observe moral graph example fully connected exhibits conditional independence properties contrast original directed graph thus general convert directed graph undirected graph ﬁrst add additional undirected links pairs parents node graph graphical models drop arrows original links give moral graph initialize clique potentials moral graph take conditional distribution factor original directed graph multiply one clique potentials always exist least one maximal clique contains variables factor result moralization step note cases partition function given process converting directed graph undirected graph plays important role exact inference techniques junction tree algorithm section converting undirected directed representation much less common general presents problems due normalization constraints saw going directed undirected representation discard conditional independence properties graph course could always trivially convert distribution directed graph one undirected graph simply using fully connected undirected graph would however discard conditional independence properties would vacuous process moralization adds fewest extra links retains maximum number independence properties seen procedure determining conditional independence properties different directed undirected graphs turns two types graph express different conditional independence properties worth exploring issue detail return view speciﬁc directed undirected graph ﬁlter set possible section distributions given variables could reduced subset respects conditional independencies implied graph graph said map dependency map distribution every conditional independence statement satisﬁed distribution reﬂected graph thus completely disconnected graph links trivial map distribution alternatively consider speciﬁc distribution ask graphs appropriate conditional independence properties every conditional indepen dence statement implied graph satisﬁed speciﬁc distribution graph said map independence map distribution clearly fully connected graph trivial map distribution case every conditional independence property distribution reﬂected graph vice versa graph said perfect map figure venn diagram illustrating set distributions given set variables together set distributions represented perfect map using directed graph set represented perfect map using undirected graph inference graphical models figure directed graph whose conditional independence properties cannot expressed using undirected graph three variables distribution perfect map therefore map consider set distributions distribution exists directed graph perfect map set distinct set distributions distribution exists undirected graph perfect map addition distributions neither directed undirected graphs offer perfect map illustrated venn diagram figure figure shows example directed graph perfect map distribution satisfying conditional independence properties corresponding undirected graph three variables perfect map conversely consider undirected graph four variables shown figure graph exhibits properties directed graph four variables implies set conditional independence properties graphical framework extended consistent way graphs include directed undirected links called chain graphs lauritzen wermuth frydenberg contain directed undirected graphs considered far special cases although graphs represent broader class distributions either directed undirected alone remain distributions even chain graph cannot provide perfect map chain graphs discussed book figure undirected graph whose conditional independence properties cannot expressed terms directed graph variables 
"
141,"[graphical, models, inference, graphical, models]"," turn problem inference graphical models nodes graph clamped observed values wish compute posterior distributions one subsets nodes shall see exploit graphical structure ﬁnd efﬁcient algorithms inference graphical models figure graphical representation bayes theorem see text details make structure algorithms transparent speciﬁcally shall see many algorithms expressed terms propagation local messages around graph section shall focus primarily techniques exact inference chapter shall consider number approximate inference algorithms start let consider graphical interpretation bayes theorem suppose decompose joint distribution two variables product factors form represented directed graph shown figure suppose observe value indicated shaded node figure view marginal distribution prior latent variable goal infer corresponding posterior distribution using sum product rules probability evaluate used bayes theorem calculate thus joint distribution expressed terms graphical perspective joint distribution represented graph shown figure direction arrow reversed simplest example inference problem graphical model 
"
142,"[graphical, models, inference, graphical, models, inference, chain]"," consider complex problem involving chain nodes form shown figure example lay foundation discussion exact inference general graphs later section speciﬁcally shall consider undirected graph figure already seen directed chain transformed equivalent undirected chain directed graph nodes one parent require addition extra links directed undirected versions graph express exactly set conditional independence statements inference graphical models joint distribution graph takes form shall consider speciﬁc case nodes represent discrete variables states case potential function comprises table joint distribution parameters let consider inference problem ﬁnding marginal distribution speciﬁc node part way along chain note moment observed nodes deﬁnition required marginal obtained summing joint distribution variables except naive implementation would ﬁrst evaluate joint distribution perform summations explicitly joint distribution represented set numbers one possible value variables states values evaluation storage joint distribution well marginalization obtain involve storage computation scale exponentially length chain however obtain much efﬁcient algorithm exploiting con ditional independence properties graphical model substitute factor ized expression joint distribution rearrange order summations multiplications allow required marginal evaluated much efﬁciently consider instance summation potential one depends perform summation ﬁrst give function use perform summation involve new function together potential place appears similarly summation involves potential performed separately give function summation effectively removes variable distribution viewed removal node graph group potentials summations together way express graphical models desired marginal form reader encouraged study ordering carefully underlying idea forms basis later discussion general sum product algorithm key concept exploiting multiplication distributive addition left hand side involves three arithmetic operations whereas right hand side reduces two operations let work computational cost evaluating required marginal using ordered expression perform summations states involves function two variables instance summation involves function table numbers sum table value cost resulting vector numbers multiplied matrix numbers summations multiplications kind total cost evaluating marginal linear length chain contrast exponential cost naive approach therefore able exploit many conditional independence properties simple graph order obtain efﬁcient calculation graph fully connected would conditional independence properties would forced work directly full joint distribution give powerful interpretation calculation terms passing local messages around graph see expression marginal decomposes product two factors times normalization constant shall interpret message passed forwards along chain node node similarly viewed message passed backwards inference graphical models figure marginal distribution node along chain tained multiplying two messages normalizing messages evaluated recursively passing messages ends chain wards node along chain node node note messages comprises set values one choice product two message evaluated recursively therefore ﬁrst evaluate obtained multiplying incoming message local potential similarly message evaluated recursively starting node using operation requires computation graphs form shown figure called markov chains graphical models suppose wish evaluate marginals every node chain simply applying procedure separately node computational cost however approach would wasteful computation instance ﬁnd need prop agate message node back node similarly evaluate need propagate messages node back node involve much duplicated computation messages identical two cases suppose instead ﬁrst launch message starting node propagate corresponding messages way back node suppose similarly launch message starting node propagate corre sponding messages way forward node provided store intermediate messages along way node evaluate marginal simply applying computational cost twice ﬁnding marginal single node rather times much observe message passed direction across link graph note also normalization constant need evaluated using convenient node nodes graph observed corresponding variables simply clamped observed values summation see note effect clamping variable observed value expressed multiplying joint distribution one copies additional function takes value value otherwise one function absorbed potentials contain summations contain one term suppose wish calculate joint distribution two neighbouring nodes chain similar evaluation marginal single node except two variables summed moments thought show required joint distribution written exercise form thus obtain joint distributions sets variables potentials directly completed message passing required obtain marginals useful result practice may wish use parametric forms clique potentials equivalently conditional distributions started directed graph order learn parameters potentials situations variables observed employ algorithm chapter turns local joint distributions cliques conditioned observed data precisely needed step shall consider examples detail chapter 
"
143,"[graphical, models, inference, graphical, models, trees]"," seen exact inference graph comprising chain nodes performed efﬁciently time linear number nodes using algorithm inference graphical models figure examples tree structured graphs showing undirected tree directed tree directed polytree interpreted terms messages passed along chain generally inference performed efﬁciently using local message passing broader class graphs called trees particular shall shortly generalize message passing formalism derived chains give sum product algorithm provides efﬁcient framework exact inference tree structured graphs case undirected graph tree deﬁned graph one one path pair nodes graphs therefore loops case directed graphs tree deﬁned single node called root parents nodes one parent convert directed tree undirected graph see moralization step add links nodes one parent consequence corresponding moralized graph undirected tree examples undirected directed trees shown figure note distribution represented directed tree easily converted one represented undirected tree vice versa exercise nodes directed graph one parent still one path ignoring direction arrows two nodes graph called polytree illustrated figure graph one node property parents furthermore corresponding moralized undirected graph loops 
"
144,"[graphical, models, inference, graphical, models, factor, graphs]"," sum product algorithm derive next section applicable undirected directed trees polytrees cast particularly simple general form ﬁrst introduce new graphical construction called factor graph frey kschischnang directed undirected graphs allow global function several variables expressed product factors subsets variables factor graphs make decomposition explicit introducing additional nodesfactors addition nodes representing variables also allow explicit details factorization shall see let write joint distribution set variables form product factors denotes subset variables convenience shall denote graphical models figure example factor graph corresponds factorization individual variables however earlier discussions comprise groups variables vectors matrices factor function corresponding set variables directed graphs whose factorization deﬁned represent special cases factors local conditional distributions similarly undirected graphs given special case factors tential functions maximal cliques normalizing coefﬁcient viewed factor deﬁned empty set variables factor graph node depicted usual circle every variable distribution case directed undirected graphs also additional nodes depicted small squares factor joint distributionfinally undirected links connecting factor node variables nodes factor depends consider example distribution expressed terms factorization expressed factor graph shown figure note two factors deﬁned set variables undirected graph product two factors would simply lumped together clique potential similarly could combined single potential factor graph however keeps factors explicit able convey detailed information underlying factorization figure undirected graph single clique potential factor graph factor representing distribution undirected graph different factor graph representing distribution whose factors satisfy inference graphical models figure directed graph factorization factor graph representing distribution directed graph whose factor satisﬁes different factor graph representing distribution factors factor graphs said bipartite consist two distinct kinds nodes links nodes opposite type general factor graphs therefore always drawn two rows nodes variable nodes top factor nodes bottom links rows shown example figure situations however ways laying graph may intuitive example factor graph derived directed undirected graph shall see given distribution expressed terms undirected graph readily convert factor graph create variable nodes corresponding nodes original undirected graph create additional factor nodes corresponding maximal cliques factors set equal clique potentials note may several different factor graphs correspond undirected graph concepts illustrated figure similarly convert directed graph factor graph simply create variable nodes factor graph corresponding nodes directed graph create factor nodes corresponding conditional distributions ﬁnally add appropriate links multiple factor graphs correspond directed graph conversion directed graph factor graph illustrated figure already noted importance tree structured graphs performing efﬁcient inference take directed undirected tree convert factor graph result tree words factor graph loops one one path connecting two nodes case directed polytree conversion undirected graph results loops due moralization step whereas conversion factor graph results tree illustrated figure fact local cycles directed graph due links connecting parents node removed conversion factor graph deﬁning appropriate factor function shown figure seen multiple different factor graphs represent rected undirected graph allows factor graphs speciﬁc graphical models figure directed polytree result converting polytree undirected graph showing creation loops result converting polytree factor graph retains tree structure precise form factorization figure shows example fully connected undirected graph along two different factor graphs joint distri bution given general form whereas given speciﬁc factorization emphasized factorization correspond conditional independence properties 
"
145,"[graphical, models, inference, graphical, models, sum-product, algorithm]"," shall make use factor graph framework derive powerful class efﬁcient exact inference algorithms applicable tree structured graphs shall focus problem evaluating local marginals nodes subsets nodes lead sum product algorithm later shall modify technique allow probable state found giving rise max sum algorithm also shall suppose variables model discrete marginalization corresponds performing sums framework however equally applicable linear gaussian models case marginalization involves integration shall consider example detail discuss linear dynamical systems section figure fragment corrected graphical cycle conversion fragment factor graph tree struc ture inference graphical models figure fully connected undirected graph two factor graphs corresponds undirected graph algorithm exact inference directed graphs without loops known belief propagation pearl lauritzen spiegelhalter equiv alent special case sum product algorithm shall consider sum product algorithm simpler derive apply well general shall assume original graph undirected tree directed tree polytree corresponding factor graph tree structure ﬁrst convert original graph factor graph deal directed undirected models using framework goal exploit structure graph achieve two things obtain efﬁcient exact inference algorithm ﬁnding marginals situations several marginals required allow computations shared efﬁciently begin considering problem ﬁnding marginal partic ular variable node moment shall suppose variables hidden later shall see modify algorithm incorporate evidence corresponding observed variables deﬁnition marginal obtained summing joint distribution variables except denotes set variables variable omitted idea substitute using factor graph expression interchange summations products order obtain efﬁcient algorithm consider fragment graph shown figure see tree structure graph allows partition factors joint distribution groups one group associated factor nodes neighbour variable node see joint distribution written product form denotes set factor nodes neighbours denotes set variables subtree connected variable node via factor node graphical models figure represents product factors group associated factor substituting interchanging sums products tain introduced set functions deﬁned viewed messages factor nodes variable node order evaluate messages turn figure note factor described factor sub graphfactorized particular write convenience denoted variables associated factor addition factorization illustrated figure note set variables set variables factor depends also denoted using notation substituting obtain inference graphical models figure illustration factorization subgraph sociated factor node denotes set variable nodes neighbours factor node denotes set node removed deﬁned following messages variable nodes factor nodes therefore introduced two distinct kinds message factor nodes variable nodes denoted variable nodes factor nodes denoted case see messages passed along link always function variable associated variable node link connects result says evaluate message sent factor node variable node along link connecting take product incoming messages along links coming factor node multiply factor associated node marginalize variables associated incoming messages illustrated figure important note factor node send message variable node received incoming messages neighbouring variable nodes finally derive expression evaluating messages variable nodes factor nodes making use sub graph factorization figure see term associated node given product terms associated one factor nodes linked node excluding node product taken neighbours node except node note factors represents subtree original graph precisely kind introduced substituting graphical models figure illustration evaluation message sent variable node adjacent factor node obtain used deﬁnition messages passed factor nodes variable nodes thus evaluate message sent variable node adjacent factor node along connecting link simply take product incoming messages along links note variable node two neighbours performs computation simply passes messages changed also note variable node send message factor node received incoming messages neighbouring factor nodes recall goal calculate marginal variable node marginal given product incoming messages along links arriving node messages computed recursively terms messages order start recursion view node root tree begin leaf nodes deﬁnition see leaf node variable node message sends along one link given illustrated figure similarly leaf node factor node see message sent take form figure sum product algorithm begins messages sent leaf nodes pend whether leaf node variable node factor node inference graphical models illustrated figure point worth pausing summarize particular version sum product algorithm obtained far evaluating marginal start viewing variable node root factor graph initiating messages leaves graph using message passing steps applied recursively messages propagated along every link root node received messages neighbours node send message towards root received messages neighbours root node received messages neighbours required marginal evaluated using shall illustrate process shortly see node always receive enough messages able send message use simple inductive argument follows clearly graph comprising variable root node connected directly several factor leaf nodes algorithm trivially involves sending messages form directly leaves root imagine building general graph adding nodes one time suppose particular graph valid algorithm one variable factor node added connected single link overall graph must remain tree new node leaf node therefore sends message node linked turn therefore receive messages requires order send message towards root valid algorithm thereby completing proof suppose wish ﬁnd marginals every variable node graph could done simply running algorithm afresh node however would wasteful many required computations would repeated obtain much efﬁcient procedure overlaying multiple message passing algorithms obtain general sum product algorithm follows arbitrarily pick variable factor node designate root propagate messages leaves root point root node received messages neighbours therefore send messages neighbours turn received messages neighbours send messages along links going away root way messages passed outwards root way leaves message passed directions across every link graph every node received message neighbours simple inductive argument used verify validity message passing protocol every variable exercise node received messages neighbours readily calculate marginal distribution every variable graph number messages computed given twice number links graph involves twice computation involved ﬁnding single marginal comparison run sum product algorithm separately node amount computation would grow quadratically size graph note algorithm fact independent node designated root graphical models figure marginalizing variables next suppose wish ﬁnd marginal distributions associated exercise product messages arriving factor node local factor node message sent variable node factor node seen simply far rather neglected issue normalization factor graph joint distribution ﬁrst run sum product algorithm ﬁnd corresponding unnormalized marginals directly point may helpful consider simple example illustrate operation sum product algorithm figure shows simple node factor inference graphical models figure simple factor graph used illustrate sum product algorithm graph whose unnormalized joint distribution given order apply sum product algorithm graph let designate node root case two leaf nodes starting leaf nodes following sequence six messages direction ﬂow messages illustrated figure message propagation complete propagate messages root node leaf nodes given graphical models figure flow messages sum product algorithm applied example graph figure leaf nodes towards root node root node towards leaf nodes one message passed direction across link evaluate marginals simple check let verify marginal given correct expression using substituting messages using results required far assumed variables graph hidden practical applications subset variables observed wish calculate posterior distributions conditioned observations observed nodes easily handled within sum product algorithm follows suppose partition hidden variables observed variables observed value denoted simply multiply joint distribution otherwise product corresponds hence unnormalized version running sum product algorithm efﬁciently calculate posterior marginals normalization coefﬁcient whose value found efﬁciently using local computation summations variables collapse single term assumed throughout section dealing discrete variables however nothing speciﬁc discrete variables either graphical framework probabilistic construction sum product algorithm inference graphical models table example joint distribution two binary variables maximum joint distribution occurs dif ferent variable values compared maxima two marginals continuous variables summations simply replaced integrations shall give example sum product algorithm applied graph linear gaussian variables consider linear dynamical systems section 
"
146,"[graphical, models, inference, graphical, models, max-sum, algorithm]"," sum product algorithm allows take joint distribution expressed factor graph efﬁciently ﬁnd marginals component variables two common tasks ﬁnd setting variables largest prob ability ﬁnd value probability addressed closely related algorithm called max sum viewed application dynamic programming context graphical models cormen simple approach ﬁnding latent variable values high probability would run sum product algorithm obtain marginals ery variable marginal turn ﬁnd value maximizes marginal however would give set values individually probable practice typically wish ﬁnd set values jointly largest probability words vector max maximizes joint distribution max arg max corresponding value joint probability given max max general max set values easily show using simple example consider joint distribution two binary variables given table joint distribution maximized setting corresponding value however marginal obtained summing values given similarly marginal given marginals maximized corresponds value joint distribution fact difﬁcult construct examples set individually probable values probability zero joint distribution exercise therefore seek efﬁcient algorithm ﬁnding value maximizes joint distribution allow obtain value joint distribution maximum address second problems shall simply write max operator terms components max max max graphical models total number variables substitute using expansion terms product factors deriving sum product algorithm made use distributive law multiplication make use analogous law max operator max max holds always case factors graphical model allows exchange products maximizations consider ﬁrst simple example chain nodes described evaluation probability maximum written max max max max max calculation marginals see exchanging max product operators results much efﬁcient computation one easily inter preted terms messages passed node backwards along chain node readily generalize result arbitrary tree structured factor graphs substituting expression factor graph expansion exchanging maximizations products structure calculation identical sum product algorithm simply translate results present context particular suppose designate particular variable node root graph start set messages propagating inwards leaves tree towards root node sending message towards root received incoming messages neighbours ﬁnal maximization performed product messages arriving root node gives maximum value could called max product algorithm identical sum product algorithm except summations replaced maximizations note stage messages sent leaves root direction practice products many small probabilities lead numerical ﬂow problems convenient work logarithm joint distri bution logarithm monotonic function hence max operator logarithm function interchanged max max distributive property preserved max max thus taking logarithm simply effect replacing products max product algorithm sums obtain max sum algorithm inference graphical models results derived earlier sum product algorithm readily write max sum algorithm terms message passing simply replacing sum max replacing products sums logarithms give max initial messages sent leaf nodes obtained analogy given root node maximum probability computed analogy using max max far seen ﬁnd maximum joint distribution prop agating messages leaves arbitrarily chosen root node result irrespective node chosen root turn second problem ﬁnding conﬁguration variables joint distributionattains maximum value far sent messages leaves root process evaluating also give value max probable value root node variable deﬁned max arg max point might tempted simply continue message passing gorithm send messages root back leaves using apply remaining variable nodes however maximizing rather summing possible may mul tiple conﬁgurations give rise maximum value cases strategy fail possible individual variable values obtained maximizing product messages node belong different maximizing conﬁgurations giving overall conﬁguration longer corresponds maximum problem resolved adopting rather different kind message passing root node leaves see works let return simple chain example variables states graphical models figure lattice trellis diagram showing explicitly possible states one per row diagram variables chain model illustration variable corresponding column dia gram function deﬁnes unique state corresponding graph shown figure suppose take node root node ﬁrst phase propagate messages leaf node root node using max probable value given max arg max arg max given indicated inference graphical models lines connecting nodes know probable value nal node simply follow link back ﬁnd probable state node back initial node corresponds propagating message back chain using max max known back tracking note could several values give maximum value provided chose one values back tracking assured globally consistent maximizing conﬁguration figure indicated two paths shall suppose corresponds global maximum joint probability distribution represent possible values max starting either state tracing back along black lines corresponds iterating obtain valid global maximum conﬁguration note run forward pass max sum message passing followed backward pass applied node separately could end selecting states one path path giving overall conﬁguration global maximizer see necessary instead keep track maximizing states forward pass using functions use back tracking ﬁnd consistent solution extension general tree structured factor graph clear message sent factor node variable node maximization performed variable nodes neighboursfactor node using perform maximization keep record values variables gave rise maximum back tracking step found max use stored values sign consistent maximizing states max max max sum algorithm back tracking gives exact maximizing conﬁguration variables provided factor graph tree important application technique ﬁnding probable sequence hidden states hidden markov model case known viterbi algorithm section sum product algorithm inclusion evidence form observed variables straightforward observed variables clamped observed values maximization performed remaining hidden variables shown formally including identity functions observed variables factor functions sum product algorithm interesting compare max sum iterated conditional modes icm algorithm described page step icm computationally simpler cause messages passed one node next comprise single value consisting new state node conditional distribution maximized max sum algorithm complex messages functions node variables hence comprise set values pos sible state unlike max sum however icm guaranteed ﬁnd global maximum even tree structured graphs graphical models 
"
147,"[graphical, models, inference, graphical, models, exact, inference, general, graphs]"," sum product max sum algorithms provide efﬁcient exact solutions inference problems tree structured graphs many practical applications however deal graphs loops message passing framework generalized arbitrary graph topolo gies giving exact inference procedure known junction tree algorithm lau ritzen spiegelhalter jordan give brief outline key steps involved intended convey detailed understanding algorithm rather give ﬂavour various stages involved starting point directed graph ﬁrst converted undirected graph moralization whereas starting undirected graph step required next graph triangulated involves ﬁnding chord less cycles containing four nodes adding extra links eliminate chord less cycles instance graph figure cycle chord less link could added alternatively note joint distributionresulting triangulated graph still deﬁned product potential functions considered functions expanded sets variables next triangulated graph used construct new tree structured undirected graph called join tree whose nodes correspond maximal cliques triangulated graph whose links connect pairs cliques variables common selection pairs cliques connect way important done give maximal spanning tree deﬁned follows possible trees link cliques one chosen one weight tree largest weight link number nodes shared two cliques connects weight tree sum weights links tree condensed clique subset another clique absorbed larger clique gives junction tree consequence triangulation step resulting tree satisﬁes running intersection property means variable contained two cliques must also con tained every clique path connects ensures inference variables consistent across graph finally two stage message passing algorithm essentially equivalent sum product algorithm applied junction tree order ﬁnd marginals conditionals although junction tree algorithm sounds complicated heart simple idea used already exploiting factorization properties distribution allow sums products interchanged partial summations per formed thereby avoiding work directly joint distribution role junction tree provide precise efﬁcient way organize computations worth emphasizing achieved using purely graphical operations junction tree exact arbitrary graphs efﬁcient sense given graph general exist computationally cheaper approach unfortunately algorithm must work joint distributions within node corresponds clique triangulated graph compu tational cost algorithm determined number variables largest inference graphical models clique grow exponentially number case discrete variables important concept treewidth graph bodlaender ﬁned terms number variables largest clique fact deﬁned one less size largest clique ensure tree treewidth general multiple different junction trees constructed given starting graph treewidth deﬁned junction tree largest clique fewest variables treewidth original graph high junction tree algorithm becomes impractical 
"
148,"[graphical, models, inference, graphical, models, loopy, belief, propagation]"," many problems practical interest feasible use exact ference need exploit effective approximation methods important class approximations broadly called variational methods discussed detail chapter complementing deterministic approaches wide range sampling methods also called monte carlo methods based stochastic numerical sampling distributions discussed length chapter consider one simple approach approximate inference graphs loops builds directly previous discussion exact inference trees idea simply apply sum product algorithm even though guarantee yield good results approach known loopy belief propagation frey mackay possible message passing rules sum product algorithm purely local however graph cycles information ﬂow many times around graph models algorithm converge whereas others order apply approach need deﬁne message passing schedule let assume one message passed time given link given direction message sent node replaces previous message sent direction across link function recent messages received node previous steps algorithm seen message sent across link node messages received node across links loops graph raises problem initiate message passing algorithm resolve suppose initial message given unit function passed across every link direction every node position send message many possible ways organize message passing schedule example ﬂooding schedule simultaneously passes message across every link directions time step whereas schedules pass one message time called serial schedules following kschischnang say variable factor node message pending link node node received message links since last time send message thus node receives message one links creates pending messages links pending messages need transmitted graphical models messages would simply duplicate previous message link graphs tree structure schedule sends pending messages eventually terminate message passed direction across every link point pending messages product received exercise messages every variable give exact marginal graphs loops however algorithm may never terminate might always pending messages although practice generally found converge within reasonable time applications algorithm converged stopped convergence observed approximate local marginals computed using product recently received incoming messages variable node factor node every link applications loopy belief propagation algorithm give poor sults whereas applications proven effective particular state art algorithms decoding certain kinds error correcting codes equivalent loopy belief propagation gallager berrou mceliece mackay neal frey 
"
149,"[graphical, models, inference, graphical, models, learning, graph, structure]"," discussion inference graphical models assumed structure graph known ﬁxed however also interesting beyond inference problem learning graph structure data friedman koller requires deﬁne space possible struc tures well measure used score structure bayesian viewpoint would ideally like compute posterior distributiongraph structures make predictions averaging respect distribution prior graphs indexed posterior distribution given observed data set model evidence provides score model however evaluation evidence involves marginalization latent variables presents challenging computational problem many models exploring space structures also problematic number different graph structures grows exponentially number nodes often necessary resort heuristics ﬁnd good candidates 
"
150,"[graphical, models, exercises]"," marginalizing variables order show representation joint distribution directed graph correctly normalized provided conditional distributions normalized show property directed cycles directed graph follows statement exists ordered numbering nodes node links going lower numbered node exercises table joint distribution three binary variables consider three binary variables joint distribution given table show direct evaluation distribution property marginally dependent become independent conditioned evaluate distributions corresponding joint distribution given table hence show direct evaluation draw corresponding directed graph draw directed probabilistic graphical model corresponding relevance vector machine described model shown figure seen number parameters required specify conditional distribution could reduced making use logistic sigmoid represen tation alternative representation pearl given parameters represent probabilities additional parameters satisfying conditional distribution known noisy show interpreted soft probabilistic form logical function function gives whenever least one discuss interpretation using recursion relations show mean covariance joint distribution graph shown figure given respectively show implies using separation criterion show conditional distribution node directed graph conditioned nodes markov blanket independent remaining variables graph graphical models figure example graphical model used explore con ditional independence properties head head path descendant namely node observed consider directed graph shown figure none variables observed show suppose observe variable show general consider example car fuel system shown figure suppose instead observing state fuel gauge directly gauge seen driver reports reading gauge report either gauge shows full shows empty driver bit unreliable expressed following probabilities suppose driver tells fuel gauge shows empty words observe evaluate probability tank empty given observation similarly evaluate corresponding probability given also observation battery ﬂat note second probability lower discuss intuition behind result relate result figure show distinct undirected graphs set distinct random variables draw possibilities case consider use iterated conditional modes icm minimize energy function given write expression difference values energy associated two states particular variable variables held ﬁxed show depends quantities local graph consider particular case energy function given coefﬁcients show probable conﬁguration latent variables given show joint distribution two neighbouring nodes graph shown figure given expression form exercises consider inference problem evaluating graph shown figure nodes show message passing algorithm discussed section used solve efﬁciently discuss messages modiﬁed way consider graph form shown figure nodes nodes observed use separation show show message passing algorithm section applied evaluation result independent value show distribution represented directed tree trivially written equivalent distribution corresponding undirected tree also show distribution expressed undirected tree suitable normalization clique potentials written directed tree calculate number distinct directed trees constructed given undirected tree apply sum product algorithm derived section chain nodes model discussed section show results recovered special case consider message passing protocol sum product algorithm tree structured factor graph messages ﬁrst propagated leaves arbitrarily chosen root node root node leaves use proof induction show messages passed order every step node must send message received incoming messages necessary construct outgoing messages show marginal distributions sets variables associated factors factor graph found ﬁrst running sum product message passing algorithm evaluating required marginals using consider tree structured factor graph given subset variable nodes form connected subgraph variable node subset connected least one variable nodes via single factor node show sum product algorithm used compute marginal distribution subset section showed marginal distribution variable node factor graph given product messages arriving node neighbouring factor nodes form show marginal also written product incoming message along one links outgoing message along link show marginal distribution variables factor tree structured factor graph running sum product message passing algo rithm written product message arriving factor node along links times local factor form graphical models veriﬁed sum product algorithm run graph figure node designated root node gives correct marginal show correct marginals obtained also similarly show use result running sum product algorithm graph gives correct joint distribution consider tree structured factor graph discrete variables suppose wish evaluate joint distribution associated two variables belong common factor deﬁne procedure using sum product algorithm evaluate joint distribution one variables successively clamped allowed values consider two discrete variables three possible states example construct joint distribution variables property value maximizes marginal along value maximizes marginal together probability zero joint distribution concept pending message sum product algorithm factor graph deﬁned section show graph one cycles always least one pending message irrespective long algorithm runs show sum product algorithm run factor graph tree structure loops ﬁnite number messages sent pending messages 
"
151,"[mixture, models]"," deﬁne joint distribution observed latent variables corresponding distribution observed variables alone obtained marginalization allows relatively complex marginal distributions observed variables pressed terms tractable joint distributions expanded space observed latent variables introduction latent variables thereby allows complicated distributions formed simpler components chapter shall see mixture distributions gaussian mixture discussed section interpreted terms discrete latent variables continuous latent variables form subject chapter well providing framework building complex probability distributions mixture models also used cluster data therefore begin discussion mixture distributions considering problem ﬁnding clusters set data points approach ﬁrst using nonprobabilistic technique called means algorithm lloyd introduce latent variable section mixture models view mixture distributions discrete latent variables interpreted deﬁning assignments data points speciﬁc components mixture gen section general technique ﬁnding maximum likelihood estimators latent variable models expectation maximization algorithm ﬁrst use gaussian mixture distribution motivate algorithm fairly informal way give careful treatment based latent variable viewpoint shall section see means algorithm corresponds particular nonprobabilistic limit applied mixtures gaussians finally discuss generality section gaussian mixture models widely used data mining pattern recognition machine learning statistical analysis many applications parameters determined maximum likelihood typically using algorithm however shall see signiﬁcant limitations maximum likelihood approach chapter shall show elegant bayesian treatment given using framework variational inference requires little additional computation compared resolves principal difﬁculties maxi mum likelihood also allowing number components mixture inferred automatically data 
"
152,"[mixture, models, -means, clustering]"," begin considering problem identifying groups clusters data points multidimensional space suppose data set consisting observations random dimensional euclidean variable goal partition data set number clusters shall suppose moment value given intuitively might think cluster comprising group data points whose inter point distances small compared distances points outside cluster formalize notion ﬁrst introducing set dimensional vectors prototype associated cluster shall see shortly think representing centres clusters goal ﬁnd assignment data points clusters well set vectors sum squares distances data point closest vector minimum convenient point deﬁne notation describe assignment data points clusters data point introduce corresponding set binary indicator variables describing clusters data point assigned data point assigned cluster known coding scheme deﬁne objective function sometimes called distortion measure given represents sum squares distances data point means clustering assigned vector goal ﬁnd values minimize iterative procedure iteration involves two successive steps corresponding successive optimizations respect first choose initial values ﬁrst phase minimize respect keeping ﬁxed second phase minimize respect keeping ﬁxed two stage optimization repeated convergence shall see two stages updating updating correspond respectively expectation maximization steps algorithm emphasize shall use section terms step step context means algorithm consider ﬁrst determination linear function optimization performed easily give closed form solution terms involving different independent optimize separately choosing whichever value gives minimum value words simply assign data point closest cluster centre formally expressed arg min otherwise consider optimization held ﬁxed objective function quadratic function minimized setting derivative respect zero giving easily solve give denominator expression equal number points assigned cluster result simple interpretation namely set equal mean data points assigned cluster reason procedure known means algorithm two phases assigning data points clusters computing clustermeans repeated turn change assignments maximum number iterations exceeded phase reduces value objective function convergence algorithm assured exercise ever may converge local rather global minimum convergence properties means algorithm studied macqueen means algorithm illustrated using old faithful data set fig appendix ure purposes example made linear scaling data known standardizing variables zero mean unit standard deviation example chosen mixture models figure illustration means algorithm using scaled old faithful data set green points denote data set two dimensional euclidean space initial choices centres shown red blue crosses respectively initial step data point assigned either red cluster blue cluster according cluster centre nearer equivalent classifying points according side perpendicular bisector two cluster centres shown magenta line lie subsequent step cluster centre computed mean points assigned corresponding cluster show successive steps ﬁnal convergence algorithm means clustering figure plot cost function given step blue points step red points means algorithm example shown figure algo rithm converged third step ﬁnal cycle pro duces changes either signments prototype vectors case assignment data point nearest cluster centre equivalent classiﬁcation data points according side lie perpendicular bisector two cluster centres plot cost function given old faithful example shown figure note deliberately chosen poor initial values cluster centres algorithm takes several steps convergence practice better initialization procedure would choose cluster centres equal random subset data points also worth noting means algorithm often used initialize parameters gaussian mixture model applying algorithm section direct implementation means algorithm discussed relatively slow step necessary compute euclidean dis tance every prototype vector every data point various schemes proposed speeding means algorithm based precomputing data structure tree nearby points subtree ramasubramanian paliwal moore approaches make use triangle inequality distances thereby avoiding unnecessary dis tance calculations hodgson elkan far considered batch version means whole data set used together update prototype vectors also derive line stochastic algorithm macqueen applying robbins monro procedure section problem ﬁnding roots regression function given derivatives respect leads sequential update exercise data point turn update nearest prototype using new old old learning rate parameter typically made decrease mono tonically data points considered means algorithm based use squared euclidean distance measure dissimilarity data point prototype vector limit type data variables considered would inappropriate cases variables represent categorical labels instance mixture models also make determination cluster means nonrobust outliers section generalize means algorithm introducing general dissimilarity measure two vectors minimizing following distortion measure gives medoids algorithm step involves given cluster prototypes assigning data point cluster dissimilarity corresponding prototype smallest computational cost case standard means algorithm general choice dissimi larity measure step potentially complex means common restrict cluster prototype equal one data vectors signed cluster allows algorithm implemented choice dissimilarity measure long readily evaluated thus step involves cluster discrete search points assigned cluster requires evaluations one notable feature means algorithm iteration every data point assigned uniquely one one clusters whereas data points much closer particular centre centre may data points lie roughly midway cluster centres latter case clear hard assignment nearest cluster appropriate shall see next section adopting probabilistic approach obtain soft assignments data points clusters way reﬂects level uncertainty appropriate assignment probabilistic formulation brings numerous beneﬁts 
"
153,"[mixture, models, -means, clustering, image, segmentation, compression]"," illustration application means algorithm consider related problems image segmentation image compression goal segmentation partition image regions reasonably homogeneous visual appearance corresponds objects parts objects forsyth ponce pixel image point dimensional space comprising intensities red blue green channels segmentation algorithm simply treats pixel image separate data point note strictly space euclidean channel intensities bounded interval nevertheless apply means algorithm without difﬁ culty illustrate result running means convergence particular value drawing image replacing pixel vector intensity triplet given centre pixel assigned results various values shown figure see given value algorithm representing image using palette colours emphasized use means particularly sophisticated approach image segmentation least takes account spatial proximity different pixels image segmentation problem general extremely difﬁcult means clustering original image figure two examples application means clustering algorithm image segmentation showing initial images together means segmentations obtained using various values also illustrates use vector quantization data compression smaller values give higher compression expense poorer image quality remains subject active research introduced simply illustrate behaviour means algorithm also use result clustering algorithm perform data compression important distinguish lossless data compression goal able reconstruct original data exactly compressed representation lossy data compression accept errors reconstruction return higher levels compression achieved lossless case apply means algorithm problem lossy data compression follows data points store identity cluster assigned also store values clustercentres typically requires signiﬁcantly less data provided choose new data points similarly compressed ﬁrst ﬁnding nearest storing label instead original data vector framework often called vector quantization vectors called code book vectors mixture models image segmentation problem discussed also provides illustration use clustering data compression suppose original image pixels comprising values stored bits precision transmit whole image directly would cost bits suppose ﬁrst run means image data instead transmitting original pixel intensity vectors transmit identity nearest vector vectors requires log bits per pixel must also transmit code book vectors requires bits total number bits required transmit image log rounding nearest integer original image shown figure pixels requires bits transmit directly comparison compressed images require bits bits bits respectively transmit represent compression ratios compared original image respectively see trade degree compression image quality note aim example illustrate means algorithm aiming produce good image compressor would fruitful consider small blocks adjacent pixels instance thereby exploit correlations exist natural images nearby pixels 
"
154,"[mixture, models, mixtures, gaussians]"," section motivated gaussian mixture model simple linear super position gaussian components aimed providing richer class density models single gaussian turn formulation gaussian mixtures terms discrete latent variables provide deeper insight important distribution also serve motivate expectation maximization algorithm recall gaussian mixture distribution written linear superposition gaussians form let introduce dimensional binary random variable repre sentation particular element equal elements equal values therefore satisfy see possible states vector according element nonzero shall deﬁne joint distribution terms marginal distributionconditional distribution corresponding graphical model figure marginal distribution speciﬁed terms mixing coefﬁcients mixtures gaussians figure graphical representation mixture model joint distribution expressed form parameters must satisfy together similarly conditional distribution given particular value gaussian also written form joint distribution given marginal distribution obtained summing joint distribution possible states give exercise represented marginal distribution form follows every observed data point corresponding latent variable therefore found equivalent formulation gaussian mixture mixture models instead marginal distribution lead signiﬁcant simpliﬁcations notably introduction expectation maximization algorithm another quantity play important role conditional probability given shall use denote whose value found using bayes theorem shall view prior probability quantity corresponding posterior probability observed shall see later also viewed responsibility component takes explaining observation use technique ancestral sampling generate random samples section distributed according gaussian mixture model ﬁrst generate value denote marginal distribution generate value conditional distribution techniques sampling standard distributions discussed chapter depict samples joint distribution plotting points corresponding values colouring according value words according gaussian component responsible generating shown figure similarly samples marginal distribution obtained taking samples joint distribution ignoring values illustrated figure plotting values without coloured labels also use synthetic data set illustrate responsibilities eval uating every data point posterior probability component mixture distribution data set generated particular represent value responsibilities associated data point plotting corresponding point using proportions red blue green ink given respectively shown figure instance data point coloured red whereas one coloured equal proportions blue green ink appear cyan compared figure data points labelled using true identity component generated 
"
155,"[mixture, models, mixtures, gaussians, maximum, likelihood]"," suppose data set observations wish model data using mixture gaussians represent data set mixtures gaussians figure example points drawn mixture gaussians shown figure samples joint distribution three states corresponding three components mixture depicted red green blue corresponding samples marginal distribution complete whereas incomplete samples colours represent value responsibilities associated data point obtained plotting corresponding point using proportions red blue green ink given respectively matrix row given similarly corresponding latent variables denoted matrix rows assume discussing maximize function worth emphasizing unit matrix although conclusions hold component mean exactly equal one data figure data points corresponding latent points mixture models figure illustration singularities likelihood function arise mixtures gaussians compared case single gaussian shown figure singularities arise points value data point contribute term likelihood function form consider limit see term goes inﬁnity log likelihood function also inﬁnity thus maximization log likelihood function well posed problem singularities always present occur whenever one gaussian components collapses onto speciﬁc data point recall problem arise case single gaussian distribution understand difference note single gaussian collapses onto data point contribute multiplicative factors likelihood function arising data points factors zero exponentially fast giving overall likelihood goes zero rather inﬁnity however least two components mixture one components ﬁnite variance therefore assign ﬁnite probability data points component shrink onto one speciﬁc data point thereby contribute ever increasing additive value log likelihood illustrated figure singularities provide another example severe ﬁtting occur maximum likelihood approach shall see difﬁculty occur adopt bayesian approach moment section however simply note applying maximum likelihood gaussian mixture models must take steps avoid ﬁnding pathological solutions instead seek local maxima likelihood function well behaved hope avoid singularities using suitable heuristics instance detecting gaussian component collapsing resetting mean randomly chosen value also resetting covariance large value continuing optimization issue ﬁnding maximum likelihood solutions arises fact given maximum likelihood solution component mixture total equivalent solutions corresponding ways assigning sets parameters components words given nondegenerate point space parameter values additional points give rise exactly distribution problem known mixtures gaussians maximizing log likelihood function gaussian mixture model one approach apply gradient based optimization techniques fletcher 
"
156,"[mixture, models, mixtures, gaussians, gaussian, mixtures]"," elegant powerful method ﬁnding maximum likelihood solutions section let begin writing conditions must satisﬁed maximum likelihood function setting derivatives respect means gaussian components zero obtain assume nonsingular rearranging obtain deﬁned mixture models interpret effective number points assigned cluster note carefully form solution see mean gaussian component obtained taking weighted mean points data set weighting factor data point given posterior probability component responsible generating set derivative respect zero follow similar line reasoning making use result maximum likelihood solution covariance matrix single gaussian obtain section form corresponding result single gaussian ﬁtted data set data point weighted corresponding poste rior probability denominator given effective number points associated corresponding component finally maximize respect mixing coefﬁcients must take account constraint requires mixing coefﬁcients sum one achieved using lagrange multiplier appendix maximizing following quantity gives see appearance responsibilities multiply sides sum making use constraint ﬁnd using eliminate rearranging obtain mixing coefﬁcient component given average respon sibility component takes explaining data points worth emphasizing results con stitute closed form solution parameters mixture model responsibilities depend parameters complex way however results suggest simple iterative scheme ﬁnding solution maximum likelihood problem shall see turns instance algorithm particular case gaussian mixture model ﬁrst choose initial values means covariances mixing coefﬁcients alternate following two updates shall call step mixtures gaussians figure illustration algorithm using old faithful set used illustration means algorithm figure see text details step reasons become apparent shortly expectation step step use current values parameters evaluate posterior probabilities responsibilities given use probabilities maximization step step estimate means covariances mixing coefﬁcients using results note ﬁrst evaluate new means using use new values ﬁnd covariances using keeping corresponding result single gaussian distribution shall show update parameters resulting step followed step guaranteed increase log likelihood function practice algorithm deemed converged change section log likelihood function alternatively parameters falls threshold illustrate algorithm mixture two gaussians applied rescaled old faithful data set figure mixture two gaussians used centres initialized using values means algorithm figure precision matrices initialized proportional unit matrix plot shows data points green together initial conﬁguration mixture model one standard deviation contours two mixture models gaussian components shown blue red circles plot shows result initial step data point depicted using proportion blue ink equal posterior probability generated blue component corresponding proportion red ink given posterior probability generated red component thus points signiﬁcant probability belonging either cluster appear purple situation ﬁrst step shown plot mean blue gaussian moved mean data set weighted probabilities data point belonging blue cluster words moved centre mass blue ink similarly covariance blue gaussian set equal covariance blue ink analogous results hold red component plots show results complete cycles respectively plot algorithm close convergence note algorithm takes many iterations reach approximate convergence compared means algorithm cycle requires signiﬁcantly computation therefore common run means algo rithm order ﬁnd suitable initialization gaussian mixture model subsequently adapted using covariance matrices conveniently initialized sample covariances clusters found means algorithm mixing coefﬁcients set fractions data points assigned respective clusters gradient based approaches maximizing log like lihood techniques must employed avoid singularities likelihood function gaussian component collapses onto particular data point emphasized generally multiple local maxima log likelihood function guaranteed ﬁnd largest maxima algorithm gaussian mixtures plays important role summarize gaussian mixtures given gaussian mixture model goal maximize likelihood function respect parameters comprising means covariances components mixing coefﬁcients initialize means covariances mixing coefﬁcients evaluate initial value log likelihood step evaluate responsibilities using current parameter values alternative view step estimate parameters using current responsibilities new new new new new evaluate log likelihood check convergence either parameters log likelihood convergence criterion satisﬁed return step 
"
157,"[mixture, models, alternative, view]"," section present complementary view algorithm recog nizes key role played latent variables discuss approach ﬁrst abstract setting illustration consider case gaussian mixtures goal algorithm ﬁnd maximum likelihood solutions models latent variables denote set observed data row represents similarly denote set latent variables corresponding row set model parameters denoted log likelihood function given note discussion apply equally well continuous latent variables simply replacing sum integral key observation summation latent variables appears inside logarithm even joint distribution belongs exponential mixture models family marginal distribution typically result summation presence sum prevents logarithm acting directly joint distribution resulting complicated expressions maximum likelihood solution suppose observation told corresponding value latent variable shall call complete data set shall refer actual observed data incomplete illustrated figure likelihood function complete data set simply takes form shall suppose maximization complete data log likelihood function straightforward practice however given complete data set incomplete data state knowledge values latent variables given posterior distribution cannot use complete data log likelihood consider instead expected value posterior distribution latent variable corresponds shall see step algorithm subsequent step maximize expectation current estimate parameters denoted old pair successive steps gives rise revised estimate new algorithm initialized choosing starting value parameters use expectation may seem somewhat arbitrary however shall see motivation choice give deeper treatment section step use current parameter values old ﬁnd posterior distribution latent variables given old use posterior distribution ﬁnd expectation complete data log likelihood evaluated general parameter value expectation denoted old given old old step determine revised parameter estimate new maximizing function new arg max old note deﬁnition old logarithm acts directly joint distribution corresponding step maximization supposition tractable general algorithm summarized property shall show later cycle increase incomplete data log likelihood unless already local maximum section general algorithm given joint distribution observed variables latent variables governed parameters goal maximize likelihood function respect choose initial setting parameters old alternative view step evaluate old step evaluate new given new arg max old old old check convergence either log likelihood parameter values convergence criterion satisﬁed let old new return step algorithm also used ﬁnd map maximum posterior solutions models prior deﬁned parameters case exercise step remains maximum likelihood case whereas step quantity maximized given old suitable choices prior remove singularities kind illustrated figure considered use algorithm maximize likelihood function discrete latent variables however also applied unobserved variables correspond missing values data set distribution observed values obtained taking joint distribution variables marginalizing missing ones used maximize corresponding likelihood function shall show example application technique context principal component analysis figure valid procedure data values missing random meaning mechanism causing values missing depend unobserved values many situations case instance sensor fails return value whenever quantity measuring exceeds threshold 
"
158,"[mixture, models, alternative, view, gaussian, mixtures, revisited]"," consider application latent variable view speciﬁc case gaussian mixture model recall goal maximize log likelihood function computed using observed data set saw difﬁcult case single gaussian distribution due presence summation occurs inside logarithm suppose addition observed data set also given values corresponding discrete variables recall figure shows complete data set one includes labels showing component generated data point figure shows corresponding incomplete data set graphical model complete data shown figure mixture models figure served well data variables consider problem maximizing likelihood complete data set likelihood function takes form denotes component taking logarithm obtain dimensional vector thus see complete data log likelihood function maximized alternative view using together bayes theorem see posterior distribution takes form hence factorizes posterior distribution independent easily veriﬁed inspection directed graph figure exercise making use separation criterion expected value indicator section variable posterior distribution given responsibility component data point expected value complete data log likelihood function therefore given proceed follows first choose initial values parameters old old old use evaluate responsibilities step keep responsibilities ﬁxed maximize respect step leads closed form solutions new new new given precisely algorithm exercise gaussian mixtures derived earlier shall gain insight role expected complete data log likelihood function give proof convergence algorithm section 
"
159,"[mixture, models, alternative, view, relation, -means]"," comparison means algorithm algorithm gaussian mixtures shows close similarity whereas means algorithm performs hard assignment data points clusters data point associated uniquely one cluster algorithm makes soft assignment based posterior probabilities fact derive means algorithm particular limit gaussian mixtures follows consider gaussian mixture model covariance matrices mixture components given variance parameter shared mixture models components identity matrix exp consider algorithm mixture gaussians form treat ﬁxed constant instead parameter estimated posterior probabilities responsibilities particular data point given exponent consider limit see denominator term smallest zero slowly hence responsibilities data point zero except term responsibility unity note holds independently values long none zero thus limit obtain hard assignment data points clusters means algorithm deﬁned data point thereby assigned cluster closest mean estimation equation given reduces means result note estimation formula mixing coefﬁcients simply sets value equal fraction data points assigned cluster although parameters longer play active role algorithm finally limit expected complete data log likelihood given becomes exercise const thus see limit maximizing expected complete data log likelihood equivalent minimizing distortion measure means algorithm given note means algorithm estimate covariances clus ters cluster means hard assignment version gaussian mixture model general covariance matrices known elliptical means algorithm considered sung poggio 
"
160,"[mixture, models, alternative, view, mixtures, bernoulli, distributions]"," far chapter focussed distributions continuous variables described mixtures gaussians example mixture modelling illustrate algorithm different context discuss mix tures discrete binary variables described bernoulli distributions model also known latent class analysis lazarsfeld henry mclachlan peel well practical importance right discussion bernoulli mixtures also lay foundation consideration hidden markov models discrete variables section alternative view consider set binary variables governed bernoulli distribution parameter see individual variables independent given mean covariance distribution easily seen covariance diag let consider ﬁnite mixture distributions given mean covariance mixture distribution given exercise covariance diag covariance matrix covariance longer diagonal mixture distribution capture correlations variables unlike single bernoulli distribution given data set log likelihood function model given see appearance summation inside logarithm maximum likelihood solution longer closed form derive algorithm maximizing likelihood function mixture bernoulli distributions ﬁrst introduce explicit latent mixture models variable associated instance case gaussian mixture binary dimensional variable single component equal components equal write conditional distribution given latent variable prior distribution latent variables mixture gaussians model form product marginalize recover exercise order derive algorithm ﬁrst write complete data log likelihood function given next take expectation complete data log likelihood respect posterior distribution latent variables give posterior probability responsibility component given data point step responsibilities evaluated using bayes theorem takes form alternative view consider sum see responsibilities enter two terms written effective number data points associated component step maximize expected complete data log likelihood respect parameters set derivative respect equal zero rearrange terms obtain exercise see sets mean component equal weighted mean data weighting coefﬁcients given responsibilities component takes data points maximization respect need introduce lagrange multiplier enforce constraint following analogous steps used mixture gaussians obtain exercise represents intuitively reasonable result mixing coefﬁcient component given effective fraction points data set explained component note contrast mixture gaussians singularities likelihood function goes inﬁnity seen noting likelihood function bounded exist exercise singularities likelihood function goes zero found provided initialized pathological starting point algorithm always increases value likelihood function local maximum found illustrate bernoulli mixture model figure section using model handwritten digits digit images turned binary vectors setting elements whose values exceed setting remaining elements data set digits comprising digits mixture bernoulli distributions running iterations algorithm mixing coefﬁcients initialized parameters set random values chosen uniformly range normalized satisfy constraint see mixture bernoulli distributions able ﬁnd three clusters data set corresponding different digits conjugate prior parameters bernoulli distribution given beta distribution seen beta prior equivalent introducing mixture models figure illustration bernoulli mixture model top row shows examples digits data set converting pixel values grey scale binary using threshold bottom row ﬁrst three images show parameters three components mixture model comparison also data set using single multivariate bernoulli distribution using maximum likelihood amounts simply averaging counts pixel shown right image bottom row additional effective observations similarly introduce priors section bernoulli mixture model use maximize posterior probability distri butions exercise straightforward extend analysis bernoulli mixtures case multinomial binary variables states making use discrete dis exercise tribution introduce dirichlet priors model parameters desired 
"
161,"[mixture, models, alternative, view, bayesian, linear, regression]"," third example application return evidence proximation bayesian linear regression section obtained estimation equations hyperparameters evaluation evidence setting derivatives resulting expression zero turn alternative approach ﬁnding based algorithm recall goal maximize evidence function given respect parameter vector marginalized regard latent variable hence optimize marginal likelihood function using step compute posterior distribution given current setting parameters use ﬁnd expected complete data log likelihood step maximize quantity respect already derived posterior distribution given complete data log likelihood function given alternative view likelihood prior given respectively given taking expectation respect posterior distribution gives setting derivatives respect zero obtain step estimation equation exercise analogous result holds exercise note estimation equation takes slightly different form corresponding result derived direct evaluation evidence function however involve computation inversion eigen decomposition matrix hence comparable computational cost per iteration two approaches determining course converge result assuming ﬁnd local maximum evidence function veriﬁed ﬁrst noting quantity deﬁned stationary point evidence function estimation equation self consistently satisﬁed hence substitute give solving obtain precisely estimation equation ﬁnal example consider closely related model namely relevance vector machine regression discussed section used direct maximization marginal likelihood derive estimation equations hyper parameters consider alternative approach view weight vector latent variable apply algorithm step involves ﬁnding posterior distribution weights given step maximize expected complete data log likelihood deﬁned expectation taken respect posterior distribution computed using old parameter values compute new parameter values maximize respect give exercise mixture models new new estimation equations formally equivalent obtained direct maxmization exercise 
"
162,"[mixture, models, algorithm, general]"," expectation maximization algorithm algorithm general technique ﬁnding maximum likelihood solutions probabilistic models latent variables dempster mclachlan krishnan give general treatment algorithm process provide proof algorithm derived heuristically sections gaussian mixtures indeed maximize likelihood function csisz tusn ady hath away neal hinton discussion also form basis derivation variational inference framework section consider probabilistic model collectively denote served variables hidden variables joint distribution governed set parameters denoted goal maximize likelihood function given assuming discrete although discussion identical comprises continuous variables combination discrete continuous variables summation replaced integration appropriate shall suppose direct optimization difﬁcult opti mization complete data likelihood function signiﬁcantly easier next introduce distribution deﬁned latent variables serve choice following decomposition holds deﬁned note functional see appendix discussion functionals distribution function parameters worth studying algorithm general figure illustration decomposition given holds choice distribution kullback leibler divergence satisﬁes lower bound log likelihood function contains joint distribution contains conditional distribution given verify decomposition ﬁrst make use product rule probability give exercise substitute expression gives rise two terms one cancels gives required log likelihood noting normalized distribution sums see kullback leibler divergence posterior distribution recall kullback leibler vergence satisﬁes equality section therefore follows words lower bound decomposition illustrated figure algorithm two stage iterative optimization technique ﬁnding old step lower bound old maximized respect holding old ﬁxed old depend largest value old old case lower bound equal log likelihood illustrated figure subsequent step distribution held ﬁxed lower bound maximized respect give new value new cause lower bound increase unless already maximum new hence nonzero divergence increase log likelihood function therefore greater increase lower bound mixture models figure old causing lower old old shown figure substitute old see step lower bound takes form old old old old const thus step quantity maximized optimizing appears inside logarithm joint distribution comprises member operation algorithm also viewed space parame ters illustrated schematically figure red curve depicts figure distribution held ﬁxed lower bound give revised value new new new algorithm general figure old new old ﬁrst step evaluate poste rior distribution latent variables gives rise lower bound old whose value equals log likelihood old shown blue curve note bound makes tangential contact log likelihood old curves gradient bound convex function unique exercise new gives larger value log likelihood old subsequent step constructs bound tangential new shown green curve particular case independent identically distributed data set comprise data points comprise corresponding latent variables independence assumption marginalizing using sum product rules see posterior probability evaluated step takes form depends value parameters mixture components values data points seen steps algorithm increasing value well deﬁned bound log likelihood function mixture models complete cycle change model parameters way cause log likelihood increase unless already maximum case parameters remain unchanged also use algorithm maximize posterior distribution models introduced prior parameters see note function making use decomposition constant optimize right hand side alternately respect optimization respect gives rise step equations standard algorithm appears step equations modiﬁed introduction prior term typically requires small modiﬁcation standard maximum likelihood step equations algorithm breaks potentially difﬁcult problem maximizing likelihood function two stages step step often prove simpler implement nevertheless complex models may case either step step indeed remain intractable leads two possible extensions algorithm follows generalized gem algorithm addresses problem intractable step instead aiming maximize respect seeks instead change parameters way increase value lower bound log likelihood function complete cycle gem algorithm guaranteed increase value log likelihood unless parameters already correspond local maximum one way exploit gem approach would use one nonlinear optimization strategies conjugate gradients algorithm step another form gem algorithm known expectation conditional maximization ecm algorithm involves making several constrained optimizations within step meng rubin instance parameters might partitioned groups step broken multiple steps involves optimizing one subset remainder held ﬁxed similarly generalize step algorithm performing partial rather complete optimization respect neal hinton seen given value unique maximum respect corresponds posterior distribution choice bound equal log likelihood function follows algorithm converges global maximum ﬁnd value also global maximum log likelihood provided continuous function exercises continuity local maximum also local maximum consider case independent data points corresponding latent variables joint distribution factorizes data points structure exploited incremental form cycle one data point processed time step instead recomputing responsibilities data points evaluate responsibilities one data point might appear subsequent step would require computation involving responsibilities data points ever mixture components members exponential family responsibilities enter simple sufﬁcient statistics dated efﬁciently consider instance case gaussian mixture suppose perform update data point corresponding old new values responsibilities denoted old new step required sufﬁcient statistics updated incrementally instance means sufﬁcient statistics deﬁned obtain exercise new old new old new old together new old new old corresponding results covariances mixing coefﬁcients analogous thus step step take ﬁxed time independent total number data points parameters revised data point rather waiting whole data set processed incremental version converge faster batch version step incremental algorithm increasing value shown algorithm converges local global maximum correspond local global maximum log likelihood function 
"
163,"[mixture, models, exercises]"," consider means algorithm discussed section show consequence ﬁnite number possible assignments set discrete indicator variables assignment unique optimum means algorithm must converge ﬁnite number iterations apply robbins monro sequential estimation procedure described section problem ﬁnding roots regression function given derivatives respect show leads stochastic means algorithm data point nearest prototype updated using mixture models consider gaussian mixture model marginal distribution latent variable given conditional distribution observed variable given show marginal distribution obtained summing possible values gaussian mixture form suppose wish use algorithm maximize posterior distri bution parameters model containing latent variables observed data set show step remains maximum likelihood case whereas step quantity maximized given old old deﬁned consider directed graph gaussian mixture model shown figure making use separation criterion discussed section show posterior distribution latent variables factorizes respect different data points consider special case gaussian mixture model covariance matrices components constrained common value derive equations maximizing likelihood function model verify maximization complete data log likelihood gaussian mixture model leads result means covariances component ﬁtted independently corresponding group data points mixing coefﬁcients given fractions points group show maximize respect keeping responsibilities ﬁxed obtain closed form solution given show maximize respect keeping responsibilities ﬁxed obtain closed form solutions given consider density model given mixture distribution suppose partition vector two parts show conditional density mixture distribution ﬁnd expressions mixing coefﬁcients component densities exercises section obtained relationship means gaussian mixtures considering mixture model components covariance show limit maximizing expected complete data log likelihood model given equivalent minimizing distortion measure means algorithm given consider mixture distribution form elements could discrete continuous combination denote mean covariance respectively show mean covariance mixture distribution given using estimation equations algorithm show mix ture bernoulli distributions parameters set values corresponding maximum likelihood function property hence show parameters model initialized components mean algorithm converge one iteration choice initial mixing coefﬁcients solution property note represents degenerate case mixture model components identical practice try avoid solutions using appropriate initialization consider joint distribution latent observed variables bernoulli distribution obtained forming product given given show marginalize joint distribution respect obtain show maximize expected complete data log likelihood function mixture bernoulli distributions respect obtain step equation show maximize expected complete data log likelihood function mixture bernoulli distributions respect mixing coefﬁcients using lagrange multiplier enforce summation constraint obtain step equation show consequence constraint discrete variable incomplete data log likelihood function mixture bernoulli distributions bounded hence singularities likelihood goes inﬁnity mixture models consider bernoulli mixture model discussed section together prior distribution parameter vectors given beta distribution dirichlet prior given derive algorithm maximizing posterior probability consider dimensional variable whose components multinomial variable degree binary vector components subject constraint suppose distribution variables described mixture discrete multinomial distributions considered section kij parameters kij represent probabilities must satisfy kij together constraint kij values given observed data set derive step equations algorithm optimizing mixing coefﬁcients component parameters kij distribution maximum likelihood show maximization expected complete data log likelihood function bayesian linear regression model leads step estimation result using evidence framework section derive step estimation equations parameter bayesian linear regression model analogous result maximization expected complete data log likelihood deﬁned derive step equations estimating hyperparameters relevance vector machine regression section used direct maximization marginal like lihood derive estimation equations ﬁnding values hyperparameters regression rvm similarly section used algorithm maximize marginal likelihood giving estimation equations show two sets estimation equations formally equivalent verify relation deﬁned respectively exercises show lower bound given old gradient respect log likelihood function point old consider incremental form algorithm mixture gaussians responsibilities recomputed speciﬁc data point starting step formulae derive results updating component means derive step formulae updating covariance matrices mixing coefﬁcients gaussian mixture model responsibilities updated crementally analogous result updating means 
"
164,"[approximate, inference]"," central task application probabilistic models evaluation posterior distribution latent variables given observed visible data variables evaluation expectations computed respect distributionmodel might also contain deterministic parameters leave implicit moment may fully bayesian model unknown parameters given prior distributions absorbed set latent variables denoted vector instance algorithm need evaluate expectation complete data log likelihood respect posterior distribution latent variables many models practical interest infeasible evaluate posterior distribution indeed compute expec tations respect distribution could dimensionality latent space high work directly posterior distribution highly complex form expectations analytically tractable case continuous variables required integrations may closed form approximate inference analytical solutions dimensionality space complexity integrand may prohibit numerical integration discrete variables marginal izations involve summing possible conﬁgurations hidden variables though always possible principle often ﬁnd practice may exponentially many hidden states exact calculation prohibitively expensive situations need resort approximation schemes fall broadly two classes according whether rely stochastic deterministic approximations stochastic techniques markov chain monte carlo scribed chapter enabled widespread use bayesian methods across many domains generally property given inﬁnite computational resource generate exact results approximation arises use ﬁnite amount processor time practice sampling methods compu tationally demanding often limiting use small scale problems also difﬁcult know whether sampling scheme generating independent samples required distribution chapter introduce range deterministic approximation schemes scale well large applications based analytical proximations posterior distribution example assuming factorizes particular way speciﬁc parametric form gaussian never generate exact results strengths weaknesses complementary sampling methods section discussed laplace approximation based local gaussian approximation mode maximum distribution turn family approximation techniques called variational inference variational bayes use global criteria widely applied conclude brief introduction alternative variational framework known expectation propagation 
"
165,"[approximate, inference, variational, inference]"," variational methods origins century work euler lagrange others calculus variations standard calculus concerned ﬁnding derivatives functions think function mapping takes value variable input returns value function output derivative function describes output value varies make inﬁnitesimal changes input value similarly deﬁne functional mapping takes function input returns value functional output example would entropy takes probability distribution input returns quantity variational inference output introduce concept functional derivative presses value functional changes response inﬁnitesimal changes input function feynman rules calculus variations mirror standard calculus discussed appendix many problems expressed terms optimization problem quantity optimized functional solution obtained exploring possible input functions ﬁnd one maximizes minimizes functional variational methods broad applicability include areas ﬁnite element methods kapur maximum entropy schwarz although nothing intrinsically approximate variational methods naturally lend ﬁnding approximate solutions done restricting range functions optimization performed instance considering quadratic functions considering functions composed linear combination ﬁxed basis functions coefﬁcients linear combination vary case applications probabilistic ference restriction may example take form factorization assumptions jordan jaakkola let consider detail concept variational optimization applied inference problem suppose fully bayesian model parameters given prior distributions model may also latent variables well parameters shall denote set latent variables parameters similarly denote set observed variables example might set independent identically distributed data probabilistic model speciﬁes joint distribution goal ﬁnd approximation posterior distribution well model evidence discussion decompose log marginal probability using deﬁned differs discussion parameter vector longer appears parameters stochastic variables absorbed since chapter mainly interested continuous variables used integrations rather summations formulating decomposition ever analysis goes unchanged variables discrete simply replacing integrations summations required maximize lower bound optimization respect distribution equivalent minimizing divergence allow possible choice maximum lower bound occurs diver gence vanishes occurs equals posterior distribution approximate inference figure illustration variational approximation example considered earlier figure left hand plot shows original distribution yellow along laplace red variational green approx imations right hand plot shows negative logarithms corresponding curves however shall suppose model working true posterior distribution intractable therefore consider instead restricted family distributions seek member family divergence minimized goal restrict family sufﬁciently comprise tractable distributions time allowing family sufﬁciently rich ﬂexible provide good approximation true posterior distribution important emphasize restriction imposed purely achieve tractability sub ject requirement use rich family approximating distributions possible particular ﬁtting associated highly ﬂexible distributions using ﬂexible approximations simply allows approach true posterior distribution closely one way restrict family approximating distributions use paramet ric distribution governed set parameters lower bound becomes function exploit standard nonlinear optimization techniques determine optimal values parameters example approach variational distribution gaussian optimized respect mean variance shown figure 
"
166,"[approximate, inference, variational, inference, factorized, distributions]"," consider alternative way restrict family distri butions suppose partition elements disjoint groups denote assume distribution factorizes respect groups variational inference emphasized making assumptions distri bution particular place restriction functional forms individual factors factorized form variational inference corresponds proximation framework developed physics called mean ﬁeld theory parisi amongst distributions form seek distri bution lower bound largest therefore wish make free form variational optimization respect distributions optimizing respect factors turn achieve ﬁrst substitute dissect dependence one factors denoting simply keep notation uncluttered obtain const const deﬁned new distribution relation const notation denotes expectation respect distributions variables suppose keep ﬁxed maximize spect possible forms distribution easily done rec ognizing negative kullback leibler divergence thus maximizing equivalent minimizing kullback leibler 
"
167,"[approximate, inference, leonhard, euler]"," euler swiss mathematician physicist worked petersburg berlin widely considered one greatest mathematicians time certainly proliﬁc collected works ﬁll volumes amongst many contributions formulated modern theory function developed together lagrange calculus variations discovered formula relates four important numbers mathematics last years life almost totally blind yet pro duced nearly half results period approximate inference divergence minimum occurs thus obtain general expression optimal solution given const worth taking moments study form solution provides basis applications variational methods says log optimal lution factor obtained simply considering log joint distribution hidden visible variables taking expectation respect factors additive constant set normalizing distribution thus take exponential sides normalize exponent practice shall ﬁnd convenient work form instate normalization constant required inspection become clear subsequent examples set equations given represent set con sistency conditions maximum lower bound subject factorization constraint however represent explicit solution expression right hand side optimum depends expectations computed respect factors therefore seek consistent solution ﬁrst initializing factors appropriately cycling factors replacing turn revised estimate given right hand side evaluated using current estimates factors convergence guaranteed bound convex respect factors boyd vandenberghe 
"
168,"[approximate, inference, leonhard, euler, properties, factorized, approximations]"," approach variational inference based factorized approximation true posterior distribution let consider moment problem approx imating general distribution factorized distribution begin discuss problem approximating gaussian distribution using factorized gaussian provide useful insight types inaccuracy introduced using factorized approximations consider gaussian distribution two correlated variables mean precision elements due symmetry precision matrix suppose wish approximate distribution using factorized gaussian form ﬁrst apply general result ﬁnd expression variational inference optimal factor useful note right hand side need retain terms functional dependence terms absorbed normalization constant thus const const const next observe right hand side expression quadratic function identify gaussian distribution worth emphasizing assume gaussian rather derived result variational optimization divergence possible distributions note also need consider additive constant explicitly represents normalization constant found end inspection required using technique completing square identify section mean precision gaussian giving symmetry also gaussian written note solutions coupled depends expectations computed respect vice versa general address treating variational solutions estimation equations cycling variables turn updating convergence criterion satisﬁed shall see example shortly however note problem sufﬁciently simple closed form solution found particular see two equations satisﬁed take easily shown solution provided distributionnonsingular result illustrated figure see exercise mean correctly captured variance controlled direction smallest variance variance along orthogonal direction signiﬁcantly estimated general result factorized variational proximation tends give approximations posterior distribution compact way comparison suppose instead minimizing reverse kullback leibler divergence shall see form divergence approximate inference figure comparison two alternative forms kullback leibler divergence green contours corresponding standard deviations correlated gaussian distribution two variables red contours represent corresponding levels approximating distribution variables given product two independent univariate gaussian distributions whose parameters obtained minimization kullback leibler divergence reverse kullback leibler divergence section const easily done using lagrange multiplier give exercise case ﬁnd optimal solution given corre apply result illustrative example gaussian distribution difference two results understood noting large positive contribution kullback leibler divergence variational inference figure another comparison two alternative forms kullback leibler divergence blue contours show bimodal distribution given mixture two gaussians red contours correspond single gaussian distribution best approximates sense minimizing kullback leibler divergence red contours correspond gaussian distribution found numerical minimization kullback leibler divergence showing different local minimum kullback leibler divergence regions space near zero unless also close zero thus minimizing form divergence leads distributions avoid regions small conversely kullback leibler divergence minimized distributions nonzero regions nonzero gain insight different behaviour two diver gences consider approximating multimodal distribution unimodal one illustrated figure practical applications true posterior distri bution often multimodal posterior mass concentrated number relatively small regions parameter space multiple modes may arise nonidentiﬁability latent space complex nonlinear dependence parameters types multimodality encountered chapter context gaussian mixtures manifested multiple maxima likelihood function variational treatment based minimization tend ﬁnd one modes contrast minimize resulting approximations would average across modes context mixture model would lead poor predictive distributions average two good parameter values typically good parameter value possible make use deﬁne useful inference procedure requires rather different approach one discussed considered detail discuss expectation propagation section two forms kullback leibler divergence members alpha family approximate inference divergences ali silvey amari minka deﬁned continuous parameter kullback leibler divergence corresponds limit whereas corresponds limit values equality exercise suppose ﬁxed distribution minimize respect set distributions divergence zero forcing values typically estimate support tend seek mode largest mass conversely divergence zero avoiding values typically stretch cover estimate support obtain symmetric divergence linearly related hellinger distance given square root hellinger distance valid distance metric 
"
169,"[approximate, inference, leonhard, euler, example, univariate, gaussian]"," illustrate factorized variational approximation using gaussian distributionsingle variable mackay goal infer posterior distribution mean precision given data set observed values assumed drawn independently gaussian likelihood function given exp introduce conjugate prior distributions given gam gam gamma distribution deﬁned together distributions constitute gaussian gamma conjugate prior distribution section simple problem posterior distribution found exactly takes form gaussian gamma distribution however tutorial purposes exercise consider factorized variational approximation posterior distribution given variational inference note true posterior distribution factorize way optimum factors obtained general result follows const const completing square see gaussian mean precision given exercise note gives maximum likelihood result precision inﬁnite similarly optimal solution factor given const const hence gamma distribution gam parameters exhibits expected behaviour exercise emphasized assume speciﬁc functional forms optimal distributions arose naturally structure likelihood function corresponding conjugate priors section thus expressions optimal distributions depends moments evaluated respect distribution one approach ﬁnding solution therefore make initial guess say moment use compute distribution given revised distri bution extract required moments use recompute distribution since space hidden variables example two dimensional illustrate variational approximation posterior distribution plotting contours true posterior factorized approximation illustrated figure approximate inference figure illustration variational inference mean precision univariate gaussian distribution contours true posterior distribution shown green contours initial factorized approximation shown blue estimating factor estimating factor contours optimal factorized approximation iterative scheme converges shown red general need use iterative approach order solve optimal factorized posterior distribution simple example considering however ﬁnd explicit solution solving simultaneous equations optimal factors simplify expressions considering broad noninformative priors although parameter settings correspond improper priors see posterior distribution still well deﬁned using standard result mean gamma distribution together appendix using obtain ﬁrst second order moments variational inference form substitute moments solve give exercise recognize right hand side familiar unbiased estimator variance univariate gaussian distribution see use bayesian approach avoided bias maximum likelihood solution section 
"
170,"[approximate, inference, leonhard, euler, model, comparison]"," well performing inference hidden variables may also wish compare set candidate models labelled index prior probabilities goal approximate posterior probabilities observed data slightly complex situation considered far different models may different structure indeed different dimensionality hidden variables cannot fore simply consider factorized approximation must instead recog nize posterior must conditioned must consider readily verify following decomposition based variational distribution exercise lower bound given assuming discrete analysis applies continuous latent variables provided summations replaced integrations maximize respect distribution using lagrange multiplier result exercise exp however maximize respect ﬁnd solutions different coupled expect conditioned proceed instead ﬁrst optimizing individually optimization approximate inference subsequently determining using malization resulting values used model selection model averaging usual way 
"
171,"[approximate, inference, illustration, variational, mixture, gaussians]"," return discussion gaussian mixture model apply variational inference machinery developed previous section provide good illustration application variational methods also demonstrate bayesian treatment elegantly resolves many difﬁculties associated maximum likelihood approach attias reader encouraged work example detail provides many insights practical appli cation variational methods many bayesian models corresponding much sophisticated distributions solved straightforward extensions general izations analysis starting point likelihood function gaussian mixture model lustrated graphical model figure observation corresponding latent variable comprising binary vector elements denote observed data set similarly denote latent variables write conditional distribution given mixing coefﬁcients form similarly write conditional distribution served data vectors given latent variables component parameters note working terms precision matrices rather covariance matrices somewhat simpliﬁes mathemat ics next introduce priors parameters analysis considerably simpliﬁed use conjugate prior distributions therefore choose section dirichlet distribution mixing coefﬁcients dir symmetry chosen parameter components normalization constant dirichlet distribution deﬁned illustration variational mixture gaussians figure observations denotes denotes seen parameter interpreted effective section small posterior distribution inﬂuenced primarily data rather prior similarly introduce independent gaussian wishart prior governing mean precision gaussian component given symmetry section resulting model represented directed graph shown figure note link since variance distribution function example provides nice illustration distinction latent variables parameters variables appear inside plate regarded outside plate ﬁxed order formulate variational treatment model next write joint distribution random variables given observed approximate inference consider variational distribution factorizes latent variables parameters remarkable assumption need make order obtain tractable practical solution bayesian mixture model particular functional form factors determined automatically optimization variational distribution note omitting sub scripts distributions much distributions relying arguments distinguish different distributions corresponding sequential update equations factors easily derived making use general result let consider derivation update equation factor log optimized factor given const make use decomposition note interested functional dependence right hand side variable thus terms depend absorbed additive normalization constant giving const substituting two conditional distributions right hand side absorbing terms independent additive constant const deﬁned dimensionality data variable taking exponential sides obtain requiring distribution normalized noting value quantities binary sum values obtain exercise illustration variational mixture gaussians see optimal solution factor takes functional form prior note given exponential real quantity quantities nonnegative sum one required discrete distribution standard result see quantities playing role responsibilities note optimal solution depends moments evaluated respect distributions variables variational update equations coupled must solved iteratively point shall ﬁnd convenient deﬁne three statistics observed data set evaluated respect responsibilities given note analogous quantities evaluated maximum likelihood algorithm gaussian mixture model let consider factor variational posterior distribution using general result const observe right hand side expression decomposes sum terms involving together terms involving implies variational posterior factorizes give terms involving comprise sum terms involving leading factorization approximate inference identifying terms right hand side depend const used taking exponential sides recognize dirichlet distribution dir components given finally variational posterior distribution factorize product marginals always use product rule write form two factors found inspecting reading terms involve result expected gaussian wishart distribution given exercise deﬁned update equations analogous step equations algorithm maximum likelihood solution mixture gaussians see computations must performed order update variational posterior distribution model parameters involve evaluation sums data set arose maximum likelihood treatment order perform variational step need expectations representing responsibilities obtained normalizing given see expression involves expectations respect variational distributions parameters easily evaluated give exercise illustration variational mixture gaussians introduced deﬁnitions digamma function deﬁned results follow standard properties wishart dirichlet distributions appendix substitute make use obtain following result responsibilities exp notice similarity corresponding result responsibilities maximum likelihood written form exp used precision place covariance highlight similarity thus optimization variational posterior distribution involves cycling two stages analogous steps maximum likelihood algorithm variational equivalent step use current distributions model parameters evaluate moments hence evaluate subsequent variational equivalent step keep responsibilities ﬁxed use compute variational distribution parameters using case see variational posterior distribution functional form corresponding factor joint distribution general result consequence choice conjugate distributions section figure shows results applying approach rescaled old faith full data set gaussian mixture model components see convergence two components expected values mixing coefﬁcients numerically distinguishable prior values effect understood qualitatively terms automatic trade bayesian model ﬁtting data complexity model section complexity penalty arises components whose parameters pushed away prior values components take essentially responsibility plaining data points hence see see parameters revert prior values principle components ﬁtted slightly data points broad priors effect small seen numerically variational gaussian mixture model expected values mixing coefﬁcients posterior distribution given exercise consider component prior broad component plays role model whereas approximate inference figure variational bayesian mixture gaussians plied old faithful data set ellipses denote one standard deviation density contours components density red ink inside ellipse corresponds mean value mixing coefﬁcient component number top left diagram shows number iterations variational infer ence components whose expected mixing coefﬁcient numerically distinguishable zero plotted prior tightly constrains mixing coefﬁcients figure prior mixing coefﬁcients dirichlet form recall figure prior favours solutions mixing coefﬁcients zero figure obtained using resulted two components nonzero mixing coefﬁcients instead choose obtain three components nonzero mixing coefﬁcients six components nonzero mixing coefﬁcients seen close similarity variational solution bayesian mixture gaussians algorithm maximum likelihood fact consider limit bayesian treatment converges maximum likelihood algorithm anything small datasets dominant computational cost variational algorithm gaussian mixtures arises evaluation responsibilities together evaluation inversion weighted data covariance matrices computations mirror precisely arise maximum likelihood algorithm little computational overhead using bayesian approach compared traditional maximum likelihood one however substantial advantages first singularities arise maximum likelihood gaussian component collapses onto speciﬁc data point absent bayesian treatment illustration variational mixture gaussians indeed singularities removed simply introduce prior use map estimate instead maximum likelihood furthermore ﬁtting choose large number components mixture saw figure finally variational treatment opens possibility determining optimal number components mixture without resorting techniques cross validation section 
"
172,"[approximate, inference, illustration, variational, mixture, gaussians, variational, lower, bound]"," also straightforwardly evaluate lower bound model practice useful able monitor bound estimation order test convergence also provide valuable check math ematical expressions solutions software implementation step iterative estimation procedure value bound decrease take stage provide deeper test correctness mathematical derivation update equations software plementation using ﬁnite differences check update indeed give constrained maximum bound svens bishop variational mixture gaussians lower bound given keep notation uncluttered omitted superscript distributions along subscripts expectation operators expectation taken respect random variables argument various terms bound easily evaluated give following results exercise approximate inference dimensionality entropy wishart distribution given coefﬁcients deﬁned respectively note terms involving expectations logs distributions simply represent negative entropies distributions simpliﬁcations combination terms performed expressions summed give lower bound however kept expressions sepa rate ease understanding finally worth noting lower bound provides alternative approach deriving variational estimation equations obtained section use fact since model conjugate priors functional form factors variational posterior distribution known namely discrete dirichlet gaussian wishart taking general parametric forms distributions derive form lower bound function parameters distributions maximizing bound respect parameters gives required estimation equations exercise 
"
173,"[approximate, inference, illustration, variational, mixture, gaussians, predictive, density]"," applications bayesian mixture gaussians model often interested predictive density new value observed variable sociated observation corresponding latent variable predictive density given illustration variational mixture gaussians unknown true posterior distribution parameters using ﬁrst perform summation give remaining integrations intractable approximate predictive density replacing true posterior distribution variational approximation give made use factorization term plicitly integrated variables remaining integrations evaluated analytically giving mixture student distributions exercise component mean precision given given size data set large predictive distribution reduces mixture gaussians exercise 
"
174,"[approximate, inference, illustration, variational, mixture, gaussians, determining, number, components]"," seen variational lower bound used determine posterior distribution number components mixture model section however one subtlety needs addressed given setting parameters gaussian mixture model except speciﬁc degenerate settings exist parameter settings density observed variables identical parameter values differ labelling components instance consider mixture two gaussians single served variable parameters values parameter values two components exchanged symmetry give rise value mixture model comprising components parameter setting member family equivalent settings exercise context maximum likelihood redundancy irrelevant parameter optimization algorithm example depending initial ization parameters ﬁnd one speciﬁc solution equivalent solutions play role bayesian setting however marginalize possible approximate inference figure plot variational lower bound versus number components gaussian mixture model old faithful data showing distinct peak components value model trained different random starts results shown symbols plotted small random hori zontal perturbations distinguished note solutions ﬁnd suboptimal local maxima hap pens infrequently parameter values seen figure true posterior distribution multimodal variational inference based minimization tend approximate distribution neighbourhood one modes ignore others equivalent modes equivalent predictive densities concern provided considering model speciﬁc number components however wish compare different values need take account multimodality simple approximate solution add term onto lower bound used model comparison averaging exercise figure shows plot lower bound including multimodalityfactor versus number components old faithful data set worth emphasizing maximum likelihood would lead values likelihood function increase monotonically assuming singular solutions avoided discounting effects local maxima cannot used determine appropriate model complexity contrast bayesian inference automatically makes trade model complexity ﬁtting data section approach determination requires range models different values trained compared alternative approach determining suitable value treat mixing coefﬁcients parameters make point estimates values maximizing lower bound corduneanu bishop respect instead maintaining probability distribution fully bayesian approach leads estimation equation exercise maximization interleaved variational updates distribution remaining parameters components provide insufﬁcient contribution illustration variational mixture gaussians explaining data mixing coefﬁcients driven zero optimization effectively removed model automatic relevance determination allows make single training run start relatively large initial value allow surplus components pruned model origins sparsity optimizing respect hyperparameters discussed detail context relevance vector machine section 
"
175,"[approximate, inference, illustration, variational, mixture, gaussians, induced, factorizations]"," deriving variational update equations gaussian mixture model assumed particular factorization variational posterior distribution given however optimal solutions various factors exhibit additional factorizations particular solution given product independent distribution components mixture whereas variational posterior distribution latent variables given factorizes independent distribution observation note factorize respect value constrained sum one additional factorizations consequence interaction assumed factorization conditional independence properties true distribution characterized directed graph figure shall refer additional factorizations induced factorizations cause arise interaction factorization assumed variational posterior distribution conditional independence properties true joint distribution numerical implementation variational approach important take account additional factorizations instance would inefﬁcient maintain full precision matrix gaussian distribution set variables optimal form distribution always diagonal precision matrix corresponding factorization respect individual variables described gaussian induced factorizations easily detected using simple graphical test based separation follows partition latent variables three disjoint groups let suppose assuming factorization remaining latent variables using general result together product rule probabilities see optimal solution given const const ask whether resulting solution factorize words whether happen conditional independence relation approximate inference satisﬁed test see relation hold choice making use separation criterion illustrate consider bayesian mixture gaussians represented directed graph figure assuming variational factorization given see immediately variational posterior distribution parameters must factorize remaining parameters paths connecting either must pass one nodes conditioning set conditional independence test head tail respect paths 
"
176,"[approximate, inference, variational, linear, regression]"," second illustration variational inference return bayesian linear regression model section evidence framework approximated integration making point estimates obtained maximizing log marginal likelihood fully bayesian approach would integrate hyperparameters well parameters although exact integration intractable use variational methods ﬁnd tractable approximation order simplify discussion shall suppose noise precision parameter known ﬁxed true value although framework easily extended include distribution linear regression model variational treatment exercise turn equivalent evidence framework nevertheless provides good exercise use variational methods also lay foundation variational treatment bayesian logistic regression section recall likelihood function prior given introduce prior distribution discussion section know conjugate prior precision gaussian given gamma distribution choose gam gam deﬁned thus joint distribution variables given represented directed graphical model shown figure 
"
177,"[approximate, inference, variational, linear, regression, variational, distribution]"," ﬁrst goal ﬁnd approximation posterior distribution employ variational framework section variational variational linear regression figure probabilistic graphical model representing joint distributionbayesian linear regression model posterior distribution given factorized expression ﬁnd estimation equations factors distribution making use general result recall factor take log joint distribution variables average respect variables factor consider ﬁrst distribution keeping terms functional dependence const const recognize log gamma distribution identifying coefﬁcients obtain gam similarly ﬁnd variational estimation equation posterior distribution using general result keeping terms functional dependence const const const quadratic form distribution gaussian complete square usual way identify mean covariance giving approximate inference note close similarity posterior distribution obtained treated ﬁxed parameter difference replaced expectation variational distribution indeed chosen use notation covariance matrix cases using standard results obtain required moments follows evaluation variational posterior distribution begins initializing parameters one distributions alternately estimates factors turn suitable convergence criterion satisﬁed usually speci ﬁed terms lower bound discussed shortly instructive relate variational solution found using evidence framework section consider case corresponding limit inﬁnitely broad prior mean variational posterior distribution given comparison shows case particularly simple model variational approach gives precisely expression obtained maximizing evidence function using except point estimate replaced expected value distribution depends expectation see two approaches give identical results case inﬁnitely broad prior 
"
178,"[approximate, inference, variational, linear, regression, predictive, distribution]"," predictive distribution given new input easily evaluated model using gaussian variational posterior parameters variational linear regression evaluated integral making use result linear gaussian model input dependent variance given note takes form result obtained ﬁxed except expected value appears deﬁnition 
"
179,"[approximate, inference, variational, linear, regression, lower, bound]"," another quantity importance lower bound deﬁned evaluation various terms straightforward making use results obtained exercise previous chapters gives figure shows plot lower bound versus degree polynomial model synthetic data set generated degree three polynomial prior parameters set corresponding noninformative prior uniform discussed section saw section quantity represents lower bound log marginal likelihood model assign equal prior probabilities different values interpret approximation poste rior model probability thus variational framework assigns highest probability model contrasted maximum likelihood result assigns ever smaller residual error models increasing complexity residual error driven zero causing maximum likelihood favour severely ﬁtted models approximate inference figure plot lower bound ver sus order polynomial polynomial model set data points generated polynomial sampled interval additive gaussian noise variance value bound gives log prob ability model see value bound peaks corresponding true model data set generated 
"
180,"[approximate, inference, exponential, family, distributions]"," corresponding hidden variables member exponential family whereas marginal distribution mixture gaussians hence grouped variables model observed variables parameters intensive ﬁxed num specify component responsible generating data point represent latent variables whereas means precisions mixing proportions represent parameters consider case independent identically distributed data denote data values corresponding latent variables suppose joint distribution observed latent variables member exponential family parameterized natural parameters exp shall also use conjugate prior written exp recall conjugate prior distribution interpreted prior number observations value vector consider variational exponential family distributions distribution factorizes latent variables parameters using general result solve two factors follows const const thus see decomposes sum independent terms one value hence solution factorize example induced factorization taking exponential section sides exp normalization coefﬁcient instated comparison standard form exponential family similarly variational distribution parameters const const taking exponential sides instating normalization coef ﬁcient inspection exp deﬁned note solutions coupled solve iter atively two stage procedure variational step evaluate expected sufﬁcient statistics using current posterior distribution latent variables use compute revised posterior distribution parameters subsequent variational step use revised parameter posterior distribution ﬁnd expected natural parameters gives rise revised variational distribution latent variables 
"
181,"[approximate, inference, exponential, family, distributions, variational, message, passing]"," illustrated application variational methods considering speciﬁc model bayesian mixture gaussians detail model approximate inference described directed graph shown figure consider generally use variational methods models described directed graphs derive number widely applicable results joint distribution corresponding directed graph written using decomposition denotes variable associated node denotes parent set corresponding node note may latent variable may belong set observed variables consider variational approximation distribution assumed factorize respect note observed nodes factor variational distribution substitute general result give const terms right hand side depend absorbed additive constant fact terms depend con ditional distribution given together conditional distributions conditioning set deﬁnition conditional distributions correspond children node therefore also depend parents child nodes parents child nodes besides node see set nodes depends corresponds markov blanket node illustrated figure thus update factors variational posterior distribution represents local calculation graph makes possible construction general purpose software variational inference form model need speciﬁed advance bishop specialize case model conditional distributions conjugate exponential structure variational update proce dure cast terms local message passing algorithm winn bishop particular distribution associated particular node updated node received messages parents children turn requires children already received messages parents evaluation lower bound also simpliﬁed many required quantities already evaluated part message passing scheme distributed message passing formulation good scaling properties well suited large networks local variational methods 
"
182,"[approximate, inference, local, variational, methods]"," variational framework discussed sections considered global method sense directly seeks approximation full poste rior distribution random variables alternative local approach involves ﬁnding bounds functions individual variables groups variables within model instance might seek bound conditional distribution one factor much larger probabilistic model speciﬁed directed graph purpose introducing bound course simplify resulting distribution local approximation applied multiple variables turn tractable approximation obtained section shall give practical example approach context logistic regression focus developing bounds already seen discussion kullback leibler divergence convexity logarithm function played key role developing lower bound global variational approach deﬁned strictly convex function one every chord lies function convexity also plays section central role local variational framework note discussion equally concave functions min max interchanged lower bounds replaced upper bounds let begin considering simple example namely function exp convex function shown left hand plot figure goal approximate simpler function particular linear function figure see linear function lower bound corresponds tangent obtain tangent line speciﬁc value say making ﬁrst order taylor expansion equality example function figure left hand ure red curve shows function exp blue line shows tangent deﬁned line slope exp note tangent line ample ones shown green smaller value right hand ﬁgure shows corresponding plot function given versus maximum corresponds exp approximate inference figure left hand plot red curve shows convex function blue line represents linear function lower bound given value slope contact point tangent line slope found minimizing respect discrepancy shown green dashed lines given deﬁnes dual function corresponds negative intercept tangent line slope exp therefore obtain tangent line form exponent linear function parameterized consistency subsequent discussion let deﬁne exp different values correspond different tangent lines lines lower bounds function thus write function form max succeeded approximating convex function simpler linear function price paid introduced variational parameter obtain tightest bound must optimize respect formulate approach generally using framework convex duality rockafellar jordan consider illustration convex function shown left hand plot figure example function lower bound best lower bound achieved linear function slope tightest bound given tangent line let write equation tangent line slope negative intercept clearly depends slope tangent determine intercept note line must moved vertically amount equal smallest vertical distance line function shown figure thus min max local variational methods instead ﬁxing varying consider particular adjust tangent plane tangent particular value tangent line particular maximized value coincides contact point max see functions play dual role related let apply duality relations simple example exp see maximizing value given back substituting obtain conjugate function form obtained previously function shown right hand plot figure check substitute gives maximizing value exp back substituting recovers original function exp concave functions follow similar argument obtain upper bounds max replaced min min min function interest convex concave cannot directly apply method obtain bound however ﬁrst seek invertible transformations either function argument change convex form calculate conjugate function transform back original variables important example arises frequently pattern recognition logistic sigmoid function deﬁned stands function neither convex concave however take logarithm obtain function concave easily veriﬁed ﬁnding second derivative corresponding conjugate function takes exercise form min recognize binary entropy function variable whose probability value using obtain upper bound log appendix sigmoid approximate inference figure left hand plot shows logistic sigmoid function deﬁned red together two examples exponential upper bound shown blue right hand plot shows logistic sigmoid red together gaussian lower bound shown blue parameter bound exact denoted dashed green lines taking exponential obtain upper bound logistic sigmoid form exp plotted two values left hand plot figure also obtain lower bound sigmoid functional form gaussian follow jaakkola jordan make transformations input variable function first take log logistic function decompose note function convex function variable veriﬁed ﬁnding second derivative leads exercise lower bound linear function whose conjugate function given max stationarity condition leads tanh denote value corresponding contact point tangent line particular value tanh local variational methods instead thinking variational parameter let play role leads simpler expressions conjugate function given hence bound written bound sigmoid becomes exp deﬁned bound illustrated right hand plot figure see bound form exponential quadratic function prove useful seek gaussian representations posterior distributions deﬁned logistic sigmoid functions section logistic sigmoid arises frequently probabilistic models binary variables function transforms log odds ratio posterior prob ability corresponding transformation multiclass distribution given softmax function unfortunately lower bound derived logistic section sigmoid directly extend softmax gibbs proposes method constructing gaussian distribution conjectured bound although rigorous proof given may used apply local variational methods multiclass problems shall see example use local variational bounds sections moment however instructive consider general terms bounds used suppose wish evaluate integral form logistic sigmoid gaussian probability density integrals arise bayesian models instance wish evaluate predictive distribution case represents posterior parameter distribution integral intractable employ variational bound write form variational parameter inte gral becomes product two exponential quadratic functions integrated analytically give bound freedom choose variational parameter ﬁnding value maximizes function resulting value represents tightest bound within family bounds used approximation optimized bound however general exact approximate inference although bound logistic sigmoid optimized exactly required choice depends value bound exact one value quantity obtained integrating values value represents compromise weighted distribution 
"
183,"[approximate, inference, variational, logistic, regression]"," illustrate use local variational methods returning bayesian logistic regression model studied section focussed use laplace approximation consider variational treatment based approach jaakkola jordan like laplace method also leads gaussian approximation posterior distribution however greater ﬂexibility variational approximation leads improved accuracy compared laplace method furthermore unlike laplace method variational approach optimizing well deﬁned objective function given rigourous bound model evidence logistic regression also treated dybowski roberts bayesian perspective using monte carlo sampling techniques 
"
184,"[approximate, inference, variational, logistic, regression, variational, posterior, distribution]"," shall make use variational approximation based local bounds introduced section allows likelihood function logistic regression governed logistic sigmoid approximated exponential quadratic form therefore convenient choose conjugate gaussian prior form moment shall treat hyperparameters ﬁxed constants section shall demonstrate variational formalism extended case unknown hyper parameters whose values inferred data variational framework seek maximize lower bound marginal likelihood bayesian logistic regression model marginal likelihood takes form ﬁrst note conditional distribution written order obtain lower bound make use variational lower bound logistic sigmoid function given variational logistic regression reproduce convenience exp therefore write exp note bound applied terms likelihood function separately variational parameter corresponding training set observation using multiplying prior distribution obtain following bound joint distribution denotes set variational parameters exp evaluation exact posterior distribution would require normalization left hand side inequality intractable work instead right hand side note function right hand side cannot interpreted probability density normalized normalized give variational posterior distribution however longer represents bound logarithm function monotonically increasing inequality implies gives lower bound log joint distribution form substituting prior right hand side inequality becomes function const approximate inference quadratic function obtain corresponding variational approximation posterior distribution identifying linear quadratic terms giving gaussian variational posterior form laplace framework obtained gaussian approximation posterior distribution however additional ﬂexibility provided variational parameters leads improved accuracy approximation jaakkola jordan considered batch learning context training data available however bayesian methods intrinsically well suited sequential learning data points processed one time discarded formulation variational approach sequential case straightforward exercise note bound given applies two class problem approach directly generalize classiﬁcation problems classes alternative bound multiclass case explored gibbs 
"
185,"[approximate, inference, variational, logistic, regression, optimizing, variational, parameters]"," normalized gaussian approximation posterior distribution shall use shortly evaluate predictive distribution new data points first however need determine variational parameters maximizing lower bound marginal likelihood substitute inequality back marginal likelihood give optimization hyperparameter linear regression model section two approaches determining ﬁrst approach recognize function deﬁned integration view latent variable invoke algorithm second approach integrate analytically perform direct maximization let begin considering approach algorithm starts choosing initial values parameters denote collectively old step algorithm variational logistic regression use parameter values ﬁnd posterior distribution given step maximize expected complete data log likelihood given old expectation taken respect posterior distribution evaluated using old noting depend substituting obtain old const const denotes terms independent set derivative respect equal zero lines algebra making use deﬁnitions gives note monotonic function restrict attention nonnegative values without loss generality due symmetry bound around thus hence obtain following estimation equations exercise new used let summarize algorithm ﬁnding variational posterior distri bution ﬁrst initialize variational parameters old step evaluate posterior distribution given mean covariance deﬁned step use variational posterior compute new value given steps repeated suitable convergence criterion satisﬁed practice typically requires iterations alternative approach obtaining estimation equations note integral deﬁnition lower bound integrand gaussian like form integral evaluated analytically evaluated integral differentiate respect turns gives rise exactly estimation equations approach given exercise emphasized already application variational methods useful able evaluate lower bound given integration performed analytically noting gaussian exponential quadratic function thus completing square making use standard result normalization coefﬁcient gaussian distribution obtain closed form solution takes form exercise approximate inference figure illustration bayesian approach logistic regression simple linearly separable data drawn posterior distribution variational framework also applied situations data predictive distribution obtained marginalizing posterior dis far treated hyperparameter prior distribution known variational logistic regression speciﬁcally consider simple isotropic gaussian prior distribution form analysis readily extended general gaussian priors instance wish associate different hyperparameter different subsets parame ters usual consider conjugate hyperprior given gamma distribution gam governed constants marginal likelihood model takes form joint distribution given faced analytically intractable integration shall tackle using local global variational approaches model begin introduce variational distribution apply decomposition instance takes form lower bound kullback leibler divergence ﬁned point lower bound still intractable due form likelihood factor therefore apply local variational bound logistic sigmoid factors allows use inequality place lower bound therefore also lower bound log marginal likelihood next assume variational distribution factorizes parameters hyperparameters approximate inference factorization appeal general result ﬁnd expressions optimal factors consider ﬁrst distribution discarding terms independent const const substitute using using giving const see quadratic function solution gaussian completing square usual way obtain deﬁned similarly optimal solution factor obtained const substituting using using obtain const recognize log gamma distribution obtain gam expectation propagation also need optimize variational parameters also done maximizing lower bound omitting terms independent integrating const note precisely form appeal earlier result obtained direct optimization marginal likelihood function leading estimation equations form new obtained estimation equations three quantities making suitable initializations cycle quantities updating turn required moments given appendix 
"
186,"[approximate, inference, expectation, propagation]"," conclude chapter discussing alternative form deterministic approx imate inference known expectation propagation minka minka variational bayes methods discussed far based minimization kullback leibler divergence reverse form gives approximation rather different properties consider moment problem minimizing respect ﬁxed distribution member exponential family written form exp function kullback leibler divergence becomes const constant terms independent natural parameters mini mize within family distributions setting gradient respect zero giving however already seen negative gradient given expectation distribution equating two results obtain approximate inference see optimum solution simply corresponds matching expected suf ﬁcient statistics instance gaussian minimize kullback leibler divergence setting mean equal mean distribution covariance equal covariance sometimes called moment matching example seen figure let exploit result obtain practical algorithm approximate inference many probabilistic models joint distribution data hidden variables including parameters comprises product factors form would arise example model independent identically distributed data one factor data point along factor corresponding prior generally would also apply model deﬁned directed probabilistic graph factor conditional distribution corresponding one nodes undirected graph factor clique potential interested evaluating posterior distribution purpose making predictions well model evidence purpose model comparison posterior given model evidence given considering continuous variables following discussion applies equally discrete variables integrals replaced summations shall suppose marginalization along marginalizations respect posterior distribution required make predictions intractable form approximation required expectation propagation based approximation posterior distribution also given product factors factor approximation corresponds one factors true posterior factor normalizing constant needed ensure left hand side integrates unity order obtain practical algorithm need constrain factors way particular shall assume come exponential family product factors therefore also exponential family expectation propagation described ﬁnite set sufﬁcient statistics example gaussian overall approximation also gaussian ideally would like determine minimizing kullback leibler divergence true posterior approximation given note reverse form divergence compared used variational inference general minimization intractable vergence involves averaging respect true distribution rough approx imation could instead minimize divergences corresponding pairs factors represents much simpler problem solve advantage algorithm noniterative however factor individually approximated product factors could well give poor approximation expectation propagation makes much better approximation optimizing factor turn context remaining factors starts initializing factors cycles factors reﬁning one time similar spirit update factors variational bayes framework considered earlier suppose wish reﬁne factor ﬁrst remove factor product give conceptually determine revised form factor ensuring product new close possible keep ﬁxed factors ensures approximation accurate regions high posterior probability deﬁned remaining factors shall see example effect apply clutter problem achieve ﬁrst remove factor section current approximation posterior deﬁning unnormalized distribution note could instead ﬁnd product factors although practice division usually easier combined factor give distribution approximate inference figure illustration expectation propagation approximation using gaussian distribution example considered earlier figures left hand plot shows original distribution yellow along laplace red global variational green blue approximations right hand plot shows corresponding negative logarithms distributions note distribution broader variational inference consequence different form divergence normalization constant given determine revised factor minimizing kullback leibler diver gence new easily solved approximating distribution new exponential family appeal result tells parameters new obtained matching expected sufﬁcient statistics corresponding moments shall assume tractable operation example choose gaussian distribution set equal mean unnormalized distribution set covariance generally straightforward obtain required expectations member exponential family provided normalized expected statistics related derivatives normalization coefﬁcient given approximation illustrated figure see revised factor found taking new dividing remaining factors new used coefﬁcient determined multiplying expectation propagation sides integrating give used fact new normalized value therefore found matching zeroth order moments combining see found evaluating integral practice several passes made set factors revising factor turn posterior distribution approximated using model evidence approximated using factors replaced approximations expectation propagation given joint distribution observed data stochastic variables form product factors wish approximate posterior distribution distribution form also wish approximate model evidence initialize approximating factors initialize posterior approximation setting convergence choose factor reﬁne remove posterior division approximate inference evaluate new posterior setting sufﬁcient statistics moments new equal including evaluation normalization constant evaluate store new factor new evaluate approximation model evidence special case known assumed density ﬁltering adf moment matching maybeck lauritzen boyen koller opper winther obtained initializing approximating factors except ﬁrst unity making one pass factors updating assumed density ﬁltering appropriate line learning data points arriving sequence need learn data point discard considering next point however batch setting opportunity use data points many times order achieve improved curacy idea exploited expectation propagation furthermore apply adf batch data results undesirable dependence arbitrary order data points considered overcome one disadvantage expectation propagation guarantee iterations converge however approximations exponential family iterations converge resulting solution stationary point particular energy function minka although iteration necessarily decrease value energy function contrast variational bayes iteratively maximizes lower bound log marginal likelihood iteration guaranteed decrease bound possible optimize cost function directly case guaranteed converge although resulting algorithms slower complex implement another difference variational bayes arises form divergence minimized two algorithms former mini mizes whereas latter minimizes saw figure distributions multimodal minimizing lead poor approximations particular applied mixtures results sen sible approximation tries capture modes posterior distribution conversely logistic type models often performs local variational methods laplace approximation kuss rasmussen expectation propagation figure illustration clutter problem data space dimensionality training data points noted crosses drawn mixture two gaussians components shown red green goal infer mean green gaussian observed data 
"
187,"[approximate, inference, expectation, propagation, example, clutter, problem]"," following minka illustrate algorithm using simple exam ple goal infer mean multivariate gaussian distribution variable given set observations drawn distribution make problem interesting observations embedded background clutter also gaussian distributed illustrated figure distribution observed values therefore mixture gaussians take form proportion background clutter assumed known prior taken gaussian minka chooses parameter values joint distribution observations given posterior distribution comprises mixture gaussians thus computational cost solving problem exactly would grow exponentially size data set exact solution intractable moderately large apply clutter problem ﬁrst identify factors next select approximating distribution exponential family example convenient choose spherical gaussian approximate inference factor approximations therefore take form exponential quadratic functions form set equal prior note use imply right hand side well deﬁned gaussian density fact shall see variance parameter negative simply convenient shorthand notation approximations initialized unity corresponding dimensionality hence initial deﬁned therefore equal prior iteratively reﬁne factors taking one factor time applying note need revise term update leave term unchanged state exercise results leave reader ﬁll details first remove current estimate division using give mean inverse variance given exercise next evaluate normalization constant using give similarly compute mean variance new ﬁnding mean variance give exercise quantity simple interpretation probability point clutter use compute reﬁned factor whose parameters given new new reﬁnement process repeated suitable termination criterion satisﬁed instance maximum change parameter values resulting complete expectation propagation figure examples approximation speciﬁc factors one dimensional version clutter problem showing blue red green notice current form controls range good approximation pass factors less threshold finally use evaluate approximation model evidence given new exp new new examples factor approximations clutter problem one dimensional parameter space shown figure note factor approximations inﬁnite even negative values variance parameter simply corresponds approximations curve upwards instead downwards necessarily problematic provided overall approximate posterior posi tive variance figure compares performance variational bayes mean ﬁeld theory laplace approximation clutter problem 
"
188,"[approximate, inference, expectation, propagation, expectation, propagation, graphs]"," far general discussion allowed factors distribution functions components similarly approximating factors approximating distribution consider situations factors depend subsets variables restrictions conveniently expressed using framework probabilistic graphical models discussed chapter use factor graph representation encompasses directed undirected graphs approximate inference laplace posterior mean flops error laplace evidence flops error figure comparison expectation propagation variational inference laplace approximation clutter problem left hand plot shows error predicted posterior mean versus number ﬂoating point operations right hand plot shows corresponding results model evidence shall focus case approximating distribution fully fac first recall minimize kullback leibler diver gence respect factorized distribution optimal solution factor simply corresponding marginal consider factor graph shown left figure introduced earlier context sum product algorithm joint distribution section given seek approximation factorization apply algorithm using fully factorized approximation suppose initialized factors choose reﬁne factor expectation propagation figure left simple factor graph figure reproduced convenience right corresponding factorized approximation ﬁrst remove factor approximating distribution give multiply exact factor give ﬁnd new minimizing kullback leibler divergence new result noted new comprises product factors one variable factor given corresponding marginal four marginals given new obtained multiplying marginals together see factors change update involve variables namely obtain reﬁned factor simply divide new gives approximate inference precisely messages obtained using belief propagation mes section sages variable nodes factor nodes folded messages factor nodes variable nodes particular corresponds message sent factor node variable node given simi larly substitute obtain corre sponds corresponds giving message corresponds result differs slightly standard belief propagation messages passed directions time easily modify procedure give standard form sum product algorithm updating one factors time instance reﬁne unchanged deﬁnition reﬁned version given reﬁning one term time choose order reﬁnements done wish particular tree structured graph follow two pass update scheme corresponding standard belief propagation schedule result exact inference variable factor marginals initialization approximation factors case unimportant let consider general factor graph corresponding distribution represents subset variables associated factor approximate using fully factorized distribution form corresponds individual variable node suppose wish reﬁne particular term keeping terms ﬁxed ﬁrst remove term give multiply exact factor determine reﬁned term need consider functional dependence simply ﬁnd corresponding marginal multiplicative constant involves taking marginal multiplied terms functions variables terms correspond factors cancel numerator denominator subsequently divide therefore obtain exercises recognize sum product rule form messages variable nodes factor nodes eliminated illustrated example shown figure quantity corresponds message factor node sends variable node product factors depend variables variables variable common factor words compute outgoing message factor node take product incoming messages factor nodes multiply local factor marginalize thus sum product algorithm arises special case expectation propagation use approximating distribution fully factorized suggests ﬂexible approximating distributions corresponding partially discon nected graphs could used achieve higher accuracy another generalization group factors together sets reﬁne factors set together iteration approaches lead improvements accuracy minka general problem choosing best combination grouping disconnection open research issue seen variational message passing expectation propagation timize two different forms kullback leibler divergence minka shown broad range message passing algorithms derived common framework involving minimization members alpha family diver gences given include variational message passing loopy belief propagation expectation propagation well range algorithms space discuss tree reweighted message passing wainwright fractional belief propagation wiegerinck heskes power minka 
"
189,"[approximate, inference, exercises]"," verify log marginal distribution observed data decomposed two terms form given given use properties solve simultaneous equations hence show provided original distribution nonsingular unique solution means factors approximation distribution given consider factorized variational distribution form using technique lagrange multipliers verify minimization kullback leibler divergence respect one factors keeping factors ﬁxed leads solution suppose ﬁxed distribution wish approximate using gaussian distribution writing form divergence gaussian differentiating show approximate inference minimization respect leads result given expectation given covariance consider model set hidden stochastic variables noted collectively comprises latent variables together model parameters suppose use variational distribution factorizes tent variables parameters distribution approximated point estimate form vector free parameters show variational optimization factorized distribution equivalent algorithm step optimizes step maximizes expected complete data log posterior distribution respect alpha family divergences deﬁned show kullback leibler divergence corresponds done writing exp taking similarly show corresponds consider problem inferring mean precision univariate gaussian using factorized variational approximation considered section show factor gaussian form mean precision given respectively similarly show factor gamma distribution form gam parameters given consider variational posterior distribution precision univariate gaussian whose parameters given using standard results mean variance gamma distribution given show let variational posterior distribution mean given inverse maximum likelihood estimator variance data variance goes zero making use standard result mean gamma distribution together derive result reciprocal expected precision factorized variational treat ment univariate gaussian derive decomposition given used ﬁnd approxi mate posterior distributions models using variational inference using lagrange multiplier enforce normalization constraint distribution show maximum lower bound given starting joint distribution applying general result show optimal variational distribution latent variables bayesian mixture gaussians given verifying steps given text exercises starting derive result optimum variational posterior distribution bayesian mixture gaussians hence verify expressions parameters distribution given using distribution verify result using result show expected value mixing coefﬁcients variational mixture gaussians given verify results ﬁrst two terms lower bound variational gaussian mixture model given verify results remaining terms lower bound variational gaussian mixture model given exercise shall derive variational estimation equations gaussian mixture model direct differentiation lower bound assume variational distribution factorization deﬁned factors given substitute hence obtain lower bound function parameters variational distribution maximizing bound respect parameters derive estimation equations factors variational distribution show obtained section derive result predictive distribution variational treat ment bayesian mixture gaussians model exercise explores variational bayes solution mixture gaussians model size data set large shows reduces would expect maximum likelihood solution based derived chapternote results appendix may used help answer exercise first show posterior distribution precisions becomes sharply peaked around maximum likelihood solution posterior distributionmeans next consider posterior distribution mixing coefﬁcients show becomes sharply peaked around maximum likelihood solution similarly show responsibilities become equal corresponding maximum likelihood values large making use following asymptotic result digamma function large finally making use show large predictive distribution becomes mixture gaussians show number equivalent parameter settings due interchange symmetries mixture model components approximate inference seen mode posterior distribution gaussian mix ture model member family equivalent modes suppose result running variational inference algorithm approximate posterior distribution localized neighbourhood one modes approximate full posterior distribution mixture distributions centred mode equal mixing coefﬁcients show assume negligible overlap components mixture resulting lower bound differs single component distribution dition extra term consider variational gaussian mixture model prior distribution mixing coefﬁcients instead mixing coefﬁcients treated parameters whose values found maximizing variational lower bound log marginal likelihood show maximizing lower bound respect mixing coefﬁcients using lagrange multiplier enforce constraint mixing coefﬁcients sum one leads estimation result note need consider terms lower bound dependence bound seen section singularities arising maximum likelihood treatment gaussian mixture models arise bayesian treatment discuss whether singularities would arise bayesian model solved using maximum posterior map estimation variational treatment bayesian mixture gaussians discussed section made use factorized approximation posterior distribution saw figure factorized assumption causes variance posterior distribution estimated certain directions parameter space discuss qualitatively effect variational approximation model evidence effect vary number components mixture hence explain whether variational gaussian mixture tend estimate estimate optimal number components extend variational treatment bayesian linear regression include gamma hyperprior gam solve variationally assuming factorized variational distribution form derive variational update equations three factors variational distribution also obtain expression lower bound predictive distribution making use formulae given appendix show variational lower bound linear basis function regression model deﬁned written form various terms deﬁned rewrite model bayesian mixture gaussians introduced section conjugate model exponential family discussed section hence use general results derive speciﬁc results exercises show function concave computing second derivative determine form dual function deﬁned verify minimization respect according indeed recovers function evaluating second derivative show log logistic function concave derive variational upper bound directly making second order taylor expansion log logistic function around point ﬁnding second derivative respect show function concave function consider second derivatives respect variable hence show convex function plot graphs derive lower bound logistic sigmoid function directly making ﬁrst order taylor series expansion function variable centred value consider variational treatment logistic regression sequential learning data points arriving one time must pro cessed discarded next data point arrives show gaussian proximation posterior distribution maintained use lower bound distribution initialized using prior data point absorbed corresponding variational parameter optimized differentiating quantity old deﬁned respect variational parameter show update equation bayesian logistic regression model given exercise derive estimation equations variational parame ters bayesian logistic regression model section direct maximization lower bound given set derivative spect equal zero making use result derivative log determinant together expressions deﬁne mean covariance variational posterior distribution derive result lower bound variational logistic regression model easily done substituting expressions gaussian prior together lower bound likelihood function integral deﬁnes next gather together terms depend exponential complete square give gaussian integral evaluated invoking standard result normalization coefﬁcient multivariate gaussian finally take logarithm obtain consider adf approximation scheme discussed section show inclusion factor leads update model evidence form approximate inference normalization constant deﬁned applying result recursively initializing derive result consider expectation propagation algorithm section suppose one factors deﬁnition exponential family functional form approximating distribution show factor initialized update reﬁne leaves unchanged situation typically arises one factors prior see prior factor incorporated exactly need reﬁned exercise next shall verify results expectation propagation algorithm applied clutter problem begin using division formula derive expressions completing square inside exponential identify mean variance also show normalization constant deﬁned given clutter problem done making use general result show mean variance new applied clutter problem given ﬁrst prove following results expectations new make use result next prove results using completing square exponential finally use derive result 
"
190,"[sampling, methods]"," probabilistic models practical interest exact inference intractable resort form approximation chapter discussed inference algorithms based deterministic approximations include methods variational bayes expectation propagation consider approxi mate inference methods based numerical sampling also known monte carlo techniques although applications posterior distribution unobserved variables direct interest situations posterior distribution required primarily purpose evaluating expectations example order make predictions fundamental problem therefore wish address chapter involves ﬁnding expectation function respect probability distribution components might comprise discrete continuous variables combination two thus case continuous sampling methods figure schematic illustration function whose expectation evaluated respect distribution variables wish evaluate expectation integral replaced summation case discrete variables illustrated schematically single continuous variable figure shall suppose expectations complex evaluated exactly using analytical techniques general idea behind sampling methods obtain set samples drawn independently distribution allows expectation approximated ﬁnite sum long samples drawn distribution estimator correct mean variance estimator given exercise var variance function distribution worth emphasizing accuracy estimator therefore depend dimension ality principle high accuracy may achievable relatively small number samples practice ten twenty independent samples may sufﬁce estimate expectation sufﬁcient accuracy problem however samples might independent effective sample size might much smaller apparent sample size also referring back figure note small regions large vice versa expectation may dominated regions small probability implying relatively large sample sizes required achieve sufﬁcient accuracy many models joint distribution conveniently speciﬁed terms graphical model case directed graph observed variables sampling methods straightforward sample joint distribution assuming possible sample conditional distributions node using following ances tral sampling approach discussed brieﬂy section joint distribution speciﬁed set variables associated node denotes set variables associated parents node obtain sample joint distribution make one pass set variables order sampling conditional distributions always possible cause step parent values instantiated one pass graph obtained sample joint distribution consider case directed graph nodes stantiated observed values principle extend procedure least case nodes representing discrete variables give following logic sampling approach henrion seen special case impor tance sampling discussed section step sampled value obtained variable whose value observed sampled value compared observed value agree sample value retained gorithm proceeds next variable turn however sampled value observed value disagree whole sample far discarded algorithm starts ﬁrst node graph algorithm samples correctly posterior distribution corresponds simply drawing samples joint distribution hidden variables data variables discarding samples disagree observed data slight saving continuing sampling joint distribution soon one contradictory value observed however overall probability accepting sample posterior decreases rapidly number observed variables increases number states variables take increases approach rarely used practice case probability distributions deﬁned undirected graph one pass sampling strategy sample even prior distribution observed variables instead computationally expensive techniques must employed gibbs sampling discussed section well sampling conditional distributions may also require samples marginal distribution already strategy sampling joint distribution straightforward obtain samples marginal distribution simply ignoring values sample numerous texts dealing monte carlo methods partic ular interest statistical inference perspective include chen gamerman gilks liu neal robert casella also review articles besag brooks diaconis saloff coste jerrum sinclair neal tierney andrieu provide additional information sampling sampling methods methods statistical inference diagnostic tests convergence markov chain monte carlo algorithms summarized robert casella practical guidance use sampling methods context machine learning given bishop nabney 
"
191,"[sampling, methods, basic, sampling, algorithms]"," section consider simple strategies generating random samples given distribution samples generated computer algorithm fact pseudo random numbers deter ministically calculated must nevertheless pass appropriate tests randomness generating numbers raises several subtleties press lie outside scope book shall assume algorithm provided generates pseudo random numbers distributed uniformly indeed software environments facility built 
"
192,"[sampling, methods, basic, sampling, algorithms, standard, distributions]"," ﬁrst consider generate random numbers simple nonuniform distributions assuming already available source uniformly distributed random numbers suppose uniformly distributed interval transform values using function distribution governed case goal choose function resulting values speciﬁc desired distribution integrating obtain indeﬁnite integral thus exercise transform uniformly distributed random numbers using function inverse indeﬁnite integral desired distribution illustrated figure consider example exponential distribution exp case lower limit integral exp thus transform uniformly distributed variable using exponential distribution basic sampling algorithms figure geometrical interpretation trans formation method generating nonuniformly distributed random numbers indeﬁnite integral desired distributionuniformly distributed random variable transformed using distributed cording another example distribution transformation method applied given cauchy distribution case inverse indeﬁnite integral expressed terms tan function exercise generalization multiple variables straightforward involves cobian change variables ﬁnal example transformation method consider box muller method generating samples gaussian distribution first suppose generate pairs uniformly distributed random numbers transforming variable distributed uniformly using next discard pair unless satisﬁes leads uniform distribution points inside unit circle illustrated figure pair evaluate quantities figure box muller method generating gaussian dis tributed random numbers starts generating samples uniform distribution inside unit circle sampling methods joint distribution given exercise exponent independent gaussian distribution zero mean unit variance gaussian distribution zero mean unit variance gaussian distribution mean variance generate vector valued variables multivariate gaussian distribution mean variance make use cholesky decomposition takes form press vector valued random variable whose components independent gaussian distributed zero mean unit variance mean covariance exercise obviously transformation technique depends success ability calculate invert indeﬁnite integral required distribution operations feasible limited number simple distributions must turn alternative approaches search general strategy consider two techniques called rejection sampling importance sampling though mainly limited univariate distributions thus directly applicable complex problems many dimensions form important components general strategies 
"
193,"[sampling, methods, basic, sampling, algorithms, rejection, sampling]"," rejection sampling framework allows sample relatively complex distributions subject certain constraints begin considering univariate distributions discuss extension multiple dimensions subsequently suppose wish sample distribution one simple standard distributions considered far sampling directly dif ﬁcult furthermore suppose often case easily able evaluate given value normalizing constant readily evaluated unknown order apply rejection sampling need simpler distribution sometimes called proposal distribution readily draw samples basic sampling algorithms figure rejection sampling method samples drawn simple distribution rejected fall grey area tween unnormalized distribution scaled distribution resulting samples distributed according normalized version next introduce constant whose value chosen values function called comparison function illus trated univariate distribution figure step rejection sampler involves generating two random numbers first generate number distribution next generate number uniform distribution pair random numbers uniform distribution curve function finally sample rejected otherwise retained thus pair rejected lies grey shaded region figure remaining pairs uniform distribution curve hence corresponding values distributed according desired exercise original values generated distribution samples accepted probability probability sample accepted given accept thus fraction points rejected method depends ratio area unnormalized distribution area curve therefore see constant small possible subject limitation must nowhere less illustration use rejection sampling consider task sampling gamma distribution gam exp bell shaped form shown figure suitable proposal distribution therefore cauchy bell shaped use transformation method discussed earlier sample need generalize cauchy slightly ensure nowhere smaller value gamma distribution achieved transforming uniform random variable using tan gives random numbers distributed according exercise sampling methods figure plot showing gamma distribution given green curve scaled cauchy proposal distribution shown red curve samples gamma distribution obtained sampling cauchy applying rejection sam pling criterion minimum reject rate obtained setting choosing constant small possible still satisfying requirement resulting comparison function also illustrated figure 
"
194,"[sampling, methods, basic, sampling, algorithms, adaptive, rejection, sampling]"," many instances might wish apply rejection sampling proves difﬁcult determine suitable analytic form envelope distribution alternative approach construct envelope function based mea sured values distribution gilks wild construction envelope function particularly straightforward cases log concave words derivatives nonincreasing functions construction suitable envelope function illustrated graphically figure function gradient evaluated initial set grid points intersections resulting tangent lines used construct envelope function next sample value drawn envelope distribution straightforward log envelope distribution succession exercise figure case distributions log concave envelope function use rejection sampling constructed using tangent lines computed set grid points sample point rejected added set grid points used reﬁne envelope distribution basic sampling algorithms figure illustrative example rejection sampling involving sampling gaussian distribution shown green curve using rejection sampling proposal distri bution also gaussian whose scaled version shown red curve linear functions hence envelope distribution comprises piecewise exponential distribution form exp sample drawn usual rejection criterion applied sample accepted draw desired distribution however sample rejected incorporated set grid points new tangent line computed envelope function thereby reﬁned number grid points increases envelope function becomes better approximation desired distribution probability rejection decreases variant algorithm exists avoids evaluation derivatives gilks adaptive rejection sampling framework also extended distri butions log concave simply following rejection sampling step metropolis hastings step discussed section giving rise adaptive rejection metropolis sampling gilks clearly rejection sampling practical value require comparison function close required distribution rate rejection kept minimum let examine happens try use rejection sampling spaces high dimensionality consider sake illustration somewhat artiﬁcial problem wish sample zero mean mul tivariate gaussian distribution covariance unit matrix rejection sampling proposal distribution zero mean gaussian distribution covariance obviously must order exists dimensions optimum value given illustrated figure acceptance rate ratio volumes distributions normalized thus acceptance rate diminishes exponentially dimensionality even exceeds one percent acceptance ratio approximately illustrative example comparison function close required distribution practical exam ples desired distribution may multimodal sharply peaked extremely difﬁcult ﬁnd good proposal distribution comparison function sampling methods figure importance sampling addresses prob lem evaluating expectation function respect distribution difﬁcult draw samples rectly instead samples drawn simpler distribution corresponding terms summation weighted ratios 
"
195,"[sampling, methods, basic, sampling, algorithms, importance, sampling]"," one principal reasons wishing sample complicated probability ﬁnite sum approximation expectation given depends case rejection sampling importance sampling based use basic sampling algorithms samples drawn quantities known importance weights cor rect bias introduced sampling wrong distribution note unlike rejection sampling samples generated retained often case distribution evaluated normalization constant evaluated easily whereas unknown similarly may wish use importance sampling distribution property use sample set evaluate ratio result hence deﬁned rejection sampling success importance sampling approach depends crucially well sampling distribution matches desired sampling methods distribution often case strongly varying signiﬁcant proportion mass concentrated relatively small regions space set importance weights may dominated weights having large values remaining weights relatively insigniﬁcant thus effective sample size much smaller apparent sample size prob lem even severe none samples falls regions large case apparent variances may small even though estimate expectation may severely wrong hence major draw back importance sampling method potential produce results arbitrarily error diagnostic indication also highlights key quirement sampling distribution namely small zero regions may signiﬁcant distributions deﬁned terms graphical model apply impor tance sampling technique various ways discrete variables simple approach called uniform sampling joint distribution directed graph deﬁned sample joint distribution obtained ﬁrst setting variables evidence set equal observed values remaining variables sampled independently uniform distribution space possible instantiations determine corresponding weight associated sample note sampling distribution uniform possible choices denotes subset variables observed equality follows fact every sample generated necessarily consistent evidence thus weights simply proportional note variables sampled order approach yield poor results posterior distribution far uniform often case practice improvement approach called likelihood weighted sampling fung chang shachter peot based ancestral sampling variables variable turn variable evidence set set instantiated value evidence set sampled conditional distribution conditioning variables set currently sampled values weighting associated resulting sample given method extended using self importance sampling shachter peot importance sampling distribution continually updated reﬂect current estimated posterior distribution 
"
196,"[sampling, methods, basic, sampling, algorithms, sampling-importance-resampling]"," rejection sampling method discussed section depends part success determination suitable value constant many pairs distributions impractical determine suitable basic sampling algorithms value value sufﬁciently large guarantee bound desired distribution lead impractically small acceptance rates case rejection sampling sampling importance resampling sir approach also makes use sampling distribution avoids termine constant two stages scheme ﬁrst stage samples drawn second stage weights constructed using finally second set samples drawn discrete distribution probabilities given weights resulting samples approximately distributed according distribution becomes correct limit see consider univariate case note cumulative distribution resampled values given indicator function equals argument true otherwise taking limit assuming suitable regularity distributions replace sums integrals weighted according original sampling distribution cumulative distribution function see normalization required ﬁnite value given initial sample set resampled values approximately drawn desired distribution rejection sam pling approximation improves sampling distribution gets closer desired distribution initial samples desired distribution weights resampled values also desired distribution moments respect distribution required sampling methods evaluated directly using original samples together weights 
"
197,"[sampling, methods, basic, sampling, algorithms, sampling, algorithm]"," addition providing mechanism direct implementation bayesian framework monte carlo methods also play role frequentist paradigm example ﬁnd maximum likelihood solutions particular sampling methods used approximate step algorithm models step cannot performed analytically consider model hidden variables visible observed variables parameters function optimized respect step expected complete data log likelihood given old old use sampling methods approximate integral ﬁnite sum samples drawn current estimate posterior distribution old old function optimized usual way step procedure called monte carlo algorithm straightforward extend problem ﬁnding mode posterior distribution map estimate prior distribution deﬁned simply adding function old performing step particular instance monte carlo algorithm called stochastic arises consider ﬁnite mixture model draw one sample step latent variable characterizes components mixture responsible generating data point step sample taken posterior distribution old data set effectively makes hard assignment data point one components mixture step sampled approximation posterior distribution used update model parameters usual way markov chain monte carlo suppose move maximum likelihood approach full bayesian treatment wish sample posterior distribution parameter vector principle would like draw samples joint posterior shall suppose computationally difﬁcult suppose fur ther relatively straightforward sample complete data parameter posterior inspires data augmentation algorithm alternates two steps known step imputation step analogous step step posterior step analogous step algorithm step wish sample cannot directly therefore note relation hence ﬁrst draw sample current esti mate use draw sample step given relation use samples obtained step compute revised estimate posterior distribution given assumption feasible sample approximation step note making somewhat artiﬁcial distinction parameters hidden variables blur distinction focus simply problem drawing samples given posterior distribution 
"
198,"[sampling, methods, markov, chain, monte, carlo]"," previous section discussed rejection sampling importance sam pling strategies evaluating expectations functions saw suffer severe limitations particularly spaces high dimensionality therefore turn section general powerful framework called markov chain monte carlo mcmc allows sampling large class distributions sampling methods scales well dimensionality sample space markov chain monte carlo methods origins physics metropolis ulam towards end started signiﬁcant impact ﬁeld statistics rejection importance sampling sample proposal distribution time however maintain record current state proposal distribution depends current state sequence samples forms markov chain write section assume readily evaluated given value although value may unknown proposal distribution chosen sufﬁciently simple straightforward draw samples directly cycle algorithm generate candidate sample proposal distribution accept sample according appropriate criterion basic metropolis algorithm metropolis assume proposal distribution symmetric values candidate sample accepted probability min achieved choosing random number uniform distribution unit interval accepting sample note step causes increase value candidate point certain kept candidate sample accepted otherwise candidate point discarded set another candidate sample drawn distribution contrast rejection sampling jected samples simply discarded metropolis algorithm candidate point rejected previous sample included instead ﬁnal list samples leading multiple copies samples course practical implementation single copy retained sample would kept along integer weighting factor recording many times state appears shall see long positive values sufﬁcient necessary condition distribution tends emphasized however sequence set independent samples successive samples highly correlated wish obtain independent samples discard sequence tain every sample sufﬁciently large retained samples practical purposes independent figure shows simple illustrative exam ple sampling two dimensional gaussian distribution using metropolis algorithm proposal distribution isotropic gaussian insight nature markov chain monte carlo algorithms gleaned looking properties speciﬁc example namely simple random markov chain monte carlo figure simple illustration using metropo lis algorithm sample gaussian distribution whose one standard deviation contour shown ellipse proposal distribution isotropic gaussian distri bution whose standard deviation steps accepted shown green lines rejected steps shown red total candidate samples generated rejected walk consider state space consisting integers probabilities denotes state step initial state symmetry expected state time also zero similarly easily seen thus steps random walk trav exercise elled distance average proportional square root square root dependence typical random walk behaviour shows random walks inefﬁcient exploring state space shall see central goal designing markov chain monte carlo methods avoid random walk behaviour 
"
199,"[sampling, methods, markov, chain, monte, carlo, markov, chains]"," discussing markov chain monte carlo methods detail useful study general properties markov chains detail particular ask circumstances markov chain converge desired distributionﬁrst order markov chain deﬁned series random variables following conditional independence property holds course represented directed graph form chain ample shown figure specify markov chain giving probability distribution initial variable together sampling methods conditional probabilities subsequent variables form transition probabil ities markov chain called homogeneous transition probabilities marginal probability particular variable expressed terms marginal probability previous variable chain form distribution said invariant stationary respect markov chain step chain leaves distribution invariant thus homogeneous markov chain transition probabilities distribution invariant note given markov chain may one invariant distribution instance transition probabilities given identity transformation distribution invariant sufﬁcient necessary condition ensuring required distribution invariant choose transition probabilities satisfy property detailed balance deﬁned particular distribution easily seen transition probability satisﬁes detailed balance respect particular distribution leave distribution invariant markov chain respects detailed balance said reversible goal use markov chains sample given distribution achieve set markov chain desired distribution invariant however must also require distribution converges required invariant distribution irrespective choice initial distributionproperty called ergodicity invariant distribution called equilibrium distribution clearly ergodic markov chain one equilibrium distribution shown homogeneous markov chain ergodic subject weak restrictions invariant distribution transition probabilities neal practice often construct transition probabilities set base transitions achieved mixture distribution form markov chain monte carlo set mixing coefﬁcients satisfying alternatively base transitions may combined successive application distribution invariant respect base transitions obviously also invariant respect either given case mixture base transitions sat isﬁes detailed balance mixture transition also satisfy detailed bal ance hold transition probability constructed using though symmetrizing order application base transitions form detailed balance restored common ample use composite transition probabilities base transition changes subset variables 
"
200,"[sampling, methods, markov, chain, monte, carlo, metropolis-hastings, algorithm]"," earlier introduced basic metropolis algorithm without actually demon strating samples required distribution giving proof ﬁrst discuss generalization known metropolis hastings algorithm hastings case proposal distribution longer symmetric function arguments particular step algorithm cur rent state draw sample distribution accept probability min labels members set possible transitions considered evaluation acceptance criterion require knowledge normalizing constant probability distribution symmetric proposal distribution metropolis hastings criterion reduces stan dard metropolis criterion given show invariant distribution markov chain deﬁned metropolis hastings algorithm showing detailed balance deﬁned satisﬁed using min min required speciﬁc choice proposal distribution marked effect performance algorithm continuous state spaces common choice gaussian centred current state leading important trade determining variance parameter distribution variance small sampling methods figure schematic illustration use isotropic gaussian proposal distribution blue circle sample correlated multivariate gaussian distribution red ellipse different stan dard deviations different directions using metropolis hastings algorithm order keep rejection rate low scale proposal distribution order smallest standard deviation min leads random walk behaviour number steps sep arating states approximately independent order max min max largest standard deviation max min proportion accepted transitions high progress state space takes form slow random walk leading long correlation times however variance parameter large rejection rate high kind complex problems considering many proposed steps states probability low consider multivariate distribution strong correlations components illustrated figure scale proposal distribution large possible without incurring high rejection rates suggests order smallest length scale min system explores distribution along extended direction means random walk number steps arrive state less independent original state order max min fact two dimensions increase rejection rate increases offset larger steps sizes transitions accepted generally multivariate gaussian number steps required obtain independent samples scales like max second smallest stan dard deviation neal details aside remains case length scales distributions vary different different directions metropolis hastings algorithm slow convergence 
"
201,"[sampling, methods, gibbs, sampling]"," gibbs sampling geman geman simple widely applicable markov chain monte carlo algorithm seen special case metropolis hastings algorithm consider distribution wish sample suppose chosen initial state markov chain step gibbs sampling procedure involves replacing value one variables value drawn distribution variable conditioned values remaining variables thus replace value drawn distribution denotes component denotes omitted procedure repeated either cycling variables gibbs sampling particular order choosing variable updated step random distribution example suppose distribution three variables step algorithm selected values ﬁrst replace new value obtained sampling conditional distri bution next replace value obtained sampling conditional distribution new value used straight away subsequent sampling steps update sample drawn cycling three variables turn gibbs sampling initialize sample sample sample sample 
"
202,"[sampling, methods, josiah, willard, gibbs]"," gibbs spent almost entire life living house built father new connecticut gibbs granted ﬁrst phd engineering united states appointed ﬁrst chair mathematical physics united states yale post received salary time publications veloped ﬁeld vector analysis made contri butions crystallography planetary orbits famous work entitled equilibrium het erogeneous substances laid foundations science physical chemistry sampling methods show procedure samples required distribution ﬁrst note distribution invariant gibbs sampling steps individually hence whole markov chain follows fact sample marginal distribution clearly invariant value unchanged also step deﬁnition samples correct conditional distribution conditional marginal distributions together specify joint distribution see joint distribution invariant second requirement satisﬁed order gibbs sampling proce dure samples correct distribution ergodic sufﬁcient condition ergodicity none conditional distributions anywhere zero case point space reached point ﬁnite number steps involving one update component variables requirement satisﬁed conditional distributions zeros ergodicity applies must proven explicitly distribution initial states must also speciﬁed order complete algorithm although samples drawn many iterations effectively become independent distribution course successive samples markov chain highly correlated obtain samples nearly independent necessary subsample sequence obtain gibbs sampling procedure particular instance metropolis hastings algorithm follows consider metropolis hastings sampling step involving variable remaining variables remain ﬁxed transition probability given note components unchanged sampling step also thus factor determines acceptance probability metropolis hastings given used thus metropolis hastings steps always accepted metropolis algorithm gain insight behaviour gibbs sampling investigating application gaussian distribution consider correlated gaussian two variables illustrated figure con ditional distributions width marginal distributions width typical step size governed conditional distributions order state evolves according random walk number steps needed obtain independent samples distribution order course gaussian distribution uncorrelated gibbs sampling procedure would optimally efﬁcient simple problem could rotate coordinate sys tem order decorrelate variables however practical applications generally infeasible ﬁnd transformations one approach reducing random walk behaviour gibbs sampling called relaxation adler original form applies problems gibbs sampling figure illustration gibbs sampling alternate updates two variables whose distribution correlated gaussian step size governed stan dard deviation conditional distri bution green curve leading slow progress direction elongation joint distribution red ellipse number steps needed obtain independent sample distribution conditional distributions gaussian represents general class distributions multivariate gaussian example non gaussian distribution exp gaussian conditional distributions step gibbs sampling algorithm conditional distribution particular component mean variance relaxation frame work value replaced gaussian random variable zero mean unit variance parameter method equivalent standard gibbs sampling step biased opposite side mean step leaves desired distribution invariant mean variance effect relaxation encourage directed motion state space variables highly correlated framework ordered relaxation neal generalizes approach non gaussian distributions practical applicability gibbs sampling depends ease samples drawn conditional distributions case probability distributions speciﬁed using graphical models conditional distributions individual nodes depend variables corresponding markov blankets illustrated figure directed graphs wide choice conditional distributions individual nodes conditioned parents lead conditional distributions gibbs sampling log concave adaptive jection sampling methods discussed section therefore provide framework monte carlo sampling directed graphs broad applicability graph constructed using distributions exponential family parent child relationships preserve conjugacy full conditional distri butions arising gibbs sampling functional form orig sampling methods figure gibbs sampling method requires samples drawn conditional distribution variable conditioned remaining variables graphical models conditional distribution function states nodes markov blanket undirected graph comprises set neighbours shown left directed graph markov blanket comprises parents children parents shown right inal conditional distributions conditioned parents deﬁning node standard sampling techniques employed general full conditional distributions complex form permit use standard sam pling algorithms however conditionals log concave sampling done efﬁciently using adaptive rejection sampling assuming corresponding variable scalar stage gibbs sampling algorithm instead drawing sample corresponding conditional distribution make point estimate variable given maximum conditional distribution obtain iterated conditional modes icm algorithm discussed section thus icm seen greedy approximation gibbs sampling basic gibbs sampling technique considers one variable time strong dependencies successive samples opposite extreme could draw samples directly joint distribution operation supposing intractable successive samples would independent hope improve simple gibbs sampler adopting intermediate strategy sample successively groups variables rather individual variables achieved blocking gibbs sampling algorithm choosing blocks variables necessarily disjoint sampling jointly variables block turn conditioned remaining variables jensen 
"
203,"[sampling, methods, slice, sampling]"," seen one difﬁculties metropolis algorithm sensi tivity step size small result slow decorrelation due random walk behaviour whereas large result inefﬁciency due high rejection rate technique slice sampling neal provides adaptive step size automatically adjusted match characteristics distribution requires able evaluate unnormalized distribution consider ﬁrst univariate case slice sampling involves augmenting additional variable drawing samples joint space shall see another example approach discuss hybrid monte carlo section goal sample uniformly area distribution slice sampling min max figure illustration slice sampling given value value chosen uniformly region deﬁnes slice distribution shown solid horizontal lines infeasible sample directly slice new sample drawn region min max contains previous value given otherwise marginal distribution given sample sampling ignoring values achieved alternately sampling given value evaluate sample uniformly range straightforward sample uniformly slice distribution deﬁned illustrated figure practice difﬁcult sample directly slice distribution instead deﬁne sampling scheme leaves uniform distribution invariant achieved ensuring detailed balance satisﬁed suppose current value denoted obtained corresponding sample next value obtained considering region min max contains choice region adap tation characteristic length scales distribution takes place want region encompass much slice possible allow large moves space little possible region lying outside slice makes sampling less efﬁcient one approach choice region involves starting region containing width testing end points see lie within slice either end point region extended direction increments value end point lies outside region candidate value chosen uniformly region lies within slice forms lies outside slice region shrunk forms end point region still contains another sampling methods candidate point drawn uniformly reduced region value found lies within slice slice sampling applied multivariate distributions repeatedly sam pling variable turn manner gibbs sampling requires able compute component function proportional 
"
204,"[sampling, methods, hybrid, monte, carlo, algorithm]"," already noted one major limitations metropolis algorithm exhibit random walk behaviour whereby distance traversed state space grows square root number steps problem cannot resolved simply taking bigger steps leads high rejection rate section introduce sophisticated class transitions based analogy physical systems property able make large changes system state keeping rejection probability small plicable distributions continuous variables readily evaluate gradient log probability respect state variables discuss dynamical systems framework section section explain may combined metropolis algorithm yield pow erful hybrid monte carlo algorithm background physics required section self contained key results derived ﬁrst principles 
"
205,"[sampling, methods, hybrid, monte, carlo, algorithm, dynamical, systems]"," dynamical approach stochastic sampling origins algorithms simulating behaviour physical systems evolving hamiltonian dynam ics markov chain monte carlo simulation goal sample given probability distribution framework hamiltonian dynamics exploited casting probabilistic simulation form hamiltonian system order remain keeping literature area make use relevant dynamical systems terminology appropriate deﬁned along dynamics consider corresponds evolution state variable continuous time denote classical dynamics scribed newton second law motion acceleration object proportional applied force corresponding second order differential equation time decompose second order equation two coupled ﬁrst order equations introducing intermediate momentum variables corresponding rate change state variables components regarded position variables dynamics perspective thus hybrid monte carlo algorithm position variable corresponding momentum variable joint space position momentum variables called phase space without loss generality write probability distribution form exp interpreted potential energy system state system acceleration rate change momentum given applied force negative gradient potential energy convenient reformulate dynamical system using hamiltonian framework ﬁrst deﬁne kinetic energy total energy system sum potential kinetic energies hamiltonian function using express dynamics system terms hamiltonian equations given exercise 
"
206,"[sampling, methods, william, hamilton]"," william rowan hamilton irish mathematician physicist child prodigy pointed professor astronomy trinity college dublin fore even graduated one hamilton important contributions new formulation dynamics played signiﬁcant role later development quantum mechanics great achievement development quaternions generalize concept complex numbers introducing three distinct square roots minus one satisfy ijk said equations occurred walking along royal canal dublin wife october promptly carved equations side broome bridge although longer evidence carving stone plaque bridge commemorating discovery displaying quaternion equations sampling methods evolution dynamical system value hamiltonian constant easily seen differentiation second important property hamiltonian dynamical systems known ouville theorem preserve volume phase space words consider region within space variables region evolves equations hamiltonian dynamics shape may change volume seen noting ﬂow ﬁeld rate change location phase space given divergence ﬁeld vanishes div consider joint distribution phase space whose total energy hamiltonian distribution given exp using two results conservation volume conservation follows hamiltonian dynamics leave invariant seen considering small region phase space approximately constant follow evolution hamiltonian equations ﬁnite time volume region remain unchanged value region hence probability density function also unchanged although invariant values vary integrating hamiltonian dynamics ﬁnite time duration becomes possible make large changes systematic way avoids random walk behaviour evolution hamiltonian dynamics however sample ergodically value constant order arrive ergodic sampling scheme introduce additional moves phase space change value also leaving distribution invariant simplest way achieve replace value one drawn distribution conditioned regarded gibbs sampling step hence hybrid monte carlo algorithm section see also leaves desired distribution invariant noting independent distribution see conditional distribution gaussian straightforward sample exercise practical application approach address problem performing numerical integration hamiltonian equations neces sarily introduce numerical errors devise scheme minimizes impact errors fact turns integration schemes devised liouville theorem still holds exactly property important hybrid monte carlo algorithm discussed section one scheme achieving called leapfrog discretization involves alternately updating discrete time approximations position momentum variables using see takes form half step update momentum variables step size followed full step update position variables step size followed second half step update momentum variables several leapfrog steps applied succession seen half step updates momentum variables combined full step updates step size successive updates position momentum variables leapfrog order advance dynamics time interval need take steps error involved discretized approximation continuous time dynamics zero assuming smooth function limit however nonzero used practice residual error remain shall see section effects errors eliminated hybrid monte carlo algorithm summary hamiltonian dynamical approach involves alternating tween series leapfrog updates resampling momentum variables marginal distribution note hamiltonian dynamics method unlike basic metropolis algo rithm able make use information gradient log probability distribution well distribution analogous situation familiar domain function optimization cases gradient information available highly advantageous make use informally follows fact space dimension additional computational cost evaluating gradient compared evaluating function typically ﬁxed factor independent whereas dimensional gradient vector conveys pieces information compared one piece information given function sampling methods 
"
207,"[sampling, methods, william, hamilton, hybrid, monte, carlo]"," discussed previous section nonzero step size discretization leapfrog algorithm introduce errors integration hamil tonian dynamical equations hybrid monte carlo duane neal combines hamiltonian dynamics metropolis algorithm thereby removes bias associated discretization speciﬁcally algorithm uses markov chain consisting alternate stochastic updates momentum variable hamiltonian dynamical updates using leapfrog algorithm application leapfrog algorithm resulting candidate state accepted rejected according metropolis criterion based value hamiltonian thus initial state state leapfrog integration candidate state accepted probability min exp leapfrog integration simulate hamiltonian dynamics perfectly every candidate step would automatically accepted value would unchanged due numerical errors value may sometimes decrease would like metropolis criterion remove bias due effect ensure resulting samples indeed drawn required distributionorder case need ensure update equations corresponding leapfrog integration satisfy detailed balance easily achieved modifying leapfrog scheme follows start leapfrog integration sequence choose random equal probability whether integrate forwards time using step size backwards time using step size ﬁrst note leapfrog integration scheme time reversible integration steps using step size exactly undo effect integration steps using step size next show leapfrog integration preserves phase space volume exactly follows fact step leapfrog scheme updates either variable variable amount function variable shown figure effect shearing region phase space altering volume finally use results show detailed balance holds consider small region phase space sequence leapfrog iterations step size maps region using conservation volume leapfrog iteration see volume choose initial point distribution update using leapfrog interactions probability transition going given exp min exp factor arises probability choosing integrate positive step size rather negative one similarly probability starting hybrid monte carlo algorithm figure step leapfrog algorithm modiﬁes either position variable momentum variable change one variable function region phase space sheared without change volume region integrating backwards time end region given exp min exp easily seen two probabilities equal hence detailed balance holds note proof ignores overlap regions exercise easily generalized allow overlap difﬁcult construct examples leapfrog algorithm returns starting position ﬁnite number iterations cases random replacement momentum values leapfrog integration sufﬁcient ensure ergodicity position variables never updated phenomena easily avoided choosing magnitude step size random small interval leapfrog integration gain insight behaviour hybrid monte carlo algo rithm considering application multivariate gaussian convenience consider gaussian distribution independent components hamiltonian given conclusions equally valid gaussian distribution correlated components hybrid monte carlo algorithm exhibits rotational isotropy leapfrog integration pair phase space variables evolves dependently however acceptance rejection candidate point based value depends values variables thus signiﬁcant integration error one variables could lead high prob ability rejection order discrete leapfrog integration reasonably sampling methods good approximation true continuous time dynamics necessary leapfrog integration scale smaller shortest length scale potential varying signiﬁcantly governed smallest value denote min recall goal leapfrog integration hybrid monte carlo move substantial distance phase space new state relatively independent initial state still achieve high probability acceptance order achieve leapfrog integration must continued number iterations order max min contrast consider behaviour simple metropolis algorithm isotropic gaussian proposal distribution variance considered earlier order avoid high rejection rates value must order min exploration state space proceeds random walk takes order max min steps arrive roughly independent state 
"
208,"[sampling, methods, estimating, partition, function]"," seen sampling algorithms considered chapter quire functional form probability distribution multiplicative constant thus write exp value normalization constant also known partition function needed order draw samples however knowledge value useful bayesian model comparison since represents model evidence probability observed data given model interest consider value might obtained assume direct evaluation summing integrating function exp state space intractable model comparison actually ratio partition functions two models required multiplication ratio ratio prior probabilities gives ratio posterior probabilities used model selection model averaging one way estimate ratio partition functions use importance sampling distribution energy function exponentonent exponent estimating partition function samples drawn distribution deﬁned distributionone partition function evaluated analytically example gaussian absolute value obtained approach yield accurate results importance sampling distri bution closely matched distribution ratio wide variations practice suitable analytically speciﬁed importance sampling distributions cannot readily found kinds complex models considered book alternative approach therefore use samples obtained markov chain deﬁne importance sampling distribution transition probability markov chain given sample set given sampling distribution written exp used directly methods estimating ratio two partition functions require suc cess two corresponding distributions reasonably closely matched especially problematic wish ﬁnd absolute value partition function complex distribution relatively simple distributions partition function evaluated directly attempting estimate ratio partition functions directly unlikely successful problem tackled using technique known chaining neal barber bishop involves introducing succession intermediate distributions interpolate simple distribution evaluate normalization coefﬁcient desired complex distribution intermediate ratios determined using monte carlo methods discussed one way construct sequence intermediate systems use energy function containing continuous parameter interpolates two distributions intermediate ratios found using monte carlo may efﬁcient use single markov chain run restart markov chain ratio case markov chain run initially system suitable number steps moves next distribution sequence note however system must remain close equilibrium distribution stage sampling methods 
"
209,"[sampling, methods, exercises]"," show ﬁnite sample estimator deﬁned mean equal variance given suppose random variable uniform distribution transform using given show distribution given random variable uniformly distributed ﬁnd trans formation cauchy distribution given suppose uniformly distributed unit circle shown figure make change variables given show distributed according let dimensional random variable gaussian distribution zero mean unit covariance matrix suppose positive deﬁnite symmetric matrix cholesky decomposition lower triangular matrix one zeros leading diagonal show variable gaussian distribution mean covariance provides technique generating samples general multivariate gaussian using samples univariate gaussian zero mean unit variance exercise show carefully rejection sampling indeed draw samples desired distribution suppose proposal distributionshow probability sample value accepted given unnormalized distribution proportional constant set smallest value ensures values note probability drawing value given probability drawing value times probability accepting value given drawn make use along sum product rules probability write normalized form distribution show equals suppose uniform distribution interval show variable tan cauchy distribution given determine expressions coefﬁcients envelope distribution adaptive rejection sampling using requirements continuity malization making use technique discussed section sampling single exponential distribution devise algorithm sampling piecewise exponential distribution deﬁned show simple random walk integers deﬁned property hence induction exercises figure probability distribution two variables uniform shaded regions zero everywhere else show gibbs sampling algorithm discussed section satisﬁes detailed balance deﬁned consider distribution shown figure discuss whether standard gibbs sampling procedure distribution ergodic therefore whether would sample correctly distribution consider simple node graph shown figure observed node given gaussian distribution mean precision suppose marginal distributions mean precision given gam gam denotes gamma distribution write expressions conditional distributions would required order apply gibbs sampling posterior distribution verify relaxation update mean variance zero mean unit variance gives value mean variance using show hamiltonian equation equivalent similarly using show equivalent making use show conditional distributiongaussian figure graph involving observed gaussian variable prior distributions mean precision sampling methods verify two probabilities equal hence detailed balance holds hybrid monte carlo algorithm 
"
210,"[combining, models]"," earlier chapters explored range different models solving classiﬁ cation regression problems often found improved performance obtained combining multiple models together way instead using single model isolation instance might train different models make predictions using average predictions made model combinations models sometimes called committees section discuss ways apply committee concept practice also give insight sometimes effective procedure one important variant committee method known boosting involves training multiple models sequence error function used train particular model depends performance previous models produce substantial improvements performance compared use single model discussed section instead averaging predictions set models alternative form combining models model combination select one models make prediction choice model function input variables thus different models come responsible making predictions different regions input space one widely used framework kind known decision tree selection process described sequence binary selections corresponding traversal tree structure discussed section case individual models generally chosen simple overall ﬂexibility model arises input dependent selection process decision trees applied classiﬁcation regression problems one limitation decision trees division input space based hard splits one model responsible making predictions given value input variables decision process softened moving probabilistic framework combining models discussed section example set models conditional distribution input variable target variable indexes model form probabilistic mixture form represent input dependent mixing coefﬁcients models viewed mixture distributions component densities well mixing coefﬁcients conditioned input variables known mixtures experts closely related mixture density network model discussed section 
"
211,"[combining, models, bayesian, model, averaging]"," important distinguish model combination methods bayesian model averaging two often confused understand difference consider example density estimation using mixture gaussians several section gaussian components combined probabilistically model contains binary latent variable indicates component mixture responsible generating corresponding data point thus model speciﬁed terms joint distribution corresponding density observed variable obtained marginal izing latent variable committees case gaussian mixture example leads distribution form usual interpretation symbols example model combi nation independent identically distributed data use write marginal probability data set form thus see observed data point corresponding latent variable suppose several different models indexed prior probabilities instance one model might mixture gaussians another model might mixture cauchy distributions marginal distribution data set given example bayesian model averaging interpretation summation one model responsible generating whole data set probability distribution simply reﬂects uncertainty model size data set increases uncertainty reduces posterior probabilities become increasingly focussed one models highlights key difference bayesian model averaging model combination bayesian model averaging whole data set generated single model contrast combine multiple models see different data points within data set potentially generated different values latent variable hence different components although considered marginal probability considerations apply predictive density conditional distributions exercise 
"
212,"[combining, models, committees]"," simplest way construct committee average predictions set individual models procedure motivated frequentist perspective considering trade bias variance decomposes section ror due model bias component arises differences model true function predicted variance component repre sents sensitivity model individual data points recall figure combining models trained multiple polynomials using sinusoidal data aver aged resulting functions contribution arising variance term tended cancel leading improved predictions averaged set low bias models corresponding higher order polynomials obtained accurate predictions underlying sinusoidal function data generated practice course single data set ﬁnd way introduce variability different models within committee one approach use bootstrap datasets discussed section consider regression problem trying predict value single continuous variable suppose generate bootstrap datasets use train separate copy predictive model committee prediction given procedure known bootstrap aggregation bagging breiman suppose true regression function trying predict given output models written true value plus error form average sum squares error takes form denotes frequentist expectation respect distribution input vector average error made models acting individually therefore similarly expected error committee given assume errors zero mean uncorrelated boosting obtain exercise apparently dramatic result suggests average error model reduced factor simply averaging versions model unfortunately depends key assumption errors due individual models uncorrelated practice errors typically highly correlated reduction overall error generally small however shown expected committee error exceed expected error constituent models order achieve signiﬁcant improvements turn exercise sophisticated technique building committees known boosting 
"
213,"[combining, models, boosting]"," boosting powerful technique combining multiple base classiﬁers produce form committee whose performance signiﬁcantly better base classiﬁers describe widely used form boosting algorithm called adaboost short adaptive boosting developed freund schapire boosting give good results even base classiﬁers performance slightly better random hence sometimes base classiﬁers known weak learners originally designed solving classiﬁcation problems boosting also extended regression friedman principal difference boosting committee methods bagging discussed base classiﬁers trained sequence base classiﬁer trained using weighted form data set weighting coefﬁcient associated data point depends performance previous classiﬁers particular points misclassiﬁed one base classiﬁers given greater weight used train next classiﬁer sequence classiﬁers trained predictions combined weighted majority voting scheme illustrated schematically figure consider two class classiﬁcation problem training data comprises input vectors along corresponding binary target variables data point given associated weighting parameter initially set data points shall suppose procedure available training base classiﬁer using weighted data give function stage algorithm adaboost trains new classiﬁer using data set weighting coefﬁcients adjusted according performance previously trained classiﬁer give greater weight misclassiﬁed data points finally desired number base classiﬁers trained combined form committee using coefﬁcients give different weight different base classiﬁers precise form adaboost algorithm given combining models figure base classiﬁer trained depend red arrows sign adaboost initialize data weighting coefﬁcients setting fit classiﬁer training data minimizing weighted error function indicator function equals otherwise evaluate quantities use evaluate update data weighting coefﬁcients exp boosting make predictions using ﬁnal model given sign see ﬁrst base classiﬁer trained using weighting coefﬁcients equal therefore corresponds usual procedure training single classiﬁer see subsequent iterations weighting coefﬁcients increased data points misclassiﬁed decreased data points correctly classiﬁed successive classiﬁers therefore forced place greater emphasis points misclassiﬁed previous classiﬁers data points continue misclassiﬁed successive classiﬁers receive ever greater weight quantities represent weighted mea sures error rates base classiﬁers data set therefore see weighting coefﬁcients deﬁned give greater weight accurate classiﬁers computing overall output given adaboost algorithm illustrated figure using subset data points taken toy classiﬁcation data set shown figure base learners consists threshold one input variables simple classiﬁer corresponds form decision tree known decision stumps deci sectionsion tree single node thus base learner classiﬁes input according whether one input features exceeds threshold therefore simply partitions space two regions separated linear decision surface parallel one axes 
"
214,"[combining, models, boosting, minimizing, exponential, error]"," boosting originally motivated using statistical learning theory leading upper bounds generalization error however bounds turn loose practical value actual performance boosting much better bounds alone would suggest friedman gave different simple interpretation boosting terms sequential minimization exponential error function consider exponential error function deﬁned exp classiﬁer deﬁned terms linear combination base classiﬁers form training set target values goal minimize respect weighting coefﬁcients parameters base classiﬁers combining models figure illustration boosting base learners consist simple thresholds applied one axes ﬁgure shows number base learners trained far along decision boundary recent base learner dashed black line combined decision boundary semble solid green line data point depicted circle whose radius indicates weight assigned data point training recently added base learner thus instance see points misclassiﬁed base learner given greater weight training base learner instead global error function minimization however shall suppose base classiﬁers ﬁxed coefﬁcients minimizing respect sep arating contribution base classiﬁer write error function form exponent coefﬁcients exp viewed constants optimizing denote set data points correctly classiﬁed denote remaining misclassiﬁed points turn rewrite error function boosting form minimize respect see second term con stant equivalent minimizing overall multiplica tive factor front summation affect location minimum similarly minimizing respect obtain deﬁned exercise see found weights data points updated using exp making use fact see weights updated next iteration using exponent term exp independent see weights data points factor discarded thus obtain finally base classiﬁers trained new data points classiﬁed evaluating sign combined function deﬁned according factor affect sign omitted giving 
"
215,"[combining, models, boosting, error, functions, boosting]"," exponential error function minimized adaboost algorithm differs considered previous chapters gain insight nature exponential error function ﬁrst consider expected error given exponent perform variational minimization respect possible functions obtain exercise combining models figure plot exponential green rescaled cross entropy red error functions along hinge ror blue used support vector machines misclassiﬁcation error black note large negative values cross entropy gives linearly creasing penalty whereas exponential loss gives exponentially creasing penalty half log odds thus adaboost algorithm seeking best approx imation log odds ratio within space functions represented linear combination base classiﬁers subject constrained minimization resulting sequential optimization strategy result motivates use sign function arrive ﬁnal classiﬁcation decision already seen minimizer cross entropy error two class classiﬁcation given posterior class probability case target variable seen error function given section exp compared exponential error function figure divided cross entropy error constant factor passes point ease comparison see seen continuous approximations ideal misclassiﬁcation error function advantage exponential error sequential minimization leads simple adaboost scheme one drawback however penalizes large negative values much strongly cross entropy particular see large negative values cross entropy grows linearly whereas exponential error function grows exponentially thus exponential error function much less robust outliers misclassiﬁed data points another important difference cross entropy exponential ror function latter cannot interpreted log likelihood function well deﬁned probabilistic model furthermore exponential error exercise generalize classiﬁcation problems classes contrast cross entropy probabilistic model easily generalized give section interpretation boosting sequential optimization additive model exponential error friedman opens door wide range boosting like algorithms including multiclass extensions altering choice error function also motivates extension regression problems friedman consider sum squares error function regression sequential minimization additive model form simply involves ﬁtting new base classiﬁer residual errors previous model exercise noted however sum squares error robust outliers tree based models figure comparison squared error green absolute error red showing latter places much less emphasis large errors hence robust outliers mislabelled data points addressed basing boosting algorithm absolute deviation instead two error functions compared figure 
"
216,"[combining, models, tree-based, models]"," various simple widely used models work partitioning input space cuboid regions whose edges aligned axes assigning simple model example constant region viewed model combination method one model responsible making predictions given point input space process selecting speciﬁc model given new input described sequential decision making process corresponding traversal binary tree one splits two branches node focus particular tree based framework called classiﬁcation regression trees cart breiman although many variants going names quinlan quinlan figure shows illustration recursive binary partitioning input space along corresponding tree structure example ﬁrst step figure illustration two dimensional put space partitioned ﬁve regions using axis aligned boundaries combining models figure binary tree corresponding partitioning input space shown figure divides whole input space two regions according whether parameter model creates two subregions subdivided independently instance region subdivided according whether giving rise regions denoted recursive subdivision described traversal binary tree shown figure new input determine region falls starting top tree root node following path speciﬁc leaf node according decision criteria node note decision trees probabilistic graphical models within region separate model predict target variable instance regression might simply predict constant region classiﬁcation might assign region speciﬁc class key property tree based models makes popular ﬁelds medical diagnosis example readily interpretable humans correspond sequence binary decisions applied individual input variables stance predict patient disease might ﬁrst ask temperature greater threshold answer yes might next ask blood pressure less threshold leaf tree associated speciﬁc diagnosis order learn model training set determine structure tree including input variable chosen node form split criterion well value threshold parameter split also determine values predictive variable within region consider ﬁrst regression problem goal predict single target variable dimensional vector input variables training data consists input vectors along corresponding continuous labels partitioning input space given minimize sum squares error function optimal value predictive variable within given region given average values data points fall region exercise consider determine structure decision tree even ﬁxed number nodes tree problem determining optimal structure including choice input variable split well corresponding thresh tree based models olds minimize sum squares error usually computationally infeasible due combinatorially large number possible solutions instead greedy opti mization generally done starting single root node corresponding whole input space growing tree adding nodes one time step number candidate regions input space split corresponding addition pair leaf nodes existing tree choice input variables split well value threshold joint optimization choice region split choice input variable threshold done efﬁciently exhaustive search noting given choice split variable threshold optimal choice predictive variable given local average data noted earlier repeated possible choices variable split one gives smallest residual sum squares error retained given greedy strategy growing tree remains issue stop adding nodes simple approach would stop reduction residual error falls threshold however found empirically often none available splits produces signiﬁcant reduction error yet several splits substantial error reduction found reason common practice grow large tree using stopping criterion based number data points associated leaf nodes prune back resulting tree pruning based criterion balances residual error measure model complexity denote starting tree pruning deﬁne subtree obtained pruning nodes words collapsing internal nodes combining corresponding regions suppose leaf nodes indexed leaf node representing region input space data points denoting total number leaf nodes optimal prediction region given corresponding contribution residual sum squares pruning criterion given regularization parameter determines trade overall residual sum squares error complexity model measured number leaf nodes value chosen cross validation classiﬁcation problems process growing pruning tree sim ilar except sum squares error replaced appropriate measure combining models performance deﬁne proportion data points region assigned class two commonly used choices cross entropy gini index vanish maximum encourage formation regions high proportion data points assigned one class cross entropy gini index better measures misclassiﬁcation rate growing tree sensitive node probabilities also unlike misclassiﬁcation rate differentiable exercise hence better suited gradient based optimization methods subsequent pruning tree misclassiﬁcation rate generally used human interpretability tree model cart often seen major strength however practice found particular tree structure learned sensitive details data set small change training data result different set splits hastie problems tree based methods kind considered section one splits aligned axes feature space may suboptimal instance separate two classes whose optimal decision boundary runs degrees axes would need large number axis parallel splits input space compared single non axis aligned split furthermore splits decision tree hard region input space associated one one leaf node model last issue particularly problematic regression typically aiming model smooth functions yet tree model produces piecewise constant predictions discontinuities split boundaries 
"
217,"[combining, models, conditional, mixture, models]"," seen standard decision trees restricted hard axis aligned splits input space constraints relaxed expense interpretability allowing soft probabilistic splits functions input variables one time also give leaf models probabilistic inter pretation arrive fully probabilistic tree based model called hierarchical mixture experts consider section alternative way motivate hierarchical mixture experts model start standard probabilistic mixtures unconditional density models gaussians replace component densities conditional distributions chapter consider mixtures linear regression models section mixtures conditional mixture models logistic regression models section simplest case mixing coefﬁcients independent input variables make generalization allow mixing coefﬁcients also depend inputs obtain mixture experts model finally allow component mixture model mixture experts model obtain hierarchical mixture experts 
"
218,"[combining, models, conditional, mixture, models, mixtures, linear, regression, models]"," one many advantages giving probabilistic interpretation linear regression model used component complex probabilistic models done instance viewing conditional distribution representing linear regression model node directed prob abilistic graph consider simple example corresponding mixture linear regression models represents straightforward extension gaussian mixture model discussed section case conditional gaussian distributions therefore consider linear regression models governed weight parameter many applications appropriate use common noise variance governed precision parameter components case consider restrict attention single target variable though extension multiple outputs straightforward denote exercise mixing coefﬁcients mixture distribution written denotes set adaptive parameters model namely log likelihood function model given data set observations takes form denotes vector target variables order maximize likelihood function appeal algorithm turn simple extension algorithm unconditional gaussian mixtures section therefore build expe rience unconditional mixture introduce set binary latent variables data point elements zero except single value indicating component mixture responsible generating data point joint distribution latent observed variables represented graphical model shown figure complete data log likelihood function takes form exercise combining models figure probabilistic directed graph representing mixture linear regression models deﬁned algorithm begins ﬁrst choosing initial value old model param old old complete data log likelihood takes form old step maximize function old respect keeping ﬁxed optimization respect mixing coefﬁcients need take account constraint done aid lagrange multiplier leading step estimation equation form exercise next consider maximization respect parameter vector linear regression model substituting gaussian distribution see function old function parameter vector takes form old const constant term includes contributions weight vectors note quantity maximizing similar negative represents weighted least squares conditional mixture models problem term corresponding data point carries weighting coefﬁcient given could interpreted effective precision data point see component linear regression model mixture governed parameter vector ﬁtted separately whole data set step data point weighted responsibility model takes data point setting derivative respect equal zero gives write matrix notation diag diagonal matrix size solving obtain represents set modiﬁed normal equations corresponding weighted least squares problem form found context logistic regression note step matrix change solve normal equations afresh subsequent step finally maximize old respect keeping terms depend function old written old setting derivative respect equal zero rearranging obtain step equation form figure illustrate algorithm using simple example ﬁtting mixture two straight lines data set one input variable one target variable predictive density plotted figure using converged parameter values obtained algorithm corresponding right hand plot figure also shown ﬁgure result ﬁtting single linear regression model gives unimodal predictive density see mixture model gives much better representation data distribution reﬂected higher likelihood value however mixture model also assigns signiﬁcant probability mass regions data predictive distribution bimodal values problem resolved extending model allow mixture coefﬁcients functions leading models mixture density networks discussed section hierarchical mixture experts discussed section combining models figure example synthetic data set shown green points one input variable one target variable together mixture two linear regression models whose mean functions shown blue red lines upper three plots show initial conﬁguration left result running iterations centre result iterations right initialized reciprocal true variance set target values lower three plots show corresponding responsibilities plotted vertical line data point length blue segment gives posterior probability blue line data point similarly red segment 
"
219,"[combining, models, conditional, mixture, models, mixtures, logistic, models]"," logistic regression model deﬁnes conditional distribution conditional distribution target variable probabilistic mixture logistic regression models given feature vector output component denotes adjustable parameters namely suppose given data set corresponding likelihood conditional mixture models figure left plot shows predictive conditional density corresponding converged solution figure gives log likelihood value vertical slice one plots particular value represents corresponding conditional distribution see bimodal plot right shows predictive density single linear regression model ﬁtted data set using maximum likelihood model smaller log likelihood function given maximize likelihood function iteratively making use algorithm involves introducing latent variables correspond coded binary indicator variable data point complete data likelihood function given matrix latent variables elements initialize algorithm choosing initial value old model parameters step use parameter values evaluate posterior probabilities components data point given old responsibilities used ﬁnd expected complete data log likelihood function given old combining models step involves maximization function respect keeping old hence ﬁxed maximization respect done usual way lagrange multiplier enforce summation constraint giving familiar result determine note old function comprises sum terms indexed depends one vectors different vectors decoupled step algorithm words different components interact via responsibilities ﬁxed step note step closed form solution must solved iteratively using instance iterative reweighted least squares irls algorithm gradient hessian vector given section denotes gradient respect ﬁxed indepen dent solve separately using irls algorithm thus step equations component correspond simply ﬁtting section single logistic regression model weighted data set data point carries weight figure shows example mixture logistic regression models applied simple classiﬁcation problem extension model mixture softmax models two classes straightforward exercise 
"
220,"[combining, models, conditional, mixture, models, mixtures, experts]"," section considered mixture linear regression models section discussed analogous mixture linear classiﬁers although simple mixtures extend ﬂexibility linear models include complex multimodal predictive distributions still limited increase capability models allowing mixing coefﬁcients functions input variable known mixture experts model jacobs mixing coefﬁcients known gating functions individual component densities called experts notion behind terminology different components model distribution different regions input space conditional mixture models figure illustration mixture logistic regression models left plot shows data points drawn two classes denoted red blue background colour varies pure red pure blue denotes true probability class label centre plot shows result ﬁtting single logistic regression model using maximum likelihood background colour denotes corresponding probability class label colour near uniform purple see model assigns probability around classes input space right plot shows result ﬁtting mixture two logistic regression models gives much higher probability correct labels many points blue class experts making predictions regions gating functions determine components dominant region gating functions must satisfy usual constraints mixing efﬁcients namely therefore represented example linear softmax models form experts also linear regression classiﬁcation models whole model ﬁtted efﬁciently using algorithm iterative reweighted least squares employed step jordan jacobs model still signiﬁcant limitations due use linear models gating expert functions much ﬂexible model obtained using multilevel gating function give hierarchical mixture experts hme model jordan jacobs understand structure model imagine mixture distribution component mixture mixture distribution simple unconditional mixtures hierarchical mixture trivially equivalent single ﬂat mixture distribution however mixing exercise coefﬁcients input dependent hierarchical model becomes nontrivial hme model also viewed probabilistic version decision trees discussed section trained efﬁciently maximum likelihood using algorithm irls step bayesian treatment hme section given bishop svens based variational inference shall discuss hme detail however worth pointing close connection mixture density network discussed section principal advantage mixtures experts model optimized step mixture component gating model involves convex optimization although overall optimization nonconvex con trast advantage mixture density network approach component combining models densities mixing coefﬁcients share hidden units neural network furthermore mixture density network splits input space relaxed compared hierarchical mixture experts soft constrained axis aligned also nonlinear 
"
221,"[combining, models, exercises]"," consider set models form input vector target vector indexes different models latent variable model set parameters model suppose models prior probabilities given training set write formulae needed evaluate predic tive distribution latent variables model index marginalized use formulae highlight difference bayesian averaging different models use latent variables within single model expected sum squares error simple committee model deﬁned expected error committee given assuming individual errors satisfy derive result making use jensen inequality special case convex function show average expected sum squares error members simple committee model given expected error committee given satisfy making use jensen equality show result derived previous exercise hods error function sum squares provided convex function consider committee allow unequal weighting constituent models order ensure predictions remain within sensible limits suppose require bounded value minimum maximum values given members committee min max show necessary sufﬁcient condition constraint coefﬁcients satisfy exercises differentiating error function respect show parameters adaboost algorithm updated using deﬁned making variational minimization expected exponential error function given respect possible functions show minimizing function given show exponential error function minimized adaboost algorithm correspond log likelihood well behaved probabilistic model done showing corresponding conditional distribution cannot correctly normalized show sequential minimization sum squares error function additive model form style boosting simply involves ﬁtting new base classiﬁer residual errors previous model verify minimize sum squares error set training values single predictive value optimal solution given mean consider data set comprising data points class data points class suppose tree model splits ﬁrst leaf node second leaf node denotes points assigned points assigned similarly suppose second tree model splits evaluate misclassiﬁcation rates two trees hence show equal similarly evaluate cross entropy gini index two trees show lower tree tree extend results section mixture linear regression models case multiple target values described vector make use results section verify complete data log likelihood function mixture linear regression models given use technique lagrange multipliers appendix show step estimation equation mixing coefﬁcients mixture linear regression models trained maximum likelihood given already noted use squared loss function regression problem corresponding optimal prediction target variable new input vector given conditional mean predictive distribution show conditional mean mixture linear regression models discussed section given linear combination means component distributionnote conditional distribution target data multimodal conditional mean give poor predictions combining models extend logistic regression mixture model section mixture softmax classiﬁers representing classes write algorithm determining parameters model maximum likelihood consider mixture model conditional distribution form mixture component mixture model show two level hierarchical mixture equivalent conventional single level mixture model suppose mixing coefﬁcients levels hierar chical model arbitrary functions show hierarchical model equivalent single level model dependent mixing coefﬁcients finally consider case mixing coefﬁcients levels hierarchical mixture constrained linear classiﬁcation logistic softmax models show hierarchical mixture cannot general represented single level mixture linear classiﬁcation models mixing coefﬁcients hint sufﬁcient construct single counter example consider mixture two components one components mixture two components mixing coefﬁcients given linear logistic models show cannot represented single level mixture components mixing coefﬁcients determined linear softmax model appendix give brief introduction datasets used illustrate algorithms described book detailed information ﬁle formats datasets well data ﬁles obtained book website 
"
222,"[combining, models, handwritten, digits]"," digits data used book taken mnist data set lecun constructed modifying subset much larger data set produced nist national institute standards technology comprises training set examples test set examples data collected census bureau employees rest collected high school children care taken ensure test examples written different individuals training examples original nist data binary black white pixels create mnist images size normalized pixel box preserving aspect ratio consequence anti aliasing used change resolution images resulting mnist digits grey scale images centred box examples mnist digits shown figure error rates classifying digits range simple linear classi ﬁer carefully designed support vector machine convolutional neural network lecun datasets figure one hundred examples mnist digits chosen random training set 
"
223,"[combining, models, oil, flow]"," synthetic data set arose project aimed measuring nonin vasively proportions oil water gas north sea oil transfer pipelines bishop james based principle dual energy gamma densit ometry ideas narrow beam gamma rays passed pipe attenuation intensity beam provides information density material along path thus instance beam attenuated strongly oil gas single attenuation measurement alone sufﬁcient two degrees freedom corresponding fraction oil fraction water fraction gas redundant three fractions must add one address two gamma beams different energies words different frequencies wavelengths passed pipe along path attenuation measured absorbtion properties different materials vary dif ferently function energy measurement attenuations two energies provides two independent pieces information given known absorbtion prop erties oil water gas two energies simple matter calculate average fractions oil water hence gas measured along path gamma beams complication however associated motion terials along pipe ﬂow velocity small oil ﬂoats top water gas sitting oil known laminar stratiﬁed datasets figure three geometrical conﬁgurations oil water gas phases used generate oil ﬂow data set conﬁguration pro portions three phases vary mix gas water oil homogeneous stratiﬁed annular ﬂow conﬁguration illustrated figure ﬂow velocity increased complex geometrical conﬁgurations oil water gas arise purposes data set two speciﬁc idealizations considered annular conﬁguration oil water gas form concentric cylinders water around outside gas centre whereas homogeneous conﬁguration oil water gas assumed intimately mixed might occur high ﬂow velocities turbulent conditions conﬁgurations also illustrated figure seen single dual energy beam gives oil water fractions measured along path length whereas interested volume fractions oil water addressed using multiple dual energy gamma densit ometers whose beams pass different regions pipe particular data set six beams spatial arrangement shown figure single observation therefore represented dimensional vector comprising fractions oil water measured along paths beams however interested obtaining overall volume fractions three phases pipe much like classical problem tomographic construction used medical imaging example two dimensional dis figure cross section pipe showing arrangement six beam lines comprises single dual energy gamma densitometer note vertical beams asymmetrically arranged relative central axis shown dotted line datasets tribution reconstructed number one dimensional averages far fewer line measurements typical tomography application hand range geometrical conﬁgurations much limited conﬁguration well phase fractions predicted reasonable accuracy densitometer data safety reasons intensity gamma beams kept relatively weak obtain accurate measurement attenuation measured beam intensity integrated speciﬁc time interval ﬁnite integration time random ﬂuctuations measured intensity due fact gamma beams comprise discrete packets energy called photons practice integration time chosen compromise reducing noise level requires long integration time detecting temporal variations ﬂow requires short integration time oil ﬂow data set generated using realistic known values absorption properties oil water gas two gamma energies used speciﬁc choice integration time seconds chosen characteristic typical practical setup point data set generated independently using following steps choose one three phase conﬁgurations random equal probability choose three random numbers uniform distribution deﬁne oil water treats three phases equal footing ensures volume fractions add one six beam lines calculate effective path lengths oil water given phase conﬁguration perturb path lengths using poisson distribution based known beam intensities integration time allow effect photon statistics point data set comprises path length measurements together fractions oil water binary label describing phase conﬁguration data set divided training validation test sets comprises independent data points details data format available book website bishop james statistical machine learning techniques used predict volume fractions also geometrical conﬁguration phases shown figure dimensional vector measurements dimensional observation vectors also used test data visualization algo rithms data set rich interesting structure follows given conﬁguration two degrees freedom corresponding fractions datasets oil water inﬁnite integration time data locally live two dimensional manifold ﬁnite integration time individual data points perturbed away manifold photon noise homogeneous phase conﬁguration path lengths oil water linearly related fractions oil water data points lie close linear manifold annular conﬁguration relationship phase fraction path length nonlinear manifold nonlinear case laminar conﬁguration situation even complex small variations phase fractions cause one horizontal phase boundaries move across one horizontal beam lines leading discontinuous jump dimensional observation space way two dimensional nonlinear manifold laminar conﬁguration broken six distinct segments note also manifolds different phase conﬁgurations meet speciﬁc points example pipe ﬁlled entirely oil corresponds speciﬁc instances laminar annular homoge neous conﬁgurations 
"
224,"[combining, models, old, faithful]"," old faithful shown figure hydrothermal geyser yellowstone national park state wyoming popular tourist attraction name stems supposed regularity eruptions data set comprises observations represents single eruption contains two variables corresponding duration minutes eruption time next eruption also minutes figure shows plot time next eruption versus duration eruptions seen time next eruption varies considerably although knowledge duration current eruption allows predicted accurately note exist several datasets relating eruptions old faithful figure old faithful geyser yellowstone national park bruce gourley brucegourley datasets figure plot time next eruption minutes vertical axis versus duration eruption minutes horizontal axis old faithful data set 
"
225,"[combining, models, synthetic, data]"," throughout book use two simple synthetic datasets illustrate many algorithms ﬁrst regression problem based sinusoidal function shown figure input values generated uniformly range corresponding target values obtained ﬁrst computing corresponding values function sin adding random noise gaussian distribution standard deviation various forms data set different numbers data points used book second data set classiﬁcation problem two classes equal prior probabilities shown figure blue class generated single gaussian red class comes mixture two gaussians cause know class priors class conditional densities straightforward evaluate plot true posterior probabilities well minimum misclassiﬁcation rate decision boundary shown figure datasets figure left hand plot shows synthetic regression data set along underlying sinusoidal function data points generated right hand plot shows true conditional distribution labels generated green curve denotes mean shaded region spans one standard deviation side mean figure left plot shows synthetic classiﬁcation data set data two classes shown red blue right plot true posterior probabilities shown colour scale going pure red denoting probability red class pure blue denoting probability red class probabilities known optimal decision boundary minimizing misclassiﬁcation rate corresponds contour along posterior probabilities class equal evaluated shown green curve decision boundary also plotted left hand ﬁgure appendix summarize main properties widely used probability distributions distribution list key statistics expectation variance covariance mode entropy distributions members exponential family widely used building blocks sophisticated probabilistic models 
"
226,"[combining, models, bernoulli]"," distribution single binary variable representing example result ﬂipping coin governed single continuous parameter represents probability bern var mode otherwise bernoulli special case binomial distribution case single observation conjugate prior beta distribution probability distributions 
"
227,"[combining, models, beta]"," distribution continuous variable often used represent probability binary event governed two parameters constrained ensure distribution normalized beta var mode beta conjugate prior bernoulli distribution interpreted effective prior number observations respectively density ﬁnite otherwise singularity reduces uniform distribution beta distribution special case state dirichlet distribution 
"
228,"[combining, models, binomial]"," binomial distribution gives probability observing occurrences set samples bernoulli distribution probability observing bin var mode denotes largest integer less equal quantity denotes number ways choosing objects total identical objects pronounced factorial denotes product particular case binomial distribution known bernoulli distribution large binomial distribution approximately gaussian conjugate prior beta distribution probability distributions 
"
229,"[combining, models, dirichlet]"," dirichlet multivariate distribution random variables subject constraints denoting dir var covariance mode known digamma function abramowitz stegun parameters subject constraint order ensure distribution normalized dirichlet forms conjugate prior multinomial distribution rep resents generalization beta distribution case parameters interpreted effective numbers observations corresponding values dimensional binary observation vector beta distribution dirichlet ﬁnite density everywhere provided probability distributions 
"
230,"[combining, models, gamma]"," gamma probability distribution positive random variable governed parameters subject constraints ensure distribution normalized gam var mode digamma function deﬁned gamma distribution conjugate prior precision inverse variance univariate gaussian density everywhere ﬁnite special case known exponential distribution 
"
231,"[combining, models, gaussian]"," gaussian widely used distribution continuous variables also known normal distribution case single variable governed two parameters mean variance exp var mode inverse variance called precision square root variance called standard deviation conjugate prior gaussian conjugate prior gamma distribution unknown joint conjugate prior gaussian gamma distribution dimensional vector gaussian governed dimensional mean vector covariance matrix must symmetric probability distributions positive deﬁnite exp covariance mode inverse covariance matrix precision matrix also symmetric positive deﬁnite averages random variables tend gaussian central limit theorem sum two gaussian variables gaussian gaussian distribution maximizes entropy given variance covariance linear transformation gaussian random variable gaussian marginal distribution multivariate gaussian respect subset variables gaussian similarly conditional distribution also gaussian conjugate prior gaussian conjugate prior wishart conjugate prior gaussian wishart marginal gaussian distribution conditional gaussian distribution given form marginal distribution conditional distribution given given joint gaussian distribution deﬁne following partitions conditional distribution given probability distributions marginal distribution given 
"
232,"[combining, models, gaussian-gamma]"," conjugate prior distribution univariate gaussian mean precision unknown also called normal gamma distribution comprises product gaussian distribution whose precision proportional gamma distribution gam 
"
233,"[combining, models, gaussian-wishart]"," conjugate prior distribution multivariate gaussian mean precision unknown also called normal wishart distribution comprises product gaussian distribution whose precision proportional wishart distribution particular case scalar equivalent gaussian gamma distri bution 
"
234,"[combining, models, multinomial]"," generalize bernoulli distribution dimensional binary variable components obtain following discrete distribution var covariance probability distributions element identity matrix parameters must satisfy multinomial distribution multivariate generalization binomial gives distribution counts state discrete variable state given total number observations mult var covariance quantity gives number ways taking identical objects assigning bin value gives probability random variable taking state parameters subject constraints conjugate prior distribution parameters dirichlet 
"
235,"[combining, models, normal]"," normal distribution simply another name gaussian book use term gaussian throughout although retain conventional use symbol denote distribution consistency shall refer normal gamma distribution gaussian gamma distribution similarly normal wishart called gaussian wishart 
"
236,"[combining, models, student’s]"," distribution published william gosset employer guiness breweries required publish pseudonym chose student univariate form student distribution obtained placing conjugate gamma prior precision univariate gaussian distribution inte grating precision variable therefore viewed inﬁnite mixture probability distributions gaussians mean different variances var mode called number degrees freedom distribution particular case called cauchy distribution dimensional variable student distribution corresponds marginal izing precision matrix multivariate gaussian respect conjugate wishart prior takes form covariance mode squared mahalanobis distance deﬁned limit distribution reduces gaussian mean pre decisionstudent distribution provides generalization gaussian whose maximum likelihood parameter values robust outliers 
"
237,"[combining, models, uniform]"," simple distribution continuous variable deﬁned ﬁnite interval var distribution distribution probability distributions 
"
238,"[combining, models, von, mises]"," von mises distribution also known circular normal circular gaussian univariate gaussian like periodic distribution variable exp cos zeroth order bessel function ﬁrst kind distribution period care must taken interpreting distribution simple expectations dependent arbitrary choice origin variable parameter analogous mean univariate gaussian parameter known concentration parameter analogous precision inverse variance large von mises distribution approximately gaussian centred 
"
239,"[combining, models, wishart]"," wishart distribution conjugate prior precision matrix multi variate gaussian exp symmetric positive deﬁnite matrix digamma function deﬁned parameter called number degrees freedom distribution restricted ensure gamma function normalization factor well deﬁned one dimension wishart reduces gamma distribution gam given parameters appendix gather together useful properties identities involving matrices determinants intended introductory tutorial assumed reader already familiar basic linear algebra results indicate prove whereas complex cases leave interested reader refer standard textbooks subject cases assume inverses exist matrix dimensions formulae correctly deﬁned comprehensive discussion linear algebra found golub van loan extensive collection matrix properties given utkepohl matrix derivatives discussed magnus neudecker 
"
240,"[combining, models, basic, matrix, identities]"," matrix elements indexes rows indexes columns use denote identity matrix also called unit matrix ambiguity dimensionality simply use transpose matrix elements deﬁnition transpose veriﬁed writing indices inverse denoted satisﬁes abb also properties matrices easily proven taking transpose applying useful identity involving matrix inverses following bpb easily veriﬁed right multiplying sides bpb suppose dimensionality dimensionality much cheaper evaluate right hand side left hand side special case sometimes arises another useful identity involving inverses following known woodbury identity veriﬁed multiplying sides useful instance large diagonal hence easy invert many rows columns conversely right hand side much cheaper evaluate left hand side set vectors said linearly independent relation holds implies none vectors expressed linear combination remainder rank matrix maximum number linearly independent rows equivalently maximum number linearly independent columns 
"
241,"[combining, models, traces, determinants]"," trace determinant apply square matrices trace matrix deﬁned sum elements leading diagonal writing indices see applying formula multiple times product three matrices see abc cab bca known cyclic property trace operator clearly extends product number matrices determinant matrix deﬁned sum taken products consisting precisely one element row one element column coefﬁcient according properties matrices whether permutation even odd respectively note thus matrix determinant takes form determinant product two matrices given shown also determinant inverse matrix given shown taking determinant applying matrices size useful special case dimensional column vectors 
"
242,"[combining, models, matrix, derivatives]"," sometimes need consider derivatives vectors matrices respect scalars derivative vector respect scalar vector whose components given analogous deﬁnition derivative matrix derivatives respect vectors matrices also deﬁned instance similarly following easily proven writing components properties matrices similarly derivative inverse matrix expressed shown differentiating equation using right multiplying also shall prove later choose one elements seen writing matrices using index notation write result compactly form notation following properties aba proven writing matrix indices also follows 
"
243,"[combining, models, eigenvector, equation]"," square matrix size eigenvector equation deﬁned properties matrices eigenvector corresponding eigenvalue viewed set simultaneous homogeneous linear equations condition solution known characteristic equation polynomial order must solutions though need distinct rank equal number nonzero eigenvalues particular interest symmetric matrices arise covariance trices kernel matrices hessians symmetric matrices property equivalently inverse symmetric matrix also symmetric seen taking transpose using together symmetry general eigenvalues matrix complex numbers symmetric matrices eigenvalues real seen ﬁrst left multiplying denotes complex conjugate give next take complex conjugate left multiply give used consider real matrices taking transpose second equations using see left hand sides two equations equal hence must real eigenvectors real symmetric matrix chosen orthonormal orthogonal unit length elements identity matrix show ﬁrst left multiply give hence exchange indices take transpose second equation make use symmetry property subtract two equations give hence hence orthogonal two eigenvalues equal linear combination also eigen vector eigenvalue select one linear combination arbitrarily properties matrices choose second orthogonal ﬁrst shown generate eigenvectors never linearly dependent hence eigenvectors chosen orthogonal normalizing set unit length eigenvalues corresponding orthogonal eigenvectors form complete set dimensional vector expressed linear combination eigenvectors take eigenvectors columns matrix orthonormality satisﬁes matrix said orthogonal interestingly rows matrix also orthogonal show note implies using also follows eigenvector equation expressed terms form diagonal matrix whose diagonal elements given eigenvalues consider column vector transformed orthogonal matrix give new vector length vector preserved similarly angle two vectors preserved thus multiplication interpreted rigid rotation coordinate system follows diagonal matrix say matrix diagonalized matrix left multiply right multiply obtain uλu taking inverse equation using together properties matrices last two equations also written form take determinant use obtain similarly taking trace using cyclic property trace operator together leave exercise reader verify making use results matrix said positive deﬁnite denoted values vector equivalently positive deﬁnite matrix eigenvalues seen setting eigenvectors turn noting arbitrary vector expanded linear combination eigenvectors note positive deﬁnite elements positive example matrix eigenvalues matrix said positive semidef inite holds values denoted equivalent think function operator input value returns output value way deﬁne functional operator takes function returns output value example functional length curve drawn two dimensional plane path curve deﬁned terms function context machine learning widely used functional entropy continuous variable choice probability density function returns scalar value representing entropy density thus entropy could equally well written common problem conventional calculus ﬁnd value maximizes minimizes function similarly calculus variations seek function maximizes minimizes functional possible functions wish ﬁnd particular function functional maximum minimum calculus variations used instance show shortest path two points straight line maximum entropy distribution gaussian familiar rules ordinary calculus could evaluate conventional derivative making small change variable expanding powers ﬁnally taking limit similarly function several variables corresponding partial derivatives deﬁned analogous deﬁnition functional derivative arises consider much functional changes make small change xcη function calculus variations figure functional derivative deﬁned considering value functional changes function changed xcη arbitrary function xcη arbitrary function illustrated figure denote functional derivative respect deﬁne following relation xcη seen natural extension depends continuous set variables namely values points requiring functional stationary respect small variations function gives must hold arbitrary choice follows functional derivative must vanish see imagine choosing perturbation zero everywhere except neighbourhood point case functional derivative must zero however must true every choice functional derivative must vanish values consider functional deﬁned integral function depends derivative well direct depen dence value assumed ﬁxed boundary region integration might inﬁnity consider variations function obtain xcη cast form integrate second term parts make use fact must vanish boundary integral ﬁxed boundary gives xcη calculus variations read functional derivative comparison requiring functional derivative vanishes gives known euler lagrange equations example euler lagrange equations take form second order differential equation solved making use boundary conditions often consider functionals deﬁned integrals whose integrands take form depend derivatives case station arity simply requires values optimizing functional respect probability distribution need maintain normalization constraint probabilities often conveniently done using lagrange multiplier allows uncon appendix strained optimization performed extension results multidimensional variable straight forward comprehensive discussion calculus variations see sagan lagrange multipliers also sometimes called undetermined multipliers used ﬁnd stationary points function several variables subject one constraints consider problem ﬁnding maximum function subject constraint relating write form one approach would solve constraint equation thus express function form substituted give function alone form maximum respect could found differentiation usual way give stationary value corresponding value given one problem approach may difﬁcult ﬁnd analytic solution constraint equation allows expressed explicit function also approach treats differently spoils natural symmetry variables elegant often simpler approach based introduction parameter called lagrange multiplier shall motivate technique geometrical perspective consider dimensional variable components constraint equation represents dimensional surface space indicated figure ﬁrst note point constraint surface gradient constraint function orthogonal surface see consider point lies constraint surface consider nearby point also lies surface make taylor expansion around lie constraint surface hence limit lagrange multipliers figure geometrical picture technique grange multipliers seek maximize function subject constraint dimensional constraint cor responds subspace dimensionality indicated red curve problem solved optimizing lagrangian function parallel constraint surface see vector normal surface next seek point constraint surface maximized point must property vector also orthogonal constraint surface illustrated figure otherwise could increase value moving short distance along constraint surface thus parallel anti parallel vectors must exist parameter known lagrange multiplier note either sign point convenient introduce lagrangian function deﬁned constrained stationarity condition obtained setting fur thermore condition leads constraint equation thus ﬁnd maximum function subject constraint deﬁne lagrangian function given ﬁnd stationary point respect dimensional vector gives equations determine stationary point value interested eliminate stationarity equations without needing ﬁnd value hence term undetermined multiplier simple example suppose wish ﬁnd stationary point function subject constraint illustrated figure corresponding lagrangian function given conditions lagrangian stationary respect give following coupled equations lagrange multipliers figure simple example use lagrange multipliers aim maximize subject constraint circles show contours function diagonal line shows constraint surface solution equations gives stationary point corresponding value lagrange multiplier far considered problem maximizing function subject illustrated figure two kinds solution possible according whether con corresponds however sign lagrange multiplier value either two cases product thus solution figure illustration problem maximizing subject inequality constraint lagrange multipliers problem maximizing subject obtained optimizing lagrange function respect subject conditions known karush kuhn tucker kkt conditions karush kuhn tucker note wish minimize rather maximize function sub ject inequality constraint minimize lagrangian function respect subject finally straightforward extend technique lagrange multipliers case multiple equality inequality constraints suppose wish maximize subject introduce lagrange multipliers optimize grangian function given subject extensions constrained functional derivatives similarly straightforward detailed discussion appendix technique lagrange multipliers see nocedal wright references abramowitz stegun handbook mathematical functions dover adler relaxation method monte carlo evaluation partition function multiquadratic actions physical review ahn constrained algorithm principal component analysis neural computation aizerman braverman rozo noer probability problem pattern recognition learning method potential functions automation remote control akaike new look statistical model identiﬁcation ieee transactions automatic control ali silvey general class coefﬁcients divergence one distribution another journal royal statistical ciety allwein schapire singer reducing multiclass binary unifying approach margin classiﬁers journal machine learning research amari differential geometrical methods statistics springer amari cichocki yang new learning algorithm blind signal separation touretzky mozer hasselmo eds advances neural information processing systems volume mit press amari natural gradient works efﬁciently learning neural computation anderson rosenfeld eds neurocomputing foundations research mit press anderson asymptotic theory prin cipal component analysis annals mathematical statistics andrieu freitas doucet jor dan introduction mcmc chine learning machine learning anthony biggs introduction computational learning theory cambridge university press attias independent factor analysis neural computation attias inferring parameters struc ture latent variable models variational bayes laskey prade eds references uncertainty artiﬁcial intelligence proceedings fifth conference morgan kaufmann bach jordan kernel independent component analysis journal machine learning research bakir weston learning ﬁnd pre images thrun saul eds advances neural information processing systems volume mit press baldi brunak bioinformatics machine learning approach second mit press baldi hornik neural networks principal component analysis learning examples without local minima neural net works barber bishop bayesian model comparison monte carlo chaining mozer jordan petsche eds vances neural information processing sys tems volume mit press barber bishop ensemble learning multi layer networks jor dan kearns solla eds vances neural information processing sys tems volume barber bishop ensemble learning bayesian neural networks bishop generalization neural networks machine learning springer bartholomew latent variable models factor analysis charles grifﬁn basilevsky statistical factor analysis related methods theory applications wiley bather decision theory introduction dynamic programming sequential decisions wiley baudat anouar generalized dis criminant analysis using kernel approach neural computation baum inequality associated maximization technique statistical estimation probabilistic functions markov processes inequalities becker improving con vergence back propagation learning sec ond order methods touretzky hin ton sejnowski eds proceedings connectionist models summer school morgan kaufmann bell sejnowski information maximization approach blind separation blind deconvolution neural computation bellman adaptive control processes guided tour princeton university press bengio frasconi input output hmm architecture tesauro touret zky leen eds advances neural information processing systems volume mit press bennett robust linear programming discrimination two linearly separable sets timization methods software berger statistical decision theory bayesian analysis second springer bernardo smith bayesian theory wiley berrou glavieux thitimajshima near shannon limit error correcting coding decoding turbo codes proceedings icc besag spatio temporal models markov ﬁelds transactions prague conference information theory statistical decision functions random processes academia besag statistical analysis dirty pictures journal royal statistical soci ety besag green hidgon megersen bayesian computation stochastic systems statistical science references bishop fast procedure retraining multilayer perceptron international journal neural systems bishop exact calculation hessian matrix multilayer perceptron neural computation bishop curvature driven smoothing learning algorithm feedforward networks ieee transactions neural networks bishop novelty detection neural network validation iee proceedings vision image signal processing special issue applications neural networks bishop neural networks pattern recognition oxford university press bishop training noise equiv alent tikhonov regularization neural compu tation bishop bayesian pca kearns solla cohn eds vances neural information processing sys tems volume mit press bishop variational principal components proceedings ninth international conference artiﬁcial neural networksicann volume iee bishop james analysis multiphase ﬂows using dual energy gamma den sitometry neural networks nuclear instru ments methods physics research bishop nabney modelling conditional probability distributions periodic variables neural computation bishop nabney pattern recognition machine learning matlab companion springer preparation bishop spiegelhalter winn vibes variational inference engine bayesian networks becker thrun obermeyer eds advances neural information processing systems volume mit press bishop svens bayesian erarchical mixtures experts kjaerulff meek eds proceedings nineteenth conference uncertainty artiﬁcial intelli gence morgan kaufmann bishop svens hinton distinguishing text graphics line handwritten ink kimura jisawa eds proceedings ninth international workshop frontiers handwriting recognition iwfhr tokyo japan bishop svens williams optimization latent variable density models touretzky mozer hasselmo eds advances neural information processing systems volume mit press bishop svens williams gtm principled alternative self organizing map mozer jor dan petche eds advances neural information processing systems volume mit press bishop svens williams magniﬁcation factors gtm gorithm proceedings iee fifth international conference artiﬁcial neural networks cam bridge institute electrical engineers bishop svens williams developments generative pographic mapping neurocomputing bishop svens williams gtm generative topographic mapping neural computation bishop tipping hier archical latent variable model data visualization ieee transactions pattern analysis machine intelligence references bishop winn non linear bayesian image modelling proceedings sixth european conference computer vision dublin volume springer blei jordan erarchical bayesian models applications information retrieval bayesian statistics oxford uni versity press block perceptron model brain functioning reviews modern physics reprinted anderson rosenfeld blum multidimensional stochastic proximation methods annals mathematical statistics bodlaender tourist guide treewidth acta cybernetica boser guyon vapnik training algorithm optimal margin classi ﬁers haussler proceedings fifth nual workshop computational learning ory colt acm bourlard kamp auto association multilayer perceptrons singular value composition biological cybernetics box jenkins reinsel time series analysis prentice hall box tao bayesian infer ence statistical analysis wiley boyd vandenberghe convex opti mization cambridge university press boyen koller tractable inference complex stochastic processes cooper moral eds proceedings annual conference uncertainty artiﬁcial intelli gence uai morgan kaufmann boykov veksler zabih fast approximate energy minimization via graph cuts ieee transactions pattern analysis chine intelligence breiman bagging predictors machine learning breiman friedman olshen stone classiﬁcation regression trees wadsworth brooks markov chain monte carlo method application statisti cian broomhead lowe multivariable functional interpolation adaptive net works complex systems buntine weigend bayesian back propagation complex systems buntine weigend computing second derivatives feed forward net works review ieee transactions neural networks burges tutorial support vector machines pattern recognition knowledge discovery data mining cardoso blind signal separation statis tical principles proceedings ieee casella berger statistical ference second duxbury castillo guti errez hadi expert systems probabilistic network models springer chan lee sejnowski variational bayesian learningica missing data neural computation chen hecht nielsen geometry feedforward neural network error surfaces neural computation chen shao ibrahim eds monte carlo methods bayesian computation springer chen cowan grant orthogonal least squares learning algorithm radial basis function networks ieee transactions neural networks references choudrey roberts variational mixture bayesian independent component alyzers neural computation clifford markov random ﬁelds statis tics grimmett welsh eds disorder physical systems volume hon john hammersley oxford university press collins dasgupta schapire generalization principal component analy sis exponential family dietterich becker ghahramani eds advances neural information processing systems vol ume mit press comon jutten herault blind source separation problems statement signal processing corduneanu bishop variational bayesian model selection mixture distributions richardson jaakkola eds proceedings eighth international confer ence artiﬁcial intelligence statistics morgan kaufmann cormen leiserson rivest stein introduction algorithms sec ond mit press cortes vapnik support vector networks machine learning cotter stone weierstrass theo rem application neural networks ieee transactions neural networks cover hart nearest neighbor pattern classiﬁcation ieee transactions information theory cover thomas elements information theory wiley cowell dawid lauritzen spiegelhalter probabilistic networks expert systems springer cox probability frequency reasonable expectation american journal physics cox cox multidimensional scaling second chapman hall cressie statistics spatial data wiley cristianini shawe taylor support vector machines kernel based learning methods cambridge university press csat opper sparse line gaussian processes neural computation csisz tusn ady information ometry alternating minimization procedures statistics decisions cybenko approximation superpositions sigmoidal function mathematics control signals systems dawid conditional independence statistical theory discussion journal royal statistical society series dawid conditional independence statistical operations annals statistics definetti theory probability wiley sons dempster laird rubin maximum likelihood incomplete data via algorithm journal royal statistical society denison holmes mallick smith bayesian methods nonlinear classiﬁcation regression wiley diaconis saloff coste know metropolis algorithm journal computer system sciences dietterich bakiri solving multiclass learning problems via error correcting output codes journal artiﬁcial intelligence research duane kennedy pendleton roweth hybrid monte carlo physics letters duda hart pattern classiﬁ cation scene analysis wiley references duda hart stork pattern classiﬁcation second wiley durbin eddy krogh mitchi son biological sequence analysis cam bridge university press dybowski roberts anthology probabilistic models medical informatics husmeier dybowski roberts eds probabilistic modeling bioinformatics medical informatics springer efron bootstrap methods another look jackknife annals statistics elkan using triangle inequality celerate means proceedings twelfth international conference machine learning aaai elliott aggoun moore hidden markov models estimation con trol springer ephraim malah juang application hidden markov models enhancing noisy speech ieee transactions acoustics speech signal processing erwin obermayer schulten self organizing maps ordering convergence properties energy functions biological bernetics everitt introduction latent variable models chapman hall faul tipping analysis sparse bayesian learning dietterich becker ghahramani eds advances neural information processing systems vol ume mit press feller introduction probability theory applications second vol ume wiley feynman leighton sands feynman lectures physics vol ume two addison wesley chapter fletcher practical methods optimization second wiley forsyth ponce computersion modern approach prentice hall freund schapire experiments new boosting algorithm saitta thirteenth international conference machine learning morgan kaufmann frey graphical models chine learning digital communication mit press frey mackay revolution belief propagation graphs cycles jordan kearns solla eds advances neural information processing sys tems volume mit press friedman greedy function approximation gradient boosting machine annals statistics friedman hastie tibshirani additive logistic regression statistical view boosting annals statistics friedman koller bayesian network structure bayesian approach structure discovery bayesian networks chine learning frydenberg chain graph markov property scandinavian journal statistics fukunaga introduction statistical pattern recognition second academic press funahashi approximate realization continuous mappings neural networks neural networks fung chang weighting integrating evidence stochastic simulation bayesian networks bonissone hen rion kanal lemmer eds certainty artiﬁcial intelligence volume elsevier gallager low density parity check codes mit press references gamerman markov chain monte carlo stochastic simulation bayesian inference chapman hall gelman carlin stern bin bayesian data analysis second chapman hall geman geman stochastic laxation gibbs distributions bayesian restoration images ieee transactions pattern analysis machine intelligence ghahramani beal variational inference bayesian mixtures factor ana lyzers solla leen uller eds advances neural information processing systems volume mit press ghahramani hinton algorithm mixtures factor analyzers technical report crg universitytoronto ghahramani hinton parameter estimation linear dynamical systems technical report crg universitytoronto ghahramani hinton variational learning switching state space models neural computation ghahramani jordan supervised learning incomplete data via appproach cowan tesauro alspector eds advances neural information processing systems volume morgan kaufmann ghahramani jordan factorial hidden markov models machine learning gibbs bayesian gaussian processes regression classiﬁcation phd thesis uni versity cambridge gibbs mackay variational gaussian process classiﬁers ieee trans actions neural networks gilks derivative free adaptive rejection sampling gibbs sampling bernardo berger dawid smith eds bayesian statistics volume ford university press gilks best tan adaptive rejection metropolis sampling applied statistics gilks richardson spiegelhal ter eds markov chain monte carlo practice chapman hall gilks wild adaptive rejection sampling gibbs sampling applied statis tics gill murray wright practical optimization academic press goldberg williams bishop regression input dependent noise gaussian process treatment vances neural information processing sys tems volume mit press golub van loan matrix computations third john hopkins univer sity press good probability weighing idence hafners gordon salmond smith novel approach nonlinear non gaussian bayesian state estimation iee proceedings graepel solving noisy linear operator equations gaussian processes application ordinary partial differential equations proceedings twentieth international con ference machine learning greig porteous seheult act maximum posteriori estimation binary images journal royal statistical society series gull developments maximum tropy data analysis skilling maxi mum entropy bayesian methods kluwer references hassibi stork second order derivatives network pruning optimal brain surgeon hanson cowan giles eds advances neural information processing systems volume morgan kaufmann hastie stuetzle principal curves journal american statistical association hastie tibshirani friedman elements statistical learning springer hastings monte carlo sampling methods using markov chains applications biometrika hathaway another interpretation algorithm mixture distributions statistics probability letters haussler convolution kernels discrete structures technical report ucsc crl university california santa cruz computer science department henrion propagation uncertainty logic sampling bayes networks lem mer kanal eds uncertainty arti ﬁcial intelligence volume north holland herbrich learning kernel classiﬁers mit press hertz krogh palmer troduction theory neural computation addison wesley hinton dayan revow modelling manifolds images handwrit ten digits ieee transactions neural net works hinton van camp keeping neural networks simple minimizing scription length weights proceedings sixth annual conference computational learning theory acm hinton welling teh osin dero new viewica proceedings international conference independent component analysis blind signal separation volume hodgson reducing computational quirements minimum distance classiﬁer remote sensing environments hoerl kennard ridge regression biased estimation nonorthogonal prob lems technometrics hofmann learning similarity doc uments information geometric approach document retrieval classiﬁcation solla leen uller eds vances neural information processing sys tems volume mit press hojen sorensen winther hansen mean ﬁeld approaches independent component analysis neural computation hornik approximation capabilities multilayer feedforward networks neural net works hornik stinchcombe white multilayer feedforward networks universal approximators neural networks hotelling analysis complex statis tical variables principal components jour nal educational psychology hotelling relations two sets variables biometrika hyv arinen oja fast ﬁxed point algorithm independent component analysis neural computation isard blake condensation conditional density propagation visual tracking international journal computersion ito representation functions perpositions step sigmoid function applications neural network theory neural networks references jaakkola jordan bayesian parameter estimation via variational methods statistics computing jaakkola tutorial variational proximation methods opper saad eds advances mean field methods mit press jaakkola haussler exploiting generative models discriminative classiﬁers kearns solla cohn eds advances neural information processing sys tems volume mit press jacobs jordan nowlan hinton adaptive mixtures local perts neural computation jaynes probability theory logic science cambridge university press jebara machine learning discrimina tive generative kluwer jeffries invariant form prior probability estimation problems pro roy soc jelinek statistical methods speech recognition mit press jensen kong kjaerulff blocking gibbs sampling large probabilistic expert systems international journal human computer studies special issue real world applications uncertain reasoning jensen introduction bayesian networks ucl press jerrum sinclair markov chain monte carlo method approach proximate counting integration hochbaum approximation algorithms hard problems pws publishing jolliffe principal component analysis second springer jordan learning graphical models mit press jordan introduction probabilis tic graphical models preparation jordan ghahramani jaakkola saul introduction variational methods graphical models jordan learning graphical models mit press jordan jacobs hierarchical mixtures experts algorithm neural computation jutten herault blind separation sources adaptive algorithm based neu romimetic architecture signal processing kalman new approach linear tering prediction problems transactions american society mechanical engineering series journal basic engineering kambhatla leen dimension reduction local principal component analysis neural computation kanazawa koller russel stochastic simulation algorithms dynamic probabilistic networks uncertainty artiﬁ cial intelligence volume morgan kaufmann kapadia discriminative training hidden markov models phd thesis university cambridge kapur maximum entropy methods sci ence engineering wiley karush minima functions several variables inequalities side constraints master thesis department mathematics university chicago kass raftery bayesfactors journal american statistical association kearns vazirani intro duction computational learning theory mit press references kindermann snell markov random fields applications american mathematical society kittler oglein contextual classiﬁ cation multispectral pixel data imagesion computing kohonen self organized formation topologically correct feature maps biological cybernetics kohonen self organizing maps springer kolmogorov zabih ergy functions minimized via graph cuts ieee transactions pattern analysis chine intelligence kreinovich arbitrary nonlinearity sufﬁcient represent functions neural net works theorem neural networks krogh brown mian olander haussler hidden markov models computational biology applications protein modelling journal molecular biology kschischnang frey loeliger factor graphs sum product algo rithm ieee transactions information ory kuhn tucker nonlinear programming proceedings berke ley symposium mathematical statistics probabilities university cali fornia press kullback leibler information sufﬁciency annals mathematical statistics urkov kainen functionally equivalent feed forward neural networks neural computation kuss rasmussen assessing proximations gaussian process classiﬁcation advances neural information processing systems number mit press press lasserre bishop minka principled hybrids generative discrimina tive models proceedings ieee confer ence computer vision pattern recognition new york lauritzen wermuth graphical models association variables qualitative quantitative nals statistics lauritzen propagation probabilities means variances mixed graphical association models journal american statistical association lauritzen graphical models oxford university press lauritzen spiegelhalter cal computations probabailities graphical structures application expert systems journal royal statistical society lawley modiﬁed method estimation factor analysis large sam ple results uppsala symposium psycho logical factor analysis number nordisk psykologi monograph series upp sala almqvist wiksell lawrence rowstron bishop taylor optimising synchro nisation times mobile devices etterich becker ghahramani eds advances neural information processing sys tems volume mit press lazarsfeld henry latent structure analysis houghton mifﬂin boser denker henderson howard hubbard jackel backpropagation applied handwritten zip code recognition neural computation denker solla optimal brain damage touretzky references advances neural information processing sys tems volume morgan kauf mann lecun bottou bengio haffner gradient based learning applied doc ument recognition proceedings ieee lee lin wahba multicategory support vector machines technical report department statistics university madison wisconsin leen data distributions regu larization invariant learning neural computation lindley scoring rules evitability probability international statistical review liu monte carlo strategies scientiﬁc computing springer lloyd least squares quantization pcm ieee transactions information ory utkepohl handbook matrices wiley mackay bayesian interpolation neural computation mackay evidence framework applied classiﬁcation networks neural computation mackay practical bayesian framework back propagation networks neural computation mackay bayesian methods backprop networks domany van hemmen schulten eds models neural networks iii chapter springer mackay bayesian neural networks density networks nuclear instruments methods physics research mackay ensemble learning hidden markov models unpublished manuscript department physics university cam bridge mackay introduction gaussian processes bishop neural networks machine learning springer mackay comparison approx imate methods handling hyperparameters neural computation mackay information theory infer ence learning algorithms cambridge uni versity press mackay gibbs density networks kay tittering ton eds statistics neural networks vances interface chapter oxford university press mackay neal good error correcting codes based sparse matrices ieee transactions information theory macqueen methods classiﬁcation analysis multivariate observations lecam neyman eds proceedings fifth berkeley symposium mathe matical statistics probability volume university california press magnus neudecker matrix dif ferential calculus applications statistics econometrics wiley mallat wavelet tour signal processing second academic press manning sch utze foundations statistical natural language processing mit press mardia jupp directional statistics wiley maybeck stochastic models estimation control academic press mcallester pac bayesian stochastic model selection machine learning references mccullagh nelder generalized linear models second chapman hall mcculloch pitts logical calculus ideas immanent nervous tivity bulletin mathematical biophysics reprinted anderson rosenfeld mceliece mackay cheng turbo decoding instance pearl belief ppropagation algorithm ieee journal selected areas communications mclachlan basford mixture models inference applications clustering marcel dekker mclachlan krishnan algorithm extensions wiley mclachlan peel finite mixture models wiley meng rubin maximum like lihood estimation via ecm algorithm general framework biometrika metropolis rosenbluth rosen bluth teller teller equation state calculations fast computing machines journal chemical physics metropolis ulam monte carlo method journal american statistical association mika atsch weston fisher discriminant analysis kernels larsen wilson douglas eds neural networks signal processing ieee minka expectation propagation proximate bayesian inference breese koller eds proceedings seventeenth conference uncertainty artiﬁcial intelli gence morgan kaufmann minka family approximate gorithms bayesian inference thesis mit minka power technical report msr microsoft research cam bridge minka divergence measures message passing technical report msr microsoft research cambridge minka automatic choice dimensionality pca leen diet terich tresp eds advances neural information processing systems volume mit press minsky papert perceptrons mit press expanded edition miskin mackay ensem ble learning blind source separation roberts everson eds independent component analysis principles practice cambridge university press møller efﬁcient training feed forward neural networks thesis aarhus university denmark moody darken fast learning networks locally tuned processing units neural computation moore anchors hierarching triangle inequality survive high dimensional data proceedings twelfth con ference uncertainty artiﬁcial intelligence uller mika atsch tsuda introduction kernel based learning algorithms ieee transactions neural networks uller quintana nonparametric bayesian data analysis statistical science nabney netlab algorithms pattern recognition springer nadaraya estimating regression theory probability applications references nag wong fallside script recognition using hidden markov modelsicassp ieee neal probabilistic inference using markov chain monte carlo methods technical report crg department computer science universitytoronto canada neal bayesian learning neural networks springer lecture notes statistics neal monte carlo implementation gaussian process models bayesian regression classiﬁcation technical report partment computer statistics universitytoronto neal suppressing random walks markov chain monte carlo using ordered relaxation jordan learning graphical models mit press neal markov chain sampling dirichlet process mixture models journal computational graphical statistics neal slice sampling annals statis tics neal hinton new view algorithm justiﬁes incremental variants jordan learning graphical models mit press nelder wedderburn generalized linear models journal royal sta tistical society nilsson learning machines mcgraw hill reprinted mathematical foundations learning machines morgan kaufmann nocedal wright numerical timization springer nowlan hinton simplifying neural networks soft weight sharing neural computation ogden essential wavelets statistical applications data analysis birkh auser opper winther bayesian approach line learning saad line learning neural networks cambridge university press opper winther gaussian processes svm mean ﬁeld theory leave one smola bartlett shuurmans eds vances large margin classiﬁers mit press opper winther gaussian processes classiﬁcation neural computation osuna freund girosi support vector machines training applications memo aim mit papoulis probability random variables stochastic processes second mcgraw hill parisi statistical field theory addison wesley pearl probabilistic reasoning intelli gent systems morgan kaufmann pearlmutter fast exact multiplication hessian neural computation pearlmutter parra maximum likelihood source separation context sensitive generalizationica mozer jor dan petsche eds advances neural information processing systems volume mit press pearson lines planes closest systems points space london edin burgh dublin philosophical magazine journal science sixth series platt fast training support vector machines using sequential minimal optimization burges smola eds advances kernel methods support vector learning mit press references platt probabilities machines smola bartlett shuurmans eds advances large margin classiﬁers mit press platt cristianini shawe taylor large margin dags multiclass clas siﬁcation solla leen uller eds advances neural information processing systems volume mit press poggio girosi networks proximation learning proceedings ieee powell radial basis functions multivariable interpolation review mason cox eds algorithms approximation oxford university press press teukolsky vetterling flannery numerical recipes art scientiﬁc computing second cambridge university press qazaz williams bishop upper bound bayesian error bars generalized linear regression ellacott mason anderson eds mathematics neural networks models algo rithms applications kluwer quinlan induction decision trees machine learning quinlan programs machine learning morgan kaufmann rabiner juang fundamentals speech recognition prentice hall rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee ramasubramanian paliwal generalized optimization tree fast nearest neighbour search proceedings fourth ieee region international conference ten con ramsey truth probability braithwaite foundations math ematics logical essays humanities press rao mitra generalized verse matrices applications wiley rasmussen evaluation gaussian processes methods non linear regression thesis universitytoronto rasmussen qui nonero candela healing relevance vector machine aug mentation raedt wrobel eds proceedings international confer ence machine learning rasmussen williams gaussian processes machine learning mit press rauch tung striebel maximum likelihood estimates linear dynamical systems aiaa journal ricotti ragazzini martinelli learning word stress sub optimal second order backpropagation neural network pro ceedings ieee international conference neural networks volume ieee ripley pattern recognition neural networks cambridge university press robbins monro stochastic approximation method annals mathematical statistics robert casella monte carlo statistical methods springer rockafellar convex analysis princeton university press rosenblatt principles neurodynam ics perceptrons theory brain mech anisms spartan roth steinhage nonlinear discrim inant analysis using kernel functions references solla leen uller eds vances neural information processing sys tems volume mit press roweis algorithms pca spca jordan kearns solla eds advances neural information processing systems volume mit press roweis ghahramani unifying review linear gaussian models neural computation roweis saul december nonlinear dimensionality reduction locally linear bedding science rubin iteratively reweighted least squares encyclopedia statistical sciences volume wiley rubin thayer gorithms factor analysis psychome trika rumelhart hinton williams learning internal representations ror propagation rumelhart clelland pdp research group eds parallel distributed processing explorations microstructure cognition volume foundations mit press reprinted anderson rosenfeld rumelhart mcclelland pdp search group eds parallel distributed processing explorations microstruc ture cognition volume foundations mit press sagan introduction calculus variations dover savage subjective basis sta tistical practice technical report department statistics university michigan ann arbor platt shawe taylor smola williamson estimating support high dimensional distribution neural computation smola uller nonlinear component analysis kernel eigenvalue problem neural computation smola williamson bartlett new support vector algorithms neural computation smola learning kernels mit press schwarz estimating dimension model annals statistics schwarz finite element methods aca demic press seeger bayesian gaussian process models pac bayesian generalization error bounds sparse approximations thesis uni versity edinburg seeger williams lawrence fast forward selection speed sparse gaussian processes bishop frey eds proceedings ninth international work shop artiﬁcial intelligence statistics key west florida shachter peot simulation proaches general probabilistic inference lief networks bonissone henrion kanal lemmer eds uncer tainty artiﬁcial intelligence volume else vier shannon mathematical theory communication bell system technical jour nal shawe taylor cristianini kernel methods pattern analysis cambridge uni versity press sietsma dow creating artiﬁ cial neural networks generalize neural net works simard denker efﬁcient pattern recognition using new transformation distance hanson cowan references giles eds advances neural information processing systems volume morgan kaufmann simard victorri denker tangent prop formalism specifying selected invariances adaptive network moody hanson lippmann eds advances neural information processing systems volume morgan kaufmann simard steinkraus platt best practice convolutional neural networks applied visual document analysis pro ceedings international conference document analysis recognition icdar ieee computer society sirovich turbulence dynamics coherent structures quarterly applied math ematics smola bartlett sparse greedy gaussian process regression leen dietterich tresp eds advances neural information processing systems volume mit press spiegelhalter lauritzen sequential updating conditional probabilities directed graphical structures networks stinchecombe white universal approximation using feed forward networks non sigmoid hidden layer activation functions international joint conference neural net works volume ieee stone independent component analy sis tutorial introduction mit press sung poggio example based learning view based human face detection memo mit sutton barto reinforcement learning introduction mit press svens bishop bust bayesian mixture modelling neurocomputing tarassenko novelty detection identiﬁcation masses mamograms pro ceedings fourth iee international conference artiﬁcial neural networks volume iee tax duin data domain description support vectors verleysen proceedings european symposium artiﬁcial neural networks esann facto press teh jordan beal blei hierarchical dirichlet processes journal americal statistical association appear tenenbaum silva langford december global framework non linear dimensionality reduction science tesauro gammon self teaching backgammon program achieves master level play neural computation thiesson chickering heckerman meek arma time series modelling graphical models chickering halpern eds proceedings twentieth conference uncertainty artiﬁcial intelli gence banff canada auai press tibshirani regression shrinkage lection via lasso journal royal statis tical society tierney markov chains exploring posterior distributions annals statistics tikhonov arsenin solutions ill posed problems winston tino nabney hierarchical gtm constructing localized non linear projection manifolds principled way ieee trans actions pattern analysis machine intelli gence tino nabney suning directional curvatures visualize folding patterns gtm projection manifolds references dorffner bischof hornik eds artiﬁcial neural networksicann springer tipping probabilistic visualisation high dimensional binary data kearns solla cohn eds advances neural information processing systems vol ume mit press tipping sparse bayesian learning relevance vector machine journal chine learning research tipping bishop probabilis tic principal component analysis technical port ncrg neural computing research group aston university tipping bishop mixtures probabilistic principal component analyzers neural computation tipping bishop prob abilistic principal component analysis journal royal statistical society series tipping faul fast marginal likelihood maximization sparse bayesian models bishop frey eds proceedings ninth international workshop artiﬁcial intelligence statistics key west florida tong koller restricted bayes timal classiﬁers proceedings national conference artiﬁcial intelligence aaai tresp scaling kernel based systems large datasets data mining knowledge dis covery uhlenbeck ornstein theory brownian motion phys rev valiant theory learnable communications association computing machinery vapnik estimation dependences based empirical data springer vapnik nature statistical learning theory springer vapnik statistical learning theory ley veropoulos campbell cristianini controlling sensitivity support vector machines proceedings international joint conference artiﬁcial intelligence ijcai workshop vidakovic statistical modelling wavelets wiley viola jones robust real time face detection international journal computersion viterbi error bounds convolutional codes asymptotically optimum coding algorithm ieee transactions information theory viterbi omura principles digital communication coding mcgraw hill wahba comparison gcv gml choosing smoothing parameter generalized spline smoothing problem numerical mathematics wainwright jaakkola willsky new class upper bounds log partition function ieee transactions information theory walker asymptotic behaviour posterior distributions journal royal statistical society walker damien laud smith bayesian nonparametric inference random distributions related functions discussion journal royal statistical society watson smooth regression analysis sankhy indian journal statistics series references webb functional approximation feed forward networks least squares approach generalisation ieee transactions neural networks weisstein crc concise encyclopedia mathematics chapman hall crc weston watkins multi class support vector machines verlysen pro ceedings esann brussels facto publications whittaker graphical models applied multivariate statistics wiley widrow hoff adaptive switching circuits ire wescon convention record volume reprinted derson rosenfeld widrow lehr years adap tive neural networks perceptron madeline backpropagation proceedings ieee wiegerinck heskes fractional belief propagation becker thrun obermayer eds advances neural information processing systems volume mit press williams computation inﬁ nite neural networks neural computation williams prediction gaussian processes linear regression linear prediction beyond jordan learning graphical models mit press williams barber bayesian classiﬁcation gaussian processes ieee transactions pattern analysis machine intelligence williams seeger using nystrom method speed kernel machines leen dietterich tresp eds advances neural information processing sys tems volume mit press williams blake cipolla sparse bayesian learning efﬁcient visual tracking ieee transactions pattern analysis machine intelligence williams using neural networks model conditional multivariate densities neural computation winn bishop variational message passing journal machine learning search zarchan musoff fundamentals kalman filtering practical approach sec ond aiaa index page numbers bold indicate primary source information corresponding topic coding scheme acceptance criterion activation function active constraint adaboost adaline adaptive rejection sampling adf see assumed density ﬁltering aic see akaike information criterion akaike information criterion family divergences recursion ancestral sampling annular ﬂow model see autoregressive model arc ard see automatic relevance determination arma see autoregressive moving average assumed density ﬁltering autoassociative networks automatic relevance determination autoregressive hidden markov model autoregressive model autoregressive moving average back tracking backgammon backpropagation bagging basis function batch training baum welch algorithm bayes theorem bayes thomas bayesian analysis vii hierarchical model averaging bayesian information criterion bayesian model comparison bayesian network bayesian probability belief propagation bernoulli distribution mixture model bernoulli jacob beta distribution beta recursion class covariance bias bias parameter bias variance trade bic see bayesian information criterion binary entropy binomial distribution index biological sequence bipartite graph bits blind source separation blocked path boltzmann distribution boltzmann ludwig eduard boolean logic boosting bootstrap bootstrap ﬁlter box constraints box muller method calculus variations canonical correlation analysis canonical link function cart see classiﬁcation regression trees cauchy distribution causality cca see canonical correlation analysis central differences central limit theorem chain graph chaining chapman kolmogorov equations child node cholesky decomposition chunking circular normal see von mises distribution classical probability classiﬁcation classiﬁcation regression trees clique clustering clutter problem parents code book vectors combining models committee complete data set completing square computational learning theory concave function concentration parameter condensation algorithm conditional entropy conditional expectation conditional independence conditional mixture model see mixture model conditional probability conjugate prior convex duality convex function convolutional neural network correlation matrix cost function covariance class within class covariance matrix diagonal isotropic partitioned positive deﬁnite cox axioms credit assignment cross entropy error function cross validation cumulative distribution function curse dimensionality curve ﬁtting map see dependency map separation dag see directed acyclic graph dagsvm data augmentation data compression decision boundary decision region decision surface see decision boundary decision theory decision tree decomposition methods degrees freedom degrees freedom parameter density estimation index density network dependency map descendant node design matrix differential entropy digamma function directed acyclic graph directed cycle directed factorization dirichlet distribution dirichlet lejeune discriminant function discriminative model distortion measure distributive law multiplication dna document retrieval dual representation dual energy gamma densitometry dynamic programming dynamical system step see expectation step early stopping ecm see expectation conditional maximization edge effective number observations effective number parameters elliptical means see expectation maximization emission probability empirical bayes see evidence approximation energy function entropy conditional differential relative see expectation propagation tube insensitive error function equality constraint equivalent kernel function error backpropagation see backpropagation error function error correcting output codes euler leonhard euler lagrange equations evidence approximation evidence function expectation expectation conditional maximization expectation maximization gaussian mixture generalized sampling methods expectation propagation expectation step explaining away exploitation exploration exponential distribution exponential family extensive variables face detection face tracking factor analysis mixture model factor graph factor loading factorial hidden markov model factorized distribution feature extraction feature map feature space fisher information matrix fisher kernel fisher linear discriminant ﬂooding schedule forward kinematics forward problem forward propagation forward backward algorithm fractional belief propagation frequentist probability fuel system function interpolation functional derivative index gamma densitometry gamma distribution gamma function gating function gauss carl friedrich gaussian conditional marginal maximum likelihood mixture sequential estimation sufﬁcient statistics wrapped gaussian kernel gaussian process gaussian random ﬁeld gaussian gamma distribution gaussian wishart distribution gem see expectation maximization generalized generalization generalized linear model generalized maximum likelihood see evidence proximation generative model generative topographic mapping directional curvature magniﬁcation factor geodesic distance gibbs sampling blocking gibbs josiah willard gini index global minimum gradient descent gram matrix graph cut algorithm graphical model bipartite directed factorization fully connected inference tree treewidth triangulated undirected green function gtm see generative topographic mapping hamilton william rowan hamiltonian dynamics hamiltonian function hammersley clifford theorem handwriting recognition handwritten digit head head path head tail path heaviside step function hellinger distance hessian matrix diagonal approximation exact evaluation fast multiplication ﬁnite differences inverse outer product approximation heteroscedastic hidden markov model autoregressive factorial forward backward algorithm input output left right maximum likelihood scaling factor sum product algorithm switching variational inference hidden unit hidden variable hierarchical bayesian model hierarchical mixture experts hinge error function hinton diagram histogram density estimation hme see hierarchical mixture experts hold set homogeneous ﬂow homogeneous kernel homogeneous markov chain index hooke law hybrid monte carlo hyperparameter hyperprior map see independence map see independent identically distributedica see independent component analysis icm see iterated conditional modes identiﬁability image noising importance sampling importance weights improper prior imputation step imputation posterior algorithm inactive constraint incomplete data set independence map independent component analysis independent factor analysis independent identically distributed independent variables independent identically distributed induced factorization inequality constraint inference information criterion information geometry information theory input output hidden markov model intensive variables intrinsic dimensionality invariance inverse gamma distribution inverse kinematics inverse problem inverse wishart distribution algorithm see imputation posterior algorithm irls see iterative reweighted least squares ising model isomap isometric feature map iterated conditional modes iterative reweighted least squares jacobian matrix jensen inequality join tree junction tree algorithm nearest neighbours means clustering algorithm medoids algorithm kalman ﬁlter extended kalman gain matrix kalman smoother karhunen eve transform karush kuhn tucker conditions kernel density estimator kernel function fisher gaussian homogeneous nonvectorial inputs stationary kernel pca kernel regression kernel substitution kernel trick kinetic energy kkt see karush kuhn tucker conditions divergence see kullback leibler divergence kriging see gaussian process kullback leibler divergence lagrange multiplier lagrange joseph louis lagrangian laminar ﬂow laplace approximation laplace pierre simon large margin see margin lasso latent class analysis latent trait model latent variable index lattice diagram lds see linear dynamical system leapfrog discretization learning learning rate parameter least mean squares algorithm leave one likelihood function likelihood weighted sampling linear discriminant fisher linear dynamical system inference linear independence linear regression mixture model variational linear smoother linear gaussian model linearly separable link link function liouville theorem lle see locally linear embedding lms algorithm see least mean squares algorithm local minimum local receptive ﬁeld locally linear embedding location parameter log odds logic sampling logistic regression bayesian mixture model multiclass logistic sigmoid logit function loopy belief propagation loss function loss matrix lossless data compression lossy data compression lower bound step see maximization step machine learning vii macrostate mahalanobis distance manifold map see maximum posterior margin error soft marginal likelihood marginal probability markov blanket markov boundary see markov blanket markov chain ﬁrst order homogeneous second order markov chain monte carlo markov model homogeneous markov network see markov random ﬁeld markov random ﬁeld max sum algorithm maximal clique maximal spanning tree maximization step maximum likelihood gaussian mixture singularities type see evidence approximation maximum margin see margin maximum posterior mcmc see markov chain monte carlo mdn see mixture density network mds see multidimensional scaling mean mean ﬁeld theory mean value theorem measure theory memory based methods message passing pending message schedule variational metropolis algorithm metropolis hastings algorithm index microstate minimum risk minkowski loss missing random missing data mixing coefﬁcient mixture component mixture density network mixture distribution see mixture model mixture model conditional linear regression logistic regression symmetries mixture experts mixture gaussians mlp see multilayer perceptron mnist data model comparison model evidence model selection moment matching momentum variable monte carlo algorithm monte carlo sampling moore penrose pseudo inverse see pseudo inverse moralization mrf see markov random ﬁeld multidimensional scaling multilayer perceptron multimodality multinomial distribution multiplicity mutual information nadaraya watson see kernel regression naive bayes model nats natural language modelling natural parameters nearest neighbour methods neural network convolutional regularization relation gaussian process newton raphson node noiseless coding theorem nonidentiﬁability noninformative prior nonparametric methods normal distribution see gaussian normal equations normal gamma distribution normal wishart distribution normalized exponential see softmax function novelty detection svm object recognition observed variable occam factor oil ﬂow data old faithful data line learning see sequential learning one versus one classiﬁer one versus rest classiﬁer ordered relaxation ornstein uhlenbeck process orthogonal least squares outlier outliers ﬁtting relaxation pac learning see probably approximately correct pac bayesian framework parameter shrinkage parent node particle ﬁlter partition function parzen estimator see kernel density estimator parzen window pattern recognition vii pca see principal component analysis pending message perceptron convergence theorem hardware perceptron criterion perfect map index periodic variable phase space photon noise plate polynomial curve ﬁtting polytree position variable positive deﬁnite covariance positive deﬁnite matrix positive semideﬁnite covariance positive semideﬁnite matrix posterior probability posterior step potential energy potential function power power method precision matrix precision parameter predictive distribution preprocessing principal component analysis bayesian algorithm gibbs sampling mixture distribution physical analogy principal curve principal subspace principal surface prior conjugate consistent improper noninformative probabilistic graphical model see graphical model probabilistic pca probability bayesian classical density frequentist mass function prior product rule sum rule theory probably approximately correct probit function probit regression product rule probability proposal distribution protected conjugate gradients protein sequence pseudo inverse pseudo random numbers quadratic discriminant quality parameter radial basis function rauch tung striebel equations regression regression function regularization tikhonov regularized least squares reinforcement learning reject option rejection sampling relative entropy relevance vector relevance vector machine responsibility ridge regression rms error see root mean square error robbins monro algorithm robot arm robustness root node root mean square error rosenblatt frank rotation invariance rts equations see rauch tung striebel equations running intersection property rvm see relevance vector machine sample mean sample variance sampling importance resampling scale invariance index scale parameter scaling factor schwarz criterion see bayesian information crite rion self organizing map sequential data sequential estimation sequential gradient descent sequential learning sequential minimal optimization serial message passing schedule shannon claude shared parameters shrinkage shur complement sigmoid see logistic sigmoid simplex single class support vector machine singular value decomposition sinusoidal data sir see sampling importance resampling skip layer connection slack variable slice sampling smo see sequential minimal optimization smoother matrix smoothing parameter soft margin soft weight sharing softmax function som see self organizing map sparsity sparsity parameter spectrogram speech recognition sphereing spline functions standard deviation standardizing state space model switching stationary kernel statistical bias see bias statistical independence see independent variables statistical learning theory see computational learning theory steepest descent stirling approximation stochastic stochastic stochastic gradient descent stochastic process stratiﬁed ﬂow student distribution subsampling sufﬁcient statistics sum rule probability sum squares error sum product algorithm hidden markov model supervised learning support vector support vector machine regression multiclass survival ﬁttest svd see singular value decomposition svm see support vector machine switching hidden markov model switching state space model synthetic datasets tail tail path tangent distance tangent propagation tapped delay line target vector test set threshold parameter tied parameters tikhonov regularization time warping tomography training training set transition probability translation invariance tree reweighted message passing treewidth minsky papert perceptrons mit press expanded edition miskin mackay ensem ble learning blind source separation roberts everson eds independent component analysis principles practice cambridge university press møller efﬁcient training feed forward neural networks thesis aarhus university denmark moody darken fast learning networks locally tuned processing units neural computation moore anchors hierarching triangle inequality survive high dimensional data proceedings twelfth con ference uncertainty artiﬁcial intelligence uller mika atsch tsuda introduction kernel based learning algorithms ieee transactions neural networks uller quintana nonparametric bayesian data analysis statistical science nabney netlab algorithms pattern recognition springer nadaraya estimating regression theory probability applications references nag wong fallside script recognition using hidden markov modelsicassp ieee neal probabilistic inference using markov chain monte carlo methods technical report crg department computer science universitytoronto canada neal bayesian learning neural networks springer lecture notes statistics neal monte carlo implementation gaussian process models bayesian regression classiﬁcation technical report partment computer statistics universitytoronto neal suppressing random walks markov chain monte carlo using ordered relaxation jordan learning graphical models mit press neal markov chain sampling dirichlet process mixture models journal computational graphical statistics neal slice sampling annals statis tics neal hinton new view algorithm justiﬁes incremental variants jordan learning graphical models mit press nelder wedderburn generalized linear models journal royal sta tistical society nilsson learning machines mcgraw hill reprinted mathematical foundations learning machines morgan kaufmann nocedal wright numerical timization springer nowlan hinton simplifying neural networks soft weight sharing neural computation ogden essential wavelets statistical applications data analysis birkh auser opper winther bayesian approach line learning saad line learning neural networks cambridge university press opper winther gaussian processes svm mean ﬁeld theory leave one smola bartlett shuurmans eds vances large margin classiﬁers mit press opper winther gaussian processes classiﬁcation neural computation osuna freund girosi support vector machines training applications memo aim mit papoulis probability random variables stochastic processes second mcgraw hill parisi statistical field theory addison wesley pearl probabilistic reasoning intelli gent systems morgan kaufmann pearlmutter fast exact multiplication hessian neural computation pearlmutter parra maximum likelihood source separation context sensitive generalizationica mozer jor dan petsche eds advances neural information processing systems volume mit press pearson lines planes closest systems points space london edin burgh dublin philosophical magazine journal science sixth series platt fast training support vector machines using sequential minimal optimization burges smola eds advances kernel methods support vector learning mit press references platt probabilities machines smola bartlett shuurmans eds advances large margin classiﬁers mit press platt cristianini shawe taylor large margin dags multiclass clas siﬁcation solla leen uller eds advances neural information processing systems volume mit press poggio girosi networks proximation learning proceedings ieee powell radial basis functions multivariable interpolation review mason cox eds algorithms approximation oxford university press press teukolsky vetterling flannery numerical recipes art scientiﬁc computing second cambridge university press qazaz williams bishop upper bound bayesian error bars generalized linear regression ellacott mason anderson eds mathematics neural networks models algo rithms applications kluwer quinlan induction decision trees machine learning quinlan programs machine learning morgan kaufmann rabiner juang fundamentals speech recognition prentice hall rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee ramasubramanian paliwal generalized optimization tree fast nearest neighbour search proceedings fourth ieee region international conference ten con ramsey truth probability braithwaite foundations math ematics logical essays humanities press rao mitra generalized verse matrices applications wiley rasmussen evaluation gaussian processes methods non linear regression thesis universitytoronto rasmussen qui nonero candela healing relevance vector machine aug mentation raedt wrobel eds proceedings international confer ence machine learning rasmussen williams gaussian processes machine learning mit press rauch tung striebel maximum likelihood estimates linear dynamical systems aiaa journal ricotti ragazzini martinelli learning word stress sub optimal second order backpropagation neural network pro ceedings ieee international conference neural networks volume ieee ripley pattern recognition neural networks cambridge university press robbins monro stochastic approximation method annals mathematical statistics robert casella monte carlo statistical methods springer rockafellar convex analysis princeton university press rosenblatt principles neurodynam ics perceptrons theory brain mech anisms spartan roth steinhage nonlinear discrim inant analysis using kernel functions references solla leen uller eds vances neural information processing sys tems volume mit press roweis algorithms pca spca jordan kearns solla eds advances neural information processing systems volume mit press roweis ghahramani unifying review linear gaussian models neural computation roweis saul december nonlinear dimensionality reduction locally linear bedding science rubin iteratively reweighted least squares encyclopedia statistical sciences volume wiley rubin thayer gorithms factor analysis psychome trika rumelhart hinton williams learning internal representations ror propagation rumelhart clelland pdp research group eds parallel distributed processing explorations microstructure cognition volume foundations mit press reprinted anderson rosenfeld rumelhart mcclelland pdp search group eds parallel distributed processing explorations microstruc ture cognition volume foundations mit press sagan introduction calculus variations dover savage subjective basis sta tistical practice technical report department statistics university michigan ann arbor platt shawe taylor smola williamson estimating support high dimensional distribution neural computation smola uller nonlinear component analysis kernel eigenvalue problem neural computation smola williamson bartlett new support vector algorithms neural computation smola learning kernels mit press schwarz estimating dimension model annals statistics schwarz finite element methods aca demic press seeger bayesian gaussian process models pac bayesian generalization error bounds sparse approximations thesis uni versity edinburg seeger williams lawrence fast forward selection speed sparse gaussian processes bishop frey eds proceedings ninth international work shop artiﬁcial intelligence statistics key west florida shachter peot simulation proaches general probabilistic inference lief networks bonissone henrion kanal lemmer eds uncer tainty artiﬁcial intelligence volume else vier shannon mathematical theory communication bell system technical jour nal shawe taylor cristianini kernel methods pattern analysis cambridge uni versity press sietsma dow creating artiﬁ cial neural networks generalize neural net works simard denker efﬁcient pattern recognition using new transformation distance hanson cowan references giles eds advances neural information processing systems volume morgan kaufmann simard victorri denker tangent prop formalism specifying selected invariances adaptive network moody hanson lippmann eds advances neural information processing systems volume morgan kaufmann simard steinkraus platt best practice convolutional neural networks applied visual document analysis pro ceedings international conference document analysis recognition icdar ieee computer society sirovich turbulence dynamics coherent structures quarterly applied math ematics smola bartlett sparse greedy gaussian process regression leen dietterich tresp eds advances neural information processing systems volume mit press spiegelhalter lauritzen sequential updating conditional probabilities directed graphical structures networks stinchecombe white universal approximation using feed forward networks non sigmoid hidden layer activation functions international joint conference neural net works volume ieee stone independent component analy sis tutorial introduction mit press sung poggio example based learning view based human face detection memo mit sutton barto reinforcement learning introduction mit press svens bishop bust bayesian mixture modelling neurocomputing tarassenko novelty detection identiﬁcation masses mamograms pro ceedings fourth iee international conference artiﬁcial neural networks volume iee tax duin data domain description support vectors verleysen proceedings european symposium artiﬁcial neural networks esann facto press teh jordan beal blei hierarchical dirichlet processes journal americal statistical association appear tenenbaum silva langford december global framework non linear dimensionality reduction science tesauro gammon self teaching backgammon program achieves master level play neural computation thiesson chickering heckerman meek arma time series modelling graphical models chickering halpern eds proceedings twentieth conference uncertainty artiﬁcial intelli gence banff canada auai press tibshirani regression shrinkage lection via lasso journal royal statis tical society tierney markov chains exploring posterior distributions annals statistics tikhonov arsenin solutions ill posed problems winston tino nabney hierarchical gtm constructing localized non linear projection manifolds principled way ieee trans actions pattern analysis machine intelli gence tino nabney suning directional curvatures visualize folding patterns gtm projection manifolds references dorffner bischof hornik eds artiﬁcial neural networksicann springer tipping probabilistic visualisation high dimensional binary data kearns solla cohn eds advances neural information processing systems vol ume mit press tipping sparse bayesian learning relevance vector machine journal chine learning research tipping bishop probabilis tic principal component analysis technical port ncrg neural computing research group aston university tipping bishop mixtures probabilistic principal component analyzers neural computation tipping bishop prob abilistic principal component analysis journal royal statistical society series tipping faul fast marginal likelihood maximization sparse bayesian models bishop frey eds proceedings ninth international workshop artiﬁcial intelligence statistics key west florida tong koller restricted bayes timal classiﬁers proceedings national conference artiﬁcial intelligence aaai tresp scaling kernel based systems large datasets data mining knowledge dis covery uhlenbeck ornstein theory brownian motion phys rev valiant theory learnable communications association computing machinery vapnik estimation dependences based empirical data springer vapnik nature statistical learning theory springer vapnik statistical learning theory ley veropoulos campbell cristianini controlling sensitivity support vector machines proceedings international joint conference artiﬁcial intelligence ijcai workshop vidakovic statistical modelling wavelets wiley viola jones robust real time face detection international journal computersion viterbi error bounds convolutional codes asymptotically optimum coding algorithm ieee transactions information theory viterbi omura principles digital communication coding mcgraw hill wahba comparison gcv gml choosing smoothing parameter generalized spline smoothing problem numerical mathematics wainwright jaakkola willsky new class upper bounds log partition function ieee transactions information theory walker asymptotic behaviour posterior distributions journal royal statistical society walker damien laud smith bayesian nonparametric inference random distributions related functions discussion journal royal statistical society watson smooth regression analysis sankhy indian journal statistics series references webb functional approximation feed forward networks least squares approach generalisation ieee transactions neural networks weisstein crc concise encyclopedia mathematics chapman hall crc weston watkins multi class support vector machines verlysen pro ceedings esann brussels facto publications whittaker graphical models applied multivariate statistics wiley widrow hoff adaptive switching circuits ire wescon convention record volume reprinted derson rosenfeld widrow lehr years adap tive neural networks perceptron madeline backpropagation proceedings ieee wiegerinck heskes fractional belief propagation becker thrun obermayer eds advances neural information processing systems volume mit press williams computation inﬁ nite neural networks neural computation williams prediction gaussian processes linear regression linear prediction beyond jordan learning graphical models mit press williams barber bayesian classiﬁcation gaussian processes ieee transactions pattern analysis machine intelligence williams seeger using nystrom method speed kernel machines leen dietterich tresp eds advances neural information processing sys tems volume mit press williams blake cipolla sparse bayesian learning efﬁcient visual tracking ieee transactions pattern analysis machine intelligence williams using neural networks model conditional multivariate densities neural computation winn bishop variational message passing journal machine learning search zarchan musoff fundamentals kalman filtering practical approach sec ond aiaa index page numbers bold indicate primary source information corresponding topic coding scheme acceptance criterion activation function active constraint adaboost adaline adaptive rejection sampling adf see assumed density ﬁltering aic see akaike information criterion akaike information criterion family divergences recursion ancestral sampling annular ﬂow model see autoregressive model arc ard see automatic relevance determination arma see autoregressive moving average assumed density ﬁltering autoassociative networks automatic relevance determination autoregressive hidden markov model autoregressive model autoregressive moving average back tracking backgammon backpropagation bagging basis function batch training baum welch algorithm bayes theorem bayes thomas bayesian analysis vii hierarchical model averaging bayesian information criterion bayesian model comparison bayesian network bayesian probability belief propagation bernoulli distribution mixture model bernoulli jacob beta distribution beta recursion class covariance bias bias parameter bias variance trade bic see bayesian information criterion binary entropy binomial distribution index biological sequence bipartite graph bits blind source separation blocked path boltzmann distribution boltzmann ludwig eduard boolean logic boosting bootstrap bootstrap ﬁlter box constraints box muller method calculus variations canonical correlation analysis canonical link function cart see classiﬁcation regression trees cauchy distribution causality cca see canonical correlation analysis central differences central limit theorem chain graph chaining chapman kolmogorov equations child node cholesky decomposition chunking circular normal see von mises distribution classical probability classiﬁcation classiﬁcation regression trees clique clustering clutter problem parents code book vectors combining models committee complete data set completing square computational learning theory concave function concentration parameter condensation algorithm conditional entropy conditional expectation conditional independence conditional mixture model see mixture model conditional probability conjugate prior convex duality convex function convolutional neural network correlation matrix cost function covariance class within class covariance matrix diagonal isotropic partitioned positive deﬁnite cox axioms credit assignment cross entropy error function cross validation cumulative distribution function curse dimensionality curve ﬁtting map see dependency map separation dag see directed acyclic graph dagsvm data augmentation data compression decision boundary decision region decision surface see decision boundary decision theory decision tree decomposition methods degrees freedom degrees freedom parameter density estimation index density network dependency map descendant node design matrix differential entropy digamma function directed acyclic graph directed cycle directed factorization dirichlet distribution dirichlet lejeune discriminant function discriminative model distortion measure distributive law multiplication dna document retrieval dual representation dual energy gamma densitometry dynamic programming dynamical system step see expectation step early stopping ecm see expectation conditional maximization edge effective number observations effective number parameters elliptical means see expectation maximization emission probability empirical bayes see evidence approximation energy function entropy conditional differential relative see expectation propagation tube insensitive error function equality constraint equivalent kernel function error backpropagation see backpropagation error function error correcting output codes euler leonhard euler lagrange equations evidence approximation evidence function expectation expectation conditional maximization expectation maximization gaussian mixture generalized sampling methods expectation propagation expectation step explaining away exploitation exploration exponential distribution exponential family extensive variables face detection face tracking factor analysis mixture model factor graph factor loading factorial hidden markov model factorized distribution feature extraction feature map feature space fisher information matrix fisher kernel fisher linear discriminant ﬂooding schedule forward kinematics forward problem forward propagation forward backward algorithm fractional belief propagation frequentist probability fuel system function interpolation functional derivative index gamma densitometry gamma distribution gamma function gating function gauss carl friedrich gaussian conditional marginal maximum likelihood mixture sequential estimation sufﬁcient statistics wrapped gaussian kernel gaussian process gaussian random ﬁeld gaussian gamma distribution gaussian wishart distribution gem see expectation maximization generalized generalization generalized linear model generalized maximum likelihood see evidence proximation generative model generative topographic mapping directional curvature magniﬁcation factor geodesic distance gibbs sampling blocking gibbs josiah willard gini index global minimum gradient descent gram matrix graph cut algorithm graphical model bipartite directed factorization fully connected inference tree treewidth triangulated undirected green function gtm see generative topographic mapping hamilton william rowan hamiltonian dynamics hamiltonian function hammersley clifford theorem handwriting recognition handwritten digit head head path head tail path heaviside step function hellinger distance hessian matrix diagonal approximation exact evaluation fast multiplication ﬁnite differences inverse outer product approximation heteroscedastic hidden markov model autoregressive factorial forward backward algorithm input output left right maximum likelihood scaling factor sum product algorithm switching variational inference hidden unit hidden variable hierarchical bayesian model hierarchical mixture experts hinge error function hinton diagram histogram density estimation hme see hierarchical mixture experts hold set homogeneous ﬂow homogeneous kernel homogeneous markov chain index hooke law hybrid monte carlo hyperparameter hyperprior map see independence map see independent identically distributedica see independent component analysis icm see iterated conditional modes identiﬁability image noising importance sampling importance weights improper prior imputation step imputation posterior algorithm inactive constraint incomplete data set independence map independent component analysis independent factor analysis independent identically distributed independent variables independent identically distributed induced factorization inequality constraint inference information criterion information geometry information theory input output hidden markov model intensive variables intrinsic dimensionality invariance inverse gamma distribution inverse kinematics inverse problem inverse wishart distribution algorithm see imputation posterior algorithm irls see iterative reweighted least squares ising model isomap isometric feature map iterated conditional modes iterative reweighted least squares jacobian matrix jensen inequality join tree junction tree algorithm nearest neighbours means clustering algorithm medoids algorithm kalman ﬁlter extended kalman gain matrix kalman smoother karhunen eve transform karush kuhn tucker conditions kernel density estimator kernel function fisher gaussian homogeneous nonvectorial inputs stationary kernel pca kernel regression kernel substitution kernel trick kinetic energy kkt see karush kuhn tucker conditions divergence see kullback leibler divergence kriging see gaussian process kullback leibler divergence lagrange multiplier lagrange joseph louis lagrangian laminar ﬂow laplace approximation laplace pierre simon large margin see margin lasso latent class analysis latent trait model latent variable index lattice diagram lds see linear dynamical system leapfrog discretization learning learning rate parameter least mean squares algorithm leave one likelihood function likelihood weighted sampling linear discriminant fisher linear dynamical system inference linear independence linear regression mixture model variational linear smoother linear gaussian model linearly separable link link function liouville theorem lle see locally linear embedding lms algorithm see least mean squares algorithm local minimum local receptive ﬁeld locally linear embedding location parameter log odds logic sampling logistic regression bayesian mixture model multiclass logistic sigmoid logit function loopy belief propagation loss function loss matrix lossless data compression lossy data compression lower bound step see maximization step machine learning vii macrostate mahalanobis distance manifold map see maximum posterior margin error soft marginal likelihood marginal probability markov blanket markov boundary see markov blanket markov chain ﬁrst order homogeneous second order markov chain monte carlo markov model homogeneous markov network see markov random ﬁeld markov random ﬁeld max sum algorithm maximal clique maximal spanning tree maximization step maximum likelihood gaussian mixture singularities type see evidence approximation maximum margin see margin maximum posterior mcmc see markov chain monte carlo mdn see mixture density network mds see multidimensional scaling mean mean ﬁeld theory mean value theorem measure theory memory based methods message passing pending message schedule variational metropolis algorithm metropolis hastings algorithm index microstate minimum risk minkowski loss missing random missing data mixing coefﬁcient mixture component mixture density network mixture distribution see mixture model mixture model conditional linear regression logistic regression symmetries mixture experts mixture gaussians mlp see multilayer perceptron mnist data model comparison model evidence model selection moment matching momentum variable monte carlo algorithm monte carlo sampling moore penrose pseudo inverse see pseudo inverse moralization mrf see markov random ﬁeld multidimensional scaling multilayer perceptron multimodality multinomial distribution multiplicity mutual information nadaraya watson see kernel regression naive bayes model nats natural language modelling natural parameters nearest neighbour methods neural network convolutional regularization relation gaussian process newton raphson node noiseless coding theorem nonidentiﬁability noninformative prior nonparametric methods normal distribution see gaussian normal equations normal gamma distribution normal wishart distribution normalized exponential see softmax function novelty detection svm object recognition observed variable occam factor oil ﬂow data old faithful data line learning see sequential learning one versus one classiﬁer one versus rest classiﬁer ordered relaxation ornstein uhlenbeck process orthogonal least squares outlier outliers ﬁtting relaxation pac learning see probably approximately correct pac bayesian framework parameter shrinkage parent node particle ﬁlter partition function parzen estimator see kernel density estimator parzen window pattern recognition vii pca see principal component analysis pending message perceptron convergence theorem hardware perceptron criterion perfect map index periodic variable phase space photon noise plate polynomial curve ﬁtting polytree position variable positive deﬁnite covariance positive deﬁnite matrix positive semideﬁnite covariance positive semideﬁnite matrix posterior probability posterior step potential energy potential function power power method precision matrix precision parameter predictive distribution preprocessing principal component analysis bayesian algorithm gibbs sampling mixture distribution physical analogy principal curve principal subspace principal surface prior conjugate consistent improper noninformative probabilistic graphical model see graphical model probabilistic pca probability bayesian classical density frequentist mass function prior product rule sum rule theory probably approximately correct probit function probit regression product rule probability proposal distribution protected conjugate gradients protein sequence pseudo inverse pseudo random numbers quadratic discriminant quality parameter radial basis function rauch tung striebel equations regression regression function regularization tikhonov regularized least squares reinforcement learning reject option rejection sampling relative entropy relevance vector relevance vector machine responsibility ridge regression rms error see root mean square error robbins monro algorithm robot arm robustness root node root mean square error rosenblatt frank rotation invariance rts equations see rauch tung striebel equations running intersection property rvm see relevance vector machine sample mean sample variance sampling importance resampling scale invariance index scale parameter scaling factor schwarz criterion see bayesian information crite rion self organizing map sequential data sequential estimation sequential gradient descent sequential learning sequential minimal optimization serial message passing schedule shannon claude shared parameters shrinkage shur complement sigmoid see logistic sigmoid simplex single class support vector machine singular value decomposition sinusoidal data sir see sampling importance resampling skip layer connection slack variable slice sampling smo see sequential minimal optimization smoother matrix smoothing parameter soft margin soft weight sharing softmax function som see self organizing map sparsity sparsity parameter spectrogram speech recognition sphereing spline functions standard deviation standardizing state space model switching stationary kernel statistical bias see bias statistical independence see independent variables statistical learning theory see computational learning theory steepest descent stirling approximation stochastic stochastic stochastic gradient descent stochastic process stratiﬁed ﬂow student distribution subsampling sufﬁcient statistics sum rule probability sum squares error sum product algorithm hidden markov model supervised learning support vector support vector machine regression multiclass survival ﬁttest svd see singular value decomposition svm see support vector machine switching hidden markov model switching state space model synthetic datasets tail tail path tangent distance tangent propagation tapped delay line target vector test set threshold parameter tied parameters tikhonov regularization time warping tomography training training set transition probability translation invariance tree reweighted message passing treewidth 
"
